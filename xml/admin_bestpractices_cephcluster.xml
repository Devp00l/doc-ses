<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.ceph.cluster">
	<title>&ceph; Cluster Administration</title>
<para>
The chapter gives details about basic administration tasks that you may need to perform on your cluster deployed by using <command>ceph-deploy</command>. The <command>ceph-deploy</command> is a command line utility that provides you with a wide range of subcommands to administrate the &ceph; cluster. Most of the tasks can be performed without the need to stop your cluster. 
</para>
<para>
The general <command>ceph-deply</command> syntax is the following:
</para>
<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>

<para>
   A list of selected <command>ceph-deploy</command> subcommands follows.
  </para>
<itemizedlist>
	<listitem>		
		<para>
		<command>gatherkeys</command> - the subcommand is used when you are adding monitors,OSDs or MDS to the cluster. it gathers authentication keys for provisioning new nodes. It takes host
      names as arguments. It checks for and fetches <literal>client.admin
      keyring</literal>, monitor keyring and
      <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor host. The command syntax is the following:  
		</para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
     <para>
      <replaceable>hostname</replaceable> is the host name of the monitor from
      where keys are to be pulled.
     </para>
	</listitem>
	<listitem>		
		<para>
		<command>mon add</command> - performs all steps required to add a monitor to your cluster, for more details reffer to <xref linkend="Adding.ceph.deploy.monitors"/>. 
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>osd prepare</command>
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>osd activate</command>
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>rgw prepare/activate/create</command>
		</para>
	</listitem>
	<listitem>
	<para>
		<command>purge, purgedata, forgetkeys</command>
	</para>
	</listitem>	
</itemizedlist>


<tip>
   <para>
    Administer &ceph; nodes with <command>ceph-deploy</command> from the admin
    node. Before administering them, always create a new temporary directory
    and <command>cd</command> into it. Then choose one monitor node and gather
    the authentication keys with the <command>gatherkeys</command> subcommand
    from it, and copy the <filename>/etc/ceph/ceph.conf</filename> file from
    the monitor node into the current local directory.
   </para>
<screen>&prompt.cephuser; mkdir ceph_tmp
&prompt.cephuser; cd ceph_tmp
&prompt.cephuser; ceph-deploy gatherkeys ceph_mon_host
&prompt.cephuser; scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
  </tip>

<sect1 xml:id="ceph.deploy.monitors.managment">
	<title>Monitors Managment by Using <command>ceph-deploy</command></title>
	<para>
Adding monitoring nodes to the cluster or removing nodes from the cluster can be performed by using <command>ceph-deploy</command> in several steps described in following sections. But you need to take the following into account:
</para>
<important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <command>ceph-deploy</command> restricts you to only install one monitor
      per host.
     </para>
    </listitem>
    <listitem>
     <para>
      We do not recommend mixing monitors and OSDs on the same host.
     </para>
    </listitem>
    <listitem>
     <para>
      For high availability, you should run a production &ceph; cluster with
      <emphasis>at least</emphasis> three monitors.
     </para>
    </listitem>
   </itemizedlist>
  </important>
	<sect2 xml:id="Adding.ceph.deploy.monitors">
		<title>Adding Monitors</title>
<para>
    After you create a cluster and install &ceph; packages to the monitor
    host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
    information), you may deploy the monitors to the monitor hosts. You may
    specify more monitor host names in the same command.
   </para>
   <note>
    <para>
     When adding a monitor on a host that was not in hosts initially defined
     with the <command>ceph-deploy new</command> command, a <option>public
     network</option> statement needs to be added to the
     <filename>ceph.conf</filename> file. The monitor must be added to the <literal>mon initial members</literal> and <literal>monmap</literal> options or the <option>public_addr</option> nor <option>public_network</option> keys must defined for monitors, otherwise errors arise during the monitor startup.
    </para>
   </note>
<screen>ceph-deploy mon add <replaceable>host name</replaceable></screen>
<para>
When you run the command, the following tasks are performed:
</para>
<orderedlist>
	<listitem>
		<para>
The platform and the distribution of the target host is detected.
</para>
</listitem>
<listitem>
	<para>
 The monitor is configured and added to the cluster. If there is a record for the particular monitor in the &ceph; configuration, the <literal>mon addr</literal> of the monitor will be used, otherwise <command>ceph-deploy</command> falls back and resolve the IP address.
</para>
</listitem>
<listitem>
	<para>
	</para>
</listitem>
<listitem>
	<para>
	<command>ceph-deploy</command> tries to start the monitor and checks the monitor status. 
	</para>
</listitem>
</orderedlist>
   
	</sect2>
	<sect2 xml:id="Removing.ceph.deploy.monitors">
		<title>Removing Monitors</title>
		<para>
    If you have a monitor in your cluster that you want to remove, you may use
    the destroy option. You may specify more monitor host names in the same
    command.
   </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     Ensure that if you remove a monitor, the remaining monitors will be able
     to establish a consensus. If that is not possible, consider adding a
     monitor before removing the monitor you want to take offline.
    </para>
   </note>
	</sect2>

</sect1>
<sect1 xml:id="ceph.deploy.OSD.managment">
	<title>OSDs Managment by Using <command>ceph-deploy</command></title>
	<note>
   <title>Limitations</title>
   <para>
    The procedures described further can be performed only with the
    default CRUSH map created by <command>ceph-deploy</command>.
   </para>
  </note>
<para>
This section gives you information how to add deploy OSD Daemons on particular nodes and also how to add a disk to an existing OSD node. As you may also want to remove a disk or the OSD daemon or the whole OSD node, this section provides you with procedures how to handle these tasks.
</para>

<sect2 xml:id="Adding.OSD.Nodes">
   <title>Adding &ceph; OSD Nodes</title>
   <para>
    The procedure below describes adding of a &ceph; OSD node to your cluster.
   </para>
   <procedure xml:id="proc.Adding.Ceph.Node">
    <title>Adding a &ceph; OSD Node</title>
    <step>
     <para>
      List all &ceph; OSD nodes and then choose a proper name for the new
      node/s
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      Inspect your CRUSH map to find out the bucket type, for a procedure refer
      to <xref linkend="op.crush"/>. Typically the bucket type is
      <emphasis>host</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Create a record for the new node in your CRUSH map.
     </para>
<screen>ceph osd crush add-bucket <replaceable>{bucket name} {bucket type}</replaceable></screen>
     <para>
      for example:
     </para>
<screen>ceph osd crush add-bucket ses4-4 host</screen>
    </step>
    <step>
     <para>
      Add all OSDs that the new node should use. For a procedure refer to
      <xref linkend="storage.bp.inst.add_osd_cephdeploy"/>.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="">
   	<title></title>
   </sect3>
  </sect2>

</sect1>

</chapter>
