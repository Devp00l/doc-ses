<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.hwreq">
 <title>System Requirements</title>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>Hardware Recommendations for the OSD Node</title>
  
  <sect2 xml:id="sysreq.osd">
  <title>Minimal Recommendations per Storage Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     2 GB of RAM per each Terabyte of OSD (2â€° of total raw capacity). Note that during recovery, 1 or even 2GB of RAM per Terabyte of OSD disk space is optimal. 
    </para>
   </listitem>
   <listitem>
    <para>
     1.5 GHz of a CPU core per OSD.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded or redundant 10GbE networks.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks in JBOD configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks should be exclusively used by &storage;.
    </para>
   </listitem>
   <listitem>
    <para>
     Dedicated disk/SSD for the operating system, preferably in a RAID1
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Additional 4 GB of RAM if cache tiering is used.
    </para>
   </listitem>
  </itemizedlist>
 </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Minimum Disk Size for an OSD Node</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal, and the space for the stored data. The minimum (and default)
    value for the journal is 6GB. The minimum space for data is 5GB as
    partitions smaller than 5GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11GB, we do not recommend
    a disk smaller than 20GB, even for testing purposes.
   </para>
  </sect2>



  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Using SSD for OSD Journals</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk. The <option>osd
    journal</option> configuration setting defaults to
    <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable>/journal</filename>.
    You can mount this path to an SSD or to an SSD partition so that it is not
    merely a file on the same disk as the object data.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively small space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than 6 journals on the same SSD disk.
    </para>
   </tip>
  </sect2>
  
  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>Maximum Recommended Count of Disks</title>

  <para>
   You can have as many disks in one server as it allows. There are a few
   things to consider when planning the number of disks per server:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>Network bandwidth.</emphasis> The more disks you have in a
     server, the more data must be transferred via the network card(s) for the
     disk write operations.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Memory.</emphasis> For optimum performance, reserve at least 2GB
     of RAM per Terabyte of disk space installed.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
     more disks it has, the more OSDs the cluster temporarily loses. Moreover,
     to keep the replication rules running, you need to copy all the data from
     the failed server between the other nodes in the cluster.
    </para>
   </listitem>
  </itemizedlist>
  </sect2>
 </sect1>
 
 <sect1 xml:id="sysreq.mon">
  <title>Minimal Recommendations per Monitor Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     3 SUSE Enterprise Storage monitor nodes recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     2 GB of RAM per monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD or fast hard disk in a RAID1 configuration
    </para>
   </listitem>
   <listitem>
    <para>
     On installations with fewer than seven nodes, these can be hosted on the
     system disk of the OSD nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Nodes should be bare metal, not virtualized, for performance reasons.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or &rgw; nodes with the actual workload is not
     supported. No other load generating services other than OSDs, monitors or
     &rgw;s daemons are supported on the same host.
    </para>
   </listitem>
   <listitem>
    <para>
     Configurations may vary from, and frequently exceed, these recommendations
     depending on individual sizing and performance needs.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interfaces for redundancy.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>Minimal Recommendations for &rgw; Nodes</title>

  <para>
   &rgw; nodes should have 6-8 CPU cores and 32 GB of RAM (64 GB recommended).
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>Minimal Recommendations for iSCSI Nodes</title>

  <para>
   iSCSI nodes should have 6-8 CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Naming Limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>
 
 <sect1 xml:id="storage.bp.hwreq.replicas">
  <title>Reducing Data Replication</title>

  <para>
   &ceph; stores data within pools. Pools are logical groups for storing
   objects. Data objects within a pool are replicated so that they can be
   recovered when OSDs fail. New pools are created with the default of three
   replicas. This number includes the 'original' data object itself. Three
   replicas then mean the data object and two its copies for a total of three
   instances.
  </para>

  <para>
   You can manually change the number of pool replicas (see
   <xref linkend="ceph.pools.options.num_of_replicas"/>). Setting a pool to two
   replicas means that there is only <emphasis>one</emphasis> copy of the data
   object besides the object itself, so if you lose one object instance, you
   need to trust that the other copy has not been corrupted for example since
   the last
   <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">scrubbing</link>
   during recovery.
  </para>

  <para>
   Setting a pool to one replica means that there is exactly
   <emphasis>one</emphasis> instance of the data object in the pool. If the OSD
   fails, you lose the data. A possible usage for a pool with one replica is
   storing temporary data for a short time.
  </para>

  <para>
   Setting more than three replicas for a pool means only a small increase in
   reliability, but may be suitable in rare cases. Remember that the more
   replicas, the more disk space needed for storing the object copies. If you
   need the ultimate data security, we recommend using erasure coded pools. For
   more information, see <xref linkend="cha.ceph.erasure"/>.
  </para>

  <warning>
   <para>
    We strongly encourage you to either leave the number of replicas for a pool
    at the default value of 3, or use higher value if suitable. Setting the
    number of replicas to a smaller number is dangerous and may cause the loss
    of data stored in the cluster.
   </para>
  </warning>
 </sect1>
 <sect1 xml:id="storage.bp.hwreq.ec">
  <title>Reducing Redundancy Similar to RAID 6 Arrays</title>

  <para>
   When creating a new pool, &ceph; uses the replica type by default, which
   replicates objects across multiple disks to be able to recover from an OSD
   failure. While this type of pool is safe, it uses a lot of disk space to
   store objects.
  </para>

  <para>
   To reduce the disk space needed, &ceph; implements <emphasis>erasure
   coded</emphasis> pools. This method adds extra chunks of data to detect
   errors in a data stream. Erasure coded pools exhibit similar performance,
   reliability, and storage saved as RAID 6 arrays.
  </para>

  <para>
   As erasure coding is a complex topic, you need to study it properly to be
   able to deploy it for optimum performance. For more information, see
   <xref linkend="cha.ceph.erasure"/>.
  </para>
 </sect1>



 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run OSDs and monitor nodes on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance&mdash;the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another aspect is whether to share disks between an OSD, a monitor node, and
   the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>

 </chapter>
