<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.operating">
 <title>Operating a &ceph; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section introduces information on basic cluster services operating. You
  will learn how to start or stop &ceph; services, how to monitor a cluster's
  state, how to use and modify CRUSH maps, and how to manage storage pools.
 </para>
 <sect1 xml:id="ceph.operating.services">
  <title>Operating &ceph; Services</title>

  <para>
   &ceph; related services are operated with the <command>systemctl</command>
   command. The operation takes place on the node you are currently logged in
   to. You need to have &rootuser; privileges to be able to operate on &ceph;
   services.
  </para>

  <para>
   The following subcommands are supported for all &ceph; services:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service(s) specified by
      <replaceable>target</replaceable>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service(s) specified by <replaceable>target</replaceable> if
      they are not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service(s) specified by
      <replaceable>target</replaceable>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service(s) specified by <replaceable>target</replaceable> so
      that they are automatically started on system start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service(s) specified by <replaceable>target</replaceable> so
      that they are not automatically started on system start-up. You need to
      start them manually with <command>systemctl start</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following subcommand is supported for OSD related targets:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl mask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prevents the service from being started manually or automatically.
      <command>systemctl</command> is creating a symbolic link from the
      specific target file located in <filename>/etc/systemd/system/</filename>
      to <filename>/dev/null</filename>. Targets in
      <filename>/etc/systemd</filename> override those provided by packages in
      <filename>/usr/lib/systemd</filename>. &systemd; recognizes the symbolic
      link and will not start the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl unmask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Opposite to <command>systemctl mask</command>. Removes the symbolic link
      from the specific target file located in
      <filename>/etc/systemd/system/</filename> to
      <filename>/dev/null</filename>, so that targets located in
      <filename>/usr/lib/systemd</filename> are valid again.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   You can either operate on all &ceph; services at once, or all &ceph;
   services of a certain type (such as OSD or monitor), or a specific instance
   of service identified by its instance name.
  </para>

  <sect2>
   <title>Operating on All &ceph; Services</title>
   <para>
    To operate on all &ceph; services at once, run:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph*</screen>
   <para>
    For example:
   </para>
<screen>systemctl status ceph*
ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:09:12 EST; 4min 52s ago
  Main PID: 2788 (ceph-osd)
    CGroup:
/system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─2788 /usr/bin/ceph-osd -f \-\-cluster ceph \-\-id 0
Jan 23 09:09:12 ceph-node2 ceph-osd-prestart.sh[2737]:
create-or-move updated item name 'osd.0' weight 0.01 at location
{host=ceph-node2,root=default} to crush map
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]: starting osd.0 at
:/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device

ceph-radosgw@rgw.ceph4.service - Ceph rados gateway
    Loaded: loaded
(/usr/lib/systemd/system/ceph-radosgw@rgw.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:11:29
EST; 2min 35s ago
  Main PID: 3741 (radosgw)
    CGroup:
/system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.ceph4.service
            └─3741 /usr/bin/radosgw -f \-\-conf
/etc/ceph/ceph.conf \-\-name client.radosgw.ceph4
 Jan 23 09:11:31 ceph-node2 radosgw[3741]: 2015-01-23
09:11:31.206129 7f60e1df9700 -1 failed to list objects pool_iterate returned r=-2</screen>
   <tip>
    <para>
     <command>systemctl</command> &ceph; commands cannot operate on services
     not enabled, not masked, and those which have never been started. To
     include all present services on a node, use the <command>rcceph</command>
     wrapper. For example:
    </para>
<screen>rcceph status</screen>
   </tip>
  </sect2>

  <sect2>
   <title>Operating on All &ceph; Services of a Specific Type</title>
   <para>
    It is possible to address services of a specific type only&mdash;such as
    OSDs, monitors, or &rgw;:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>*</screen>
   <para>
    A few practical examples follow:
   </para>
   <para>
    Print the status of all monitor services on a node:
   </para>
<screen>systemctl status ceph-mon*</screen>
   <para>
    Start all OSD services on a node:
   </para>
<screen>systemctl start ceph-osd*</screen>
   <para>
    Stop all &rgw; services on a node:
   </para>
<screen>systemctl stop ceph-radosgw*</screen>
  </sect2>

  <sect2>
   <title>Operating on a Specific &ceph; Service</title>
   <para>
    If you want to operate on a single service on a node, you need to know its
    instance name. It is available in the output of <command>systemctl status
    ceph*</command>.
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>@<replaceable>instance_name</replaceable>.service</screen>
   <para>
    Print the status information of the OSD service with instance name
    <literal>0</literal>:
   </para>
<screen>systemctl status ceph-osd@0.service</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.monitor">
  <title>Determining Cluster State</title>

<!-- 2014-01-13 tbazant: reusing http://docs.ceph.com/docs/master/rados/operations/monitoring/ -->

  <para>
   Once you have a running cluster, you may use the <command>ceph</command>
   tool to monitor your cluster. Determining cluster state typically involves
   checking OSD status, monitor status, placement group status and metadata
   server status.
  </para>

  <tip>
   <title>Interactive Mode</title>
   <para>
    To run the <command>ceph</command> tool in an interactive mode, type
    <command>ceph</command> at the command line with no arguments. The
    interactive mode is more convenient if you are going to enter more
    <command>ceph</command> commands in a row. For example:
   </para>
<screen>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
  </tip>

  <sect2 xml:id="monitor.health">
   <title>Checking Cluster Health</title>
   <para>
    After you start your cluster, and before you start reading and/or writing
    data, check your cluster’s health first. You can check on the health of
    your &ceph; cluster with the following:
   </para>
<screen>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
   <para>
    Upon starting the &ceph; cluster, you will likely encounter a health
    warning such as <literal>HEALTH_WARN XXX num placement groups
    stale</literal>. Wait a few moments and check it again. When your cluster
    is ready, <command>ceph health</command> should return a message such as
    <literal>HEALTH_OK</literal>. At that point, it is okay to begin using the
    cluster.
   </para>
  </sect2>

  <sect2 xml:id="monitor.watch">
   <title>Watching a Cluster</title>
   <para>
    To watch the cluster’s ongoing events, open a new terminal and enter:
   </para>
<screen>ceph -w</screen>
   <para>
    &ceph; will print each event. For example, a tiny &ceph; cluster consisting
    of one monitor, and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41338: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
             952 active+clean

2014-06-02 15:45:21.655871 osd.0 [INF] 17.71 deep-scrub ok
2014-06-02 15:45:47.880608 osd.1 [INF] 1.0 scrub ok
2014-06-02 15:45:48.865375 osd.1 [INF] 1.3 scrub ok
2014-06-02 15:45:50.866479 osd.1 [INF] 1.4 scrub ok
[...]
2014-06-02 15:45:55.720929 mon.0 [INF] pgmap v41343: 952 pgs: \
 1 active+clean+scrubbing+deep, 951 active+clean; 17130 MB data, 115 GB used, \
 167 GB / 297 GB avail</screen>
   <para>
    The output provides the following information:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Cluster ID
     </para>
    </listitem>
    <listitem>
     <para>
      Cluster health status
     </para>
    </listitem>
    <listitem>
     <para>
      The monitor map epoch and the status of the monitor quorum
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD map epoch and the status of OSDs
     </para>
    </listitem>
    <listitem>
     <para>
      The placement group map version
     </para>
    </listitem>
    <listitem>
     <para>
      The number of placement groups and pools
     </para>
    </listitem>
    <listitem>
     <para>
      The <emphasis>notional</emphasis> amount of data stored and the number of
      objects stored; and,
     </para>
    </listitem>
    <listitem>
     <para>
      The total amount of data stored.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>How &ceph; Calculates Data Usage</title>
    <para>
     The <literal>used</literal> value reflects the actual amount of raw
     storage used. The <literal>xxx GB / xxx GB</literal> value means the
     amount available (the lesser number) of the overall storage capacity of
     the cluster. The notional number reflects the size of the stored data
     before it is replicated, cloned or snapshotted. Therefore, the amount of
     data actually stored typically exceeds the notional amount stored, because
     &ceph; creates replicas of the data and may also use storage capacity for
     cloning and snapshotting.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="monitor.stats">
   <title>Checking a Cluster’s Usage Stats</title>
   <para>
    To check a cluster’s data usage and data distribution among pools, you
    can use the <command>df</command> option. It is similar to Linux
    <command>df</command>. Execute the following:
   </para>
<screen>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED 
    27570M     27304M         266M          0.97 
POOLS:
    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS 
    data             0       120         0         5064M           4 
    metadata         1         0         0         5064M           0 
    rbd              2         0         0         5064M           0 
    hot-storage      4       134         0         4033M           2 
    cold-storage     5      227k         0         5064M           1 
    pool1            6         0         0         5064M           0</screen>
   <para>
    The <literal>GLOBAL</literal> section of the output provides an overview of
    the amount of storage your cluster uses for your data.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>SIZE</literal>: The overall storage capacity of the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>AVAIL</literal>: The amount of free space available in the
      cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>RAW USED</literal>: The amount of raw storage used.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>% RAW USED</literal>: The percentage of raw storage used. Use
      this number in conjunction with the <literal>full ratio</literal> and
      <literal>near full ratio</literal> to ensure that you are not reaching
      your cluster’s capacity. See
      <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
      Capacity</link> for additional details.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <literal>POOLS</literal> section of the output provides a list of pools
    and the notional usage of each pool. The output from this section
    <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
    example, if you store an object with 1MB of data, the notional usage will
    be 1MB, but the actual usage may be 2MB or more depending on the number of
    replicas, clones and snapshots.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>NAME</literal>: The name of the pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ID</literal>: The pool ID.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>USED</literal>: The notional amount of data stored in kilobytes,
      unless the number appends M for megabytes or G for gigabytes.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>%USED</literal>: The notional percentage of storage used per
      pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>OBJECTS</literal>: The notional number of objects stored per
      pool.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     The numbers in the POOLS section are notional. They are not inclusive of
     the number of replicas, snapshots or clones. As a result, the sum of the
     USED and %USED amounts will not add up to the RAW USED and %RAW USED
     amounts in the %GLOBAL section of the output.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="monitor.status">
   <title>Checking a Cluster’s Status</title>
   <para>
    To check a cluster’s status, execute the following:
   </para>
<screen>ceph status</screen>
   <para>
    or
   </para>
<screen>ceph -s</screen>
   <para>
    In interactive mode, type <command>status</command> and press
    <keycap function="enter"/>.
   </para>
<screen>ceph&gt; status</screen>
   <para>
    &ceph; will print the cluster status. For example, a tiny &ceph; cluster
    consisting of one monitor and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
  </sect2>

  <sect2 xml:id="monitor.osdstatus">
   <title>Checking OSD Status</title>
   <para>
    You can check OSDs to ensure they are up and on by executing:
   </para>
<screen>ceph osd stat</screen>
   <para>
    or
   </para>
<screen>ceph osd dump</screen>
   <para>
    You can also view OSDs according to their position in the CRUSH map.
   </para>
<screen>ceph osd tree</screen>
   <para>
    &ceph; will print out a CRUSH tree with a host, its OSDs, whether they are
    up and their weight.
   </para>
<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
  </sect2>

  <sect2 xml:id="monitor.monstatus">
   <title>Checking Monitor Status</title>
   <para>
    If your cluster has multiple monitors (likely), you should check the
    monitor quorum status after you start the cluster before reading and/or
    writing data. A quorum must be present when multiple monitors are running.
    You should also check monitor status periodically to ensure that they are
    running.
   </para>
   <para>
    To display the monitor map, execute the following:
   </para>
<screen>ceph mon stat</screen>
   <para>
    or
   </para>
<screen>ceph mon dump</screen>
   <para>
    To check the quorum status for the monitor cluster, execute the following:
   </para>
<screen>ceph quorum_status</screen>
   <para>
    &ceph; will return the quorum status. For example, a &ceph; cluster
    consisting of three monitors may return the following:
   </para>
<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
  </sect2>

<!-- 2015-11-27 tbazant: MDS not supported yet
  <sect2 xml:id="monitor.mdsstatus">
   <title>Checking MDS Status</title>
   <para>
    Metadata servers provide metadata services for &ceph; FS. Metadata
    servers have two sets of states: <literal>up | down</literal> and
    <literal>active | inactive</literal>. To ensure your metadata servers
    are <literal>up</literal> and <literal>active</literal>, execute the
    following:
   </para>
<screen>ceph mds stat</screen>
   <para>
    To display details of the metadata cluster, execute the following:
   </para>
<screen>ceph mds dump</screen>
  </sect2>
  -->

  <sect2 xml:id="monitor.pgroupstatus">
   <title>Checking Placement Group States</title>
   <para>
    Placement groups map objects to OSDs. When you monitor your placement
    groups, you will want them to be <literal>active</literal> and
    <literal>clean</literal>. For a detailed discussion, refer to
    <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
    OSDs and Placement Groups.</link>
   </para>
  </sect2>

  <sect2 xml:id="monitor.adminsocket">
   <title>Using the Admin Socket</title>
   <para>
    The &ceph; admin socket allows you to query a daemon via a socket
    interface. By default, &ceph; sockets reside under
    <filename>/var/run/ceph</filename>. To access a daemon via the admin
    socket, log in to the host running the daemon and use the following
    command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>
   <para>
    To view the available admin socket commands, execute the following command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>
   <para>
    The admin socket command enables you to show and set your configuration at
    runtime. See
    <remark>add xref to
  http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config</remark>
    Viewing a Configuration at Runtime for details.
   </para>
   <para>
    Additionally, you can set configuration values at runtime directly (the
    admin socket bypasses the monitor, unlike <command>ceph tell</command>
    <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
    injectargs, which relies on the monitor but does not require you to log in
    directly to the host in question).
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.pools">
  <title>Managing Storage Pools</title>

<!-- 2014-11-21 tbazant: reusing
 http://ceph.com/docs/master/rados/operations/pools/ -->

  <para>
   When you first deploy a cluster without creating a pool, &ceph; uses the
   default pools for storing data. A pool provides you with:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>Resilience</emphasis>: You can set how many OSDs are allowed to
     fail without losing data. For replicated pools, it is the desired number
     of copies/replicas of an object. A typical configuration stores an object
     and one additional copy (that is <emphasis>size=2</emphasis>), but you can
     determine the number of copies/replicas. For erasure coded pools, it is
     the number of coding chunks (that is <emphasis>m=2</emphasis> in the
     erasure code profile).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Placement Groups</emphasis>: You can set the number of placement
     groups for the pool. A typical configuration uses approximately 100
     placement groups per OSD to provide optimal balancing without using up too
     many computing resources. When setting up multiple pools, be careful to
     ensure you set a reasonable number of placement groups for both the pool
     and the cluster as a whole.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, a CRUSH
     ruleset mapped to the pool enables CRUSH to identify a rule for the
     placement of the object and its replicas (or chunks for erasure coded
     pools) in your cluster. You can create a custom CRUSH rule for your pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Snapshots</emphasis>: When you create snapshots with
     <command>ceph osd pool mksnap</command>, you effectively take a snapshot
     of a particular pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Set Ownership</emphasis>: You can set a user ID as the owner of
     a pool.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To organize data into pools, you can list, create, and remove pools. You can
   also view the usage statistics for each pool.
  </para>

  <sect2 xml:id="ceph.pools.operate">
   <title>Operating Pools</title>
   <para>
    This section introduces practical information to perform basic tasks with
    pools. You can find out how to list, create, and delete pools, as well as
    show pool statistics or manage snapshots of a pool.
   </para>
   <sect3>
    <title>List Pools</title>
    <para>
     To list your cluster’s pools, execute:
    </para>
<screen>ceph osd lspools
0 data,1 metadata,2 rbd,</screen>
    <para>
     The command outputs information about data, metadata, and rados block
     devices (rbd).
    </para>
   </sect3>
   <sect3 xml:id="ceph.pools.operate.add_pool">
    <title>Create a Pool</title>
    <para>
     To create a replicated pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>crush_ruleset_name</replaceable>, <replaceable>expected_num_objects</replaceable></screen>
    <para>
     To create an erasure pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable>, <replaceable>expected_num_objects</replaceable></screen>
    <variablelist>
     <varlistentry>
      <term>pool_name</term>
      <listitem>
       <para>
        The name of the pool. It must be unique. This option is required.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The total number of placement groups for the pool. This option is
        required. Default value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The total number of placement groups for placement purposes. This
        should be equal to the total number of placement groups, except for
        placement group splitting scenarios. This option is required. Default
        value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_type</term>
      <listitem>
       <para>
        The pool type which may either be <emphasis>replicated</emphasis> to
        recover from lost OSDs by keeping multiple copies of the objects or
        <emphasis>erasure</emphasis> to get a kind of generalized RAID5
        capability. The replicated pools require more raw storage but implement
        all &ceph; operations. The erasure pools require less raw storage but
        only implement a subset of the available operations. Default is
        'replicated'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush_ruleset_name</term>
      <listitem>
       <para>
        The name of the crush ruleset for this pool. If the specified ruleset
        does not exist, the creation of replicated pool will fail with -ENOENT.
        But replicated pool will create a new erasure ruleset with specified
        name. The default value is 'erasure-code' for erasure pool. Picks up
        &ceph; configuration variable
        <option>osd_pool_default_crush_replicated_ruleset</option> for
        replicated pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>erasure_code_profile=profile</term>
      <listitem>
       <para>
        For erasure pools only. Use the erasure code profile. It must be an
        existing profile as defined by <command>osd erasure-code-profile
        set</command>.
       </para>
       <para>
        When you create a pool, set the number of placement groups to a
        reasonable value (for example 100). Consider the total number of
        placement groups per OSD too. Placement groups are computationally
        expensive, so performance will degrade when you have many pools with
        many placement groups (for example 50 pools with 100 placement groups
        each). The point of diminishing returns depends upon the power of the
        OSD host.
       </para>
       <para>
        See
        <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement
        Groups</link> for details on calculating an appropriate number of
        placement groups for your pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>expected_num_objects</term>
      <listitem>
       <para>
        The expected number of objects for this pool. By setting this value,
        the PG folder splitting happens at the pool creation time. This avoids
        the latency impact with a runtime folder splitting.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Set Pool Quotas</title>
    <para>
     You can set pool quotas for the maximum number of bytes and/or the maximum
     number of objects per pool.
    </para>
<screen>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph osd pool set-quota data max_objects 10000</screen>
    <para>
     To remove a quota, set its value to 0.
    </para>
   </sect3>
   <sect3 xml:id="ceph.pools.operate.del_pool">
    <title>Delete a Pool</title>
    <para>
     To delete a pool, execute:
    </para>
<screen>ceph osd pool delete <replaceable>pool-name</replaceable> <replaceable>pool-name</replaceable> --yes-i-really-really-mean-it</screen>
    <para>
     If you created your own rulesets and rules for a pool you created, you
     should consider removing them when you no longer need your pool. If you
     created users with permissions strictly for a pool that no longer exists,
     you should consider deleting those users too.
    </para>
   </sect3>
   <sect3>
    <title>Rename a Pool</title>
    <para>
     To rename a pool, execute:
    </para>
<screen>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
    <para>
     If you rename a pool and you have per-pool capabilities for an
     authenticated user, you must update the user’s capabilities with the new
     pool name.
    </para>
   </sect3>
   <sect3>
    <title>Show Pool Statistics</title>
    <para>
     To show a pool’s usage statistics, execute:
    </para>
<screen>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
   </sect3>
   <sect3 xml:id="ceph.pools.values">
    <title>Set Pool Values</title>
    <para>
     To set a value to a pool, execute:
    </para>
<screen>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
    <para>
     You may set values for the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>size</term>
      <listitem>
       <para>
        Sets the number of replicas for objects in the pool. See
        <xref linkend="ceph.pools.options.num_of_replicas"/> for further
        details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>min_size</term>
      <listitem>
       <para>
        Sets the minimum number of replicas required for I/O. See
        <xref linkend="ceph.pools.options.num_of_replicas"/> for further
        details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crash_replay_interval</term>
      <listitem>
       <para>
        The number of seconds to allow clients to replay acknowledged, but
        uncommitted requests.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The number of placement groups for the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating data
        placement.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush_ruleset</term>
      <listitem>
       <para>
        The ruleset to use for mapping object placement in the cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hashpspool</term>
      <listitem>
       <para>
        Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
        flag changes the algorithm to better distribute PGs to OSDs. After
        enabling this flag on a pool whose HASHPSPOOL flag was set to 0, the
        cluster starts backfilling to have a correct placement of all PGs
        again. Be aware that this can create quite substantial I/O load on a
        cluster, so good planning must be done on a highly loaded production
        clusters.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>nodelete</term>
      <listitem>
       <para>
        Prevents the pool from being removed.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>nopgchange</term>
      <listitem>
       <para>
        Prevents the pool's <option>pg_num</option> and
        <option>pgp_num</option> from being changed.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>nosizechange</term>
      <listitem>
       <para>
        Prevents the pool's size from being changed.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>write_fadvise_dontneed</term>
      <listitem>
       <para>
        Set/Unset the <literal>WRITE_FADVISE_DONTNEED</literal> flag on a given
        pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>noscrub,nodeep-scrub</term>
      <listitem>
       <para>
        Disables (deep)-scrubbing of the data for the specific pool to resolve
        temporary high I/O load.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_type</term>
      <listitem>
       <para>
        Enables hit set tracking for cache pools. See
        <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</link> for additional information. This option can have the
        following values: <literal>bloom</literal>,
        <literal>explicit_hash</literal>, <literal>explicit_object</literal>.
        Default is <literal>bloom</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_count</term>
      <listitem>
       <para>
        The number of hit sets to store for cache pools. The higher the number,
        the more RAM consumed by the <systemitem>ceph-osd</systemitem> daemon.
        Default is <literal>0</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_period</term>
      <listitem>
       <para>
        The duration of a hit set period in seconds for cache pools. The higher
        the number, the more RAM consumed by the
        <systemitem>ceph-osd</systemitem> daemon.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_fpp</term>
      <listitem>
       <para>
        The false positive probability for the bloom hit set type. See
        <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</link> for additional information. Valid range is 0.0 - 1.0
        Default is <literal>0.05</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_dirty_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing modified (dirty) objects
        before the cache tiering agent will flush them to the backing storage
        pool. Default is <literal>.4</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_dirty_high_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing modified (dirty) objects
        before the cache tiering agent will flush them to the backing storage
        pool with a higher speed. Default is <literal>.6</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_full_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing unmodified (clean) objects
        before the cache tiering agent will evict them from the cache pool.
        Default is <literal>.8</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_bytes</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_bytes</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_objects</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_objects</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_grade_decay_rate</term>
      <listitem>
       <para>
        Temperature decay rate between two successive
        <literal>hit_set</literal>s. Default is <literal>20</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_grade_search_last_n</term>
      <listitem>
       <para>
        Count at most <literal>N</literal> appearances in
        <literal>hit_set</literal>s for temperature calculation. Default is
        <literal>1</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_flush_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will flush an
        object from the cache pool to the storage pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_evict_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will evict an
        object from the cache pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>fast_read</term>
      <listitem>
       <para>
        If this flag is enabled on erasure coding pools, then the read request
        issues sub-reads to all shards, and waits until it receives enough
        shards to decode to serve the client. In the case of
        <emphasis>jerasure</emphasis> and <emphasis>isa</emphasis> erasure
        plug-ins, once the first <literal>K</literal> replies return, then the
        client’s request is served immediately using the data decoded from
        these replies. This helps to gain some resources for better
        performance. Currently, this flag is only supported for erasure coding
        pools. Default is <literal>0</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>scrub_min_interval</term>
      <listitem>
       <para>
        The minimum interval in seconds for pool scrubbing when the cluster
        load is low. The default <literal>0</literal> means that the
        <option>osd_scrub_min_interval</option> value from the &ceph;
        configuration file is used.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>scrub_max_interval</term>
      <listitem>
       <para>
        The maximum interval in seconds for pool scrubbing, regardless of the
        cluster load. The default <literal>0</literal> means that the
        <option>osd_scrub_max_interval</option> value from the &ceph;
        configuration file is used.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>deep_scrub_interval</term>
      <listitem>
       <para>
        The interval in seconds for the pool <emphasis>deep</emphasis>
        scrubbing. The default <literal>0</literal> means that the
        <option>osd_deep_scrub</option> value from the &ceph; configuration
        file is used.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Get Pool Values</title>
    <para>
     To get a value from a pool, execute:
    </para>
<screen>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
    <para>
     You can get values for keys listed in <xref linkend="ceph.pools.values"/>
     plus the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The number of placement groups for the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating data
        placement. Valid range is equal to or less than
        <literal>pg_num</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.pools.options.num_of_replicas">
    <title>Set the Number of Object Replicas</title>
    <para>
     To set the number of object replicas on a replicated pool, execute the
     following:
    </para>
<screen>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
    <important>
     <para>
      The <replaceable>num-replicas</replaceable> includes the object itself.
      If you for example want the object and two copies of the object for a
      total of three instances of the object, specify 3.
     </para>
    </important>
    <para>
     For example:
    </para>
<screen>ceph osd pool set data size 3</screen>
    <para>
     You may execute this command for each pool.
    </para>
    <note>
     <para>
      An object might accept I/Os in degraded mode with fewer than
      <literal>pool size</literal> replicas. To set a minimum number of
      required replicas for I/O, you should use the <literal>min_size</literal>
      setting. For example:
     </para>
<screen>ceph osd pool set data min_size 2</screen>
     <para>
      This ensures that no object in the data pool will receive I/O with fewer
      than <literal>min_size</literal> replicas.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Get the Number of Object Replicas</title>
    <para>
     To get the number of object replicas, execute the following:
    </para>
<screen>ceph osd dump | grep 'replicated size'</screen>
    <para>
     &ceph; will list the pools, with the <literal>replicated size</literal>
     attribute highlighted. By default, &ceph; creates two replicas of an
     object (a total of three copies, or a size of 3).
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="cha.ceph.snapshots">
   <title>Snapshots</title>
   <para>
    A snapshot is a read-only copy of the state of an object&mdash;a pool or an
    image&mdash;at a particular point in time. This way you can retain a
    history of its state. There are two types of snapshots in &ceph;&mdash;RBD
    snapshots and pool snapshots.
   </para>
   <sect3 xml:id="cha.ceph.snapshots.rbd">
    <title>RBD Snapshots</title>
    <para>
     RBD snapshot is a snapshot of a RADOS block device image. With snapshots
     you retain a history of the image’s state. &ceph; also supports snapshot
     layering, which allows you to clone VM images quickly and easily. &ceph;
     supports block device snapshots using the <command>rbd</command> command
     and many higher level interfaces, including &qemu;,
     <systemitem>libvirt</systemitem>, OpenStack and CloudStack.
    </para>
    <note>
     <para>
      Stop input/output operations before snapshotting an image. If the image
      contains a file system, the file system must be in a consistent state
      <emphasis>before</emphasis> snapshotting.
     </para>
    </note>
    <sect4>
     <title>Cephx Notes</title>
     <para>
      When <systemitem>cephx</systemitem> is enabled (see
      <link xlink:href="http://ceph.com/docs/master/rados/configuration/auth-config-ref/"/>
      for more information), you must specify a user name or ID and a path to
      the keyring containing the corresponding key for the user. See
      <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
      Management</link> for more details. You may also add the
      <systemitem>CEPH_ARGS</systemitem> environment variable to avoid re-entry
      of the following parameters.
     </para>
<screen>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
     <para>
      For example:
     </para>
<screen>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
     <tip>
      <para>
       Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
       environment variable so that you do not need to enter them each time.
      </para>
     </tip>
    </sect4>
    <sect4>
     <title>Snapshot Basics</title>
     <para>
      The following procedures demonstrate how to create, list, and remove
      snapshots using the <command>rbd</command> command on the command line.
     </para>
     <sect5>
      <title>Create Snapshot</title>
      <para>
       To create a snapshot with <command>rbd</command>, specify the
       <option>snap create</option> option, the pool name and the image name.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool rbd snap create --snap snapshot1 image1
rbd snap create rbd/image1@snapshot1</screen>
     </sect5>
     <sect5>
      <title>List Snapshots</title>
      <para>
       To list snapshots of an image, specify the pool name and the image name.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool rbd snap ls image1
rbd snap ls rbd/image1</screen>
     </sect5>
     <sect5>
      <title>Rollback Snapshot</title>
      <para>
       To rollback to a snapshot with <command>rbd</command>, specify the
       <option>snap rollback</option> option, the pool name, the image name and
       the snap name.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 snap rollback --snap snapshot1 image1
rbd snap rollback pool1/image1@snapshot1</screen>
      <note>
       <para>
        Rolling back an image to a snapshot means overwriting the current
        version of the image with data from a snapshot. The time it takes to
        execute a rollback increases with the size of the image. It is
        <emphasis>faster to clone</emphasis> from a snapshot <emphasis>than to
        rollback</emphasis> an image to a snapshot, and it is the preferred
        method of returning to a pre-existing state.
       </para>
      </note>
     </sect5>
     <sect5>
      <title>Delete a Snapshot</title>
      <para>
       To delete a snapshot with <command>rbd</command>, specify the
       <option>snap rm</option> option, the pool name, the image name and the
       user name.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 snap rm --snap snapshot1 image1
rbd snap rm pool1/imag1@snapshot1</screen>
      <note>
       <para>
        &ceph; OSDs delete data asynchronously, so deleting a snapshot does not
        free up the disk space immediately.
       </para>
      </note>
     </sect5>
     <sect5>
      <title>Purge Snapshots</title>
      <para>
       To delete all snapshots for an image with <command>rbd</command>,
       specify the <option>snap purge</option> option and the image name.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 snap purge image1
rbd snap purge pool1/image1</screen>
     </sect5>
    </sect4>
    <sect4 xml:id="ceph.snapshoti.layering">
     <title>Layering</title>
     <para>
      &ceph; supports the ability to create many copy-on-write (COW) clones of
      a block device snapshot. Snapshot layering enables &ceph; block device
      clients to create images very quickly. For example, you might create a
      block device image with a Linux VM written to it; then, snapshot the
      image, protect the snapshot, and create as many copy-on-write clones as
      you like. A snapshot is read-only, so cloning a snapshot simplifies
      semantics&mdash;making it possible to create clones rapidly.
     </para>
     <note>
      <para>
       The terms “parent” and “child” mentioned in the command line
       examples below mean a &ceph; block device snapshot (parent), and the
       corresponding image cloned from the snapshot (child).
      </para>
     </note>
     <para>
      Each cloned image (child) stores a reference to its parent image, which
      enables the cloned image to open the parent snapshot and read it.
     </para>
     <para>
      A COW clone of a snapshot behaves exactly like any other &ceph; block
      device image. You can read to, write from, clone, and resize cloned
      images. There are no special restrictions with cloned images. However,
      the copy-on-write clone of a snapshot refers to the snapshot, so you
      <emphasis>must</emphasis> protect the snapshot before you clone it.
     </para>
     <note>
      <para>
       &ceph; only supports cloning for <emphasis>format 2</emphasis> images
       (that is created with <command>rbd create --image-format 2</command>).
      </para>
     </note>
     <sect5>
      <title>Getting Started with Layering</title>
      <para>
       &ceph; block device layering is a simple process. You must have an
       image. You must create a snapshot of the image. You must protect the
       snapshot. Once you have performed these steps, you can begin cloning the
       snapshot.
      </para>
      <para>
       The cloned image has a reference to the parent snapshot, and includes
       the pool ID, image ID and snapshot ID. The inclusion of the pool ID
       means that you may clone snapshots from one pool to images in another
       pool.
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         <emphasis>Image Template</emphasis>: A common use case for block
         device layering is to create a master image and a snapshot that serves
         as a template for clones. For example, a user may create an image for
         a Linux distribution (for example &sls;), and create a snapshot for
         it. Periodically, the user may update the image and create a new
         snapshot (for example <command>zypper ref &amp;&amp; zypper
         patch</command> followed by <command>rbd snap create</command>). As
         the image matures, the user can clone any one of the snapshots.
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Extended Template</emphasis>: A more advanced use case
         includes extending a template image that provides more information
         than a base image. For example, a user may clone an image (a VM
         template) and install other software (for example a database, a
         content management system, an analytics system, etc.) and then
         snapshot the extended image, which itself may be updated same as the
         base image.
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Template Pool</emphasis>: One way to use block device
         layering is to create a pool that contains master images that act as
         templates, and snapshots of those templates. You may then extend
         read-only privileges to users so that they may clone the snapshots
         without the ability to write or execute within the pool.
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Image Migration/Recovery</emphasis>: One way to use block
         device layering is to migrate or recover data from one pool into
         another pool.
        </para>
       </listitem>
      </itemizedlist>
     </sect5>
     <sect5>
      <title>Protecting a Snapshot</title>
      <para>
       Clones access the parent snapshots. All clones would break if a user
       inadvertently deleted the parent snapshot. To prevent data loss, you
       need to protect the snapshot before you can clone it.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 snap protect --image image1 --snap snapshot1
rbd snap protect pool1/image1@snapshot1</screen>
      <note>
       <para>
        You cannot delete a protected snapshot.
       </para>
      </note>
     </sect5>
     <sect5>
      <title>Cloning a Snapshot</title>
      <para>
       To clone a snapshot, you need to specify the parent pool, image and
       snapshot, the child pool and image name. You must protect the snapshot
       before you can clone it.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
rbd clone
<replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
 <replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
      <note>
       <para>
        You may clone a snapshot from one pool to an image in another pool. For
        example, you may maintain read-only images and snapshots as templates
        in one pool, and writeable clones in another pool.
       </para>
      </note>
     </sect5>
     <sect5>
      <title>Unprotecting a Snapshot</title>
      <para>
       Before you can delete a snapshot, you must unprotect it first.
       Additionally, you may <emphasis>not</emphasis> delete snapshots that
       have references from clones. You must flatten each clone of a snapshot,
       before you can delete the snapshot.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
rbd snap unprotect pool1/image1@snapshot1</screen>
     </sect5>
     <sect5>
      <title>Listing Children of a Snapshot</title>
      <para>
       To list the children of a snapshot, execute the following:
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 children --image image1 --snap snapshot1
rbd children pool1/image1@snapshot1</screen>
     </sect5>
     <sect5>
      <title>Flattening a Cloned Image</title>
      <para>
       Cloned images retain a reference to the parent snapshot. When you remove
       the reference from the child clone to the parent snapshot, you
       effectively “flatten” the image by copying the information from the
       snapshot to the clone. The time it takes to flatten a clone increases
       with the size of the snapshot. To delete a snapshot, you must flatten
       the child images first.
      </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
      <para>
       For example:
      </para>
<screen>rbd --pool pool1 flatten --image image1
rbd flatten pool1/image1</screen>
      <note>
       <para>
        Since a flattened image contains all the information from the snapshot,
        a flattened image will take up more storage space than a layered clone.
       </para>
      </note>
     </sect5>
    </sect4>
   </sect3>
   <sect3 xml:id="cha.ceph.snapshots.pool">
    <title>Pool Snapshots</title>
    <para>
     Pool snapshots are snapshots of the state of the whole &ceph; pool. With
     pool snapshots, you can retain the history of the pool's state. Depending
     on the pool's size, creating pool snapshots may require a lot of storage
     space. Always check the related storage for enough disk space before
     creating a snapshot of a pool.
    </para>
    <sect4>
     <title>Make a Snapshot of a Pool</title>
     <para>
      To make a snapshot of a pool, execute:
     </para>
<screen>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
     <para>
      For example:
     </para>
<screen>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
    </sect4>
    <sect4>
     <title>Remove a Snapshot of a Pool</title>
     <para>
      To remove a snapshot of a pool, execute:
     </para>
<screen>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
</chapter>
