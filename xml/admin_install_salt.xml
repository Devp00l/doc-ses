<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &salt;</title>
 <para>
  &salt; is a <emphasis>stack</emphasis> of components that help you deploy and
  manage server infrastructure. It is very scalable, fast, and relatively easy
  to get running. Read the following considerations before you start deploying
  the cluster with &salt;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
    deployment. Dedicate all the host resources to the &smaster; services. Do
    not install &ceph; on the host where you want to run &smaster;.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
    the &ceph; environment, &sminion; is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    &sminion;s need to correctly resolve the &smaster;'s host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name. Therefore, we recommend to set the &smaster;'s host name to
    <systemitem>salt</systemitem>.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  To deploy &ceph; cluster using &salt;, follow these steps:
 </para>
 <procedure>
  <step>
   <para>
    Install and register &cephos; together with &storage; 4 extension on each
    node of the cluster.
   </para>
  </step>
  <step>
   <para>
    Install &smaster;:
   </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
  </step>
  <step>
   <para>
    Install &sminion;s on each node you want to include in the &ceph; cluster:
   </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
  </step>
  <step>
   <para>
    Configure all &sminion;s to connect to the &smaster;. If your &smaster; is
    not reachable by the DNS name <literal>salt</literal>, you will need to
    edit the <filename>/etc/salt/minion</filename> file on each &sminion; and
    set
   </para>
<screen>master:<replaceable>dns_name_of_salt_master</replaceable></screen>
  </step>
  <step>
   <para>
    Set up salt keys.
   </para>
   <substeps>
    <step>
     <para>
      Enable and start all &sminion;s:
     </para>
<screen>&prompt.sminion;systemctl enable salt-minion
&prompt.sminion;systemctl start salt-minion</screen>
    </step>
    <step>
     <para>
      Enable and start &smaster;:
     </para>
<screen>&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
    </step>
    <step>
     <para>
      Accept the keys from the minions:
     </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
     <note>
      <para>
       Both all &sminion;s and &smaster; have a key to do a symmetric key-based
       communication. The master must accept the minions into its control while
       minions with other keys are not accepted.
      </para>
     </note>
    </step>
   </substeps>
  </step>
  <step>
   <para>
    Install the <systemitem class="resource">salt-ceph</systemitem> package:
   </para>
<screen>&prompt.smaster;zypper in salt-ceph</screen>
  </step>
  <step>
   <para>
    Distribute the <systemitem>ceph</systemitem> execution interface to the
    minions:
   </para>
<screen>&prompt.smaster;salt "*" saltutil.sync_all
&prompt.smaster;salt '*' pkg.install python-ceph-cfg</screen>
   <tip>
    <para>
     Use "*" in the <command>salt</command> command to specify all controlled
     minions.
    </para>
   </tip>
   <para>
    The <filename>ceph</filename> directory will be synchronized to each of
    your nodes. You can test the result with:
   </para>
<screen>&prompt.smaster;salt "*" ceph_cfg -d</screen>
   <para>
    If this returns no results, check and repeat the proceeding step.
   </para>
  </step>
  <step>
   <para>
    Create a directory on &smaster;:
   </para>
<screen>&prompt.smaster;mkdir /srv/salt/osceph/</screen>
  </step>
  <step>
   <para>
    Create a new <filename>ceph.conf</filename> file and copy it into the new
    directory <filename>/srv/salt/osceph/</filename>. A reasonable minimal
    <filename>ceph.conf</filename> file contains the following settings:
   </para>
<screen>[global]
fsid = 6de3876f-f4dd-4987-90bf-44fc1b2148d5
mon_initial_members = ses-1, ses-2, ses-3
mon_host = 192.168.12.11,192.168.12.12,192.168.12.13
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx</screen>
   <note>
    <para>
     You must generate a unique FSID for your &ceph; cluster by running
     <command>uuidgen</command> from a terminal, then use that FSID in the
     <literal>fsid</literal> line in <filename>ceph.conf</filename> .
    </para>
    <para>
     The <literal>mon_initial_members</literal> and <literal>mon_host</literal>
     lines need to contain the actual host names and IP addresses of the hosts
     you will initially deploy as monitors. The order of host names and IP
     addresses on these lines needs to match.
    </para>
   </note>
  </step>
  <step>
   <para>
    Install &ceph; on all nodes:
   </para>
<screen>&prompt.smaster;salt '*' pkg.install ceph</screen>
  </step>
  <step>
   <para>
    If you wish to deploy &rgw;, install the
    <systemitem>ceph-radosgw</systemitem> package on any node(s) you wish to
    use for this service. The following example command will install
    <systemitem>ceph-radosgw</systemitem> on any node whose host name starts
    with "rgw":
   </para>
<screen>&prompt.smaster;salt 'rgw*' pkg.install ceph-radosgw</screen>
  </step>
  <step>
   <tip>
    <para>
     Salt uses <emphasis>Salt State</emphasis> files (or SLS files) that
     represent the state in which the cluster should be. SLS files use specific
     simple syntax. You can either use one complex file, or split it into
     several logical parts and include them from the main file.
    </para>
   </tip>
   <para>
    Get the example SLS file, modify it to your needs, and copy it into the
    <filename>/srv/salt/</filename> directory:
   </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_buildup.sls /srv/salt</screen>
   <tip>
    <para>
     You can find the example SLS file with comments in
     <xref linkend="app.storage.sls"/>.
    </para>
   </tip>
   <important>
    <para>
     The example SLS file includes preset authentication keys and will deploy a
     monitor, an MDS and a &rgw; on any node you apply it to. It will
     additionally try to turn <filename>/dev/vdb</filename> and
     <filename>/dev/vdc</filename> into OSDs. Thus it is suitable for a small
     test cluster of virtual machines. For a very simple test deployment,
     delete the various "*rgw*" and "*mds*" sections, and adjust the "prepare_"
     and "activate_" sections to match the actual device names of the disks you
     wish to turn into OSDs. Make sure these disks are completely blank (with
     no partitions) before applying this state, or those disks will not become
     OSDs.
    </para>
   </important>
   <note>
    <para>
     Do not use the example SLS file in production. At a minimum you need to
     replace the authentication keys with keys of your own creation.
    </para>
   </note>
  </step>
  <step>
   <para>
    Configure &ceph; following the SLS file you modified:
   </para>
   <tip>
    <para>
     The <command>state.sls</command> directive instructs Salt to apply the
     language included in the specified file.
    </para>
   </tip>
<screen>&prompt.smaster;cd /srv/salt
&prompt.smaster;salt '*' state.sls cluster_buildup</screen>
   <tip>
    <para>
     If you experience problems during this step, it is probably because of the
     settings, such as number of disks. To adjust these, try editing your
     <filename>cluster_buildup.sls</filename> file.
    </para>
   </tip>
  </step>
  <step>
   <para>
    For a production deployment, you should create your own authentication keys
    for each type of service (MON, OSD, MDS, RGW), then use them in the salt
    state files, in place of the keys in the example file. To create a minimal
    set of keys for admin, MON and OSD, run the following commands against any
    one of the &sminion;s:
   </para>
<screen>&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=admin
&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=mon
&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=osd</screen>
   <para>
    Take the key from the output of each of these commands and use it for the
    'secret' value of the appropriate section in your state file (see the
    various 'keyring_*_save' sections of
    <filename>cluster_buildup.sls</filename> for examples).
   </para>
  </step>
  <step>
   <para>
    For a larger cluster, with many nodes with different roles (for example
    three MONs, several OSDs, at least one &rgw; and one MDS), it is best to
    create multiple salt state files, one for each role. Then apply those
    states to the nodes with the relevant roles, rather than using the same
    state file for all nodes.
   </para>
   <para>
    See <xref linkend="app.storage.sls"/> for an example set of SLS files.
   </para>
  </step>
 </procedure>
 <para>
  You should now have a running &ceph; setup.
 </para>
 <tip>
  <title>Uninstalling &ceph;</title>
  <para>
   To uninstall the &ceph; cluster, copy
   <filename>cluster_teardown.sls</filename> to <filename>/srv/salt</filename>
  </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_teardown.sls /srv/salt</screen>
  <para>
   and modify it to your needs. Then change to <filename>/srv/salt</filename>
   and run
  </para>
<screen>&prompt.smaster;salt '*' state.sls cluster_teardown</screen>
 </tip>
</chapter>
