<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.cluster_mntc">
 <title>Cluster Maintenance</title>
 <para/>
 <sect1 xml:id="storage.bp.cluster_mntc.rados_striping">
  <title>Sending Large Objects with <command>rados</command> Fails with Full OSD</title>

  <para>
   <command>rados</command> is a command line utility to manage RADOS object
   storage. For more information, see <command>man 8 rados</command>.
  </para>

  <para>
   If you send a large object to a &ceph; cluster with the
   <command>rados</command> utility, such as
  </para>

<screen>rados -p mypool put myobject /file/to/send</screen>

  <para>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance. RADOS has a 'striper' API that enables applications to
   stripe large objects over multiple OSDs. If you turn the striping feature on
   with the <option>--striper</option> option, you can prevent the OSD from
   filling up.
  </para>

<screen>rados --striper -p mypool put myobject /file/to/send</screen>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.calamari_addpool">
  <title>Creating and Deleting Pools from Calamari</title>

  <para>
   Apart from using the command line to create or delete pools (see
   <xref linkend="storage.bp.cluster_mntc.add_pool"/> and
   <xref linkend="storage.bp.cluster_mntc.del_pool"/>), you can do the same
   from within Calamari in a more comfortable user interface.
  </para>

  <para>
   To create a new pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png"/>
     </imageobject>
     </inlinemediaobject> in the right top.
    </para>
   </step>
   <step>
    <para>
     Enter a name for the new pool, and either change the number of replicas,
     number of placement groups, and the CRUSH ruleset, or leave them at
     default values.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png"/>
     </imageobject>
     </inlinemediaobject> to confirm, then <guimenu>Cancel</guimenu> the
     warning dialog.
    </para>
   </step>
   <step>
    <para>
     Now you can see the new pool in the list of all existing pools. You can
     verify the existence of the new pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>

  <para>
   To delete an existing pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     From the list of pools, choose the one to delete and click the related
     <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_rmpool_thrash.png"/>
     </imageobject>
     </inlinemediaobject>
    </para>
   </step>
   <step>
    <para>
     Confirm the deletion and <guimenu>Cancel</guimenu> the warning dialog.
    </para>
   </step>
   <step>
    <para>
     You can verify the deletion of the pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.mng_keyrings">
  <title>Managing Keyring Files</title>

  <para>
   When &ceph; runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user. If you do not specify a user name, Ceph will use
   <literal>client.admin</literal> as the default user name. If you do not
   specify a keyring, Ceph will look for a keyring via the
   <option>keyring</option> setting in the &ceph; configuration. For example,
   if you execute the <command>ceph health</command> command without specifying
   a user or keyring:
  </para>

<screen>ceph health</screen>

  <para>
   &ceph; interprets the command like this:
  </para>

<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>

  <para>
   <command>ceph-authtool</command> is a utility to create, view, and modify a
   &ceph; keyring file. A keyring file stores one or more &ceph; authentication
   keys and possibly an associated capability specification. Each key is
   associated with an entity name, of the form {client,mon,mds,osd}.name.
  </para>

  <para>
   To create a new <filename>keyring</filename> file in the current directory
   containing a key for <literal>client.example1</literal>:
  </para>

<screen>ceph-authtool -C -n client.example1 --gen-key keyring</screen>

  <para>
   To add a new key for <literal>client.example2</literal>, omit the
   <option>-C</option> option:
  </para>

<screen>ceph-authtool -n client.example2 --gen-key keyring</screen>

  <para>
   The <filename>keyring</filename> now has two entries:
  </para>

<screen>ceph-authtool -l keyring
 [client.example1]
     key = AQCQ04NV8NE3JBAAHurrwc2BTVkMGybL1DYtng==
 [client.example2]
     key = AQBv2INVWMqFIBAAf/4/H3zxzAsPBTH4jsN80w==</screen>

  <para>
   For more information on <filename>ceph-authtool</filename>, see its manual
   page <filename>man 8 ceph-authtool</filename>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.create_client_keys">
  <title>Creating Client Keys</title>

  <para>
   User management functionality provides &ceph; cluster administrators with
   the ability to create, update and delete users directly in the cluster
   environment.
  </para>

  <tip>
   <para>
    When you create or delete users in the &ceph; cluster, you may need to
    distribute keys to clients so that they can be added to keyrings.
   </para>
  </tip>

  <para>
   Adding a user creates a user name (TYPE.ID), a secret key and possibly
   capabilities included in the command you use to create the user. A user’s
   key enables the user to authenticate with the cluster. The user’s
   capabilities authorize the user to read, write, or execute on monitors,
   OSDs, or metadata servers.
  </para>

  <para>
   Authentication key creation usually follows cluster user creation. There are
   several ways to add a user. The most convenient seems to be using
  </para>

<screen>ceph auth get-or-create</screen>

  <para>
   It returns a keyfile format with the user name [in brackets] and the key. If
   the user already exists, this command simply returns the user name and key
   in the keyfile format. You may use the<option>-o
   <replaceable>filename</replaceable></option> option to save the output to a
   file.
  </para>

<screen>ceph auth get-or-create client.example1
 [client.example1]
    key = AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   You can verify that the client key was added to the cluster keyring:
  </para>

<screen>ceph auth list
    [...]
 client.example1
    key: AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   When creating client users, you may create a user with no capabilities. A
   user with no capabilities is useless beyond mere authentication, because the
   client cannot retrieve the cluster map from the monitor. However, you can
   create a user with no capabilities if you want to defer adding capabilities
   later using the <command>ceph auth caps</command> command.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>

  <para>
   Find more details in the related upstream documentation, see
   <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
   Management</link>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.revoke_client_keys">
  <title>Revoking Client Keys</title>

  <para>
   If you need to remove an already generated client key from the keyring file,
   use the <command>ceph auth del</command> command. To remove the key for user
   <literal>client.example1</literal> that we added in
   <xref linkend="storage.bp.cluster_mntc.create_client_keys"/>:
  </para>

<screen>ceph auth del client.example1</screen>

  <para>
   and check the deletion with <command>ceph auth list</command>.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells &ceph; how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given rule set, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written&mdash;you can increase their weight to
   have &ceph; write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage
    instead.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.sw_upg">
  <title>Upgrading Software</title>

  <para>
   Both &sls; and &storage; products are provided with regular package updates.
   To apply new updates to the whole cluster, you need to run
  </para>

<screen>sudo zypper dup</screen>

  <para>
   on all cluster nodes. Remember to upgrade all the monitor nodes first, and
   then all the OSD nodes one by one.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pgnum">
  <title>Increasing the Number of Placement Groups</title>

  <para>
   When creating a new pool, you specify the number of placement groups for the
   pool (see <xref linkend="ceph.pools.operate.add_pool"/>). After adding more
   OSDs to the cluster, you usually need to increase the number of placement
   groups as well for performance and data durability reasons. For each
   placement group, OSD and monitor nodes need memory, network and CPU at all
   times and even more during recovery. From which follows that minimizing the
   number of placement groups saves significant amounts of resources.
  </para>

  <warning>
   <title>Too High Value of <option>pg_num</option></title>
   <para>
    When changing the <option>pg_num</option> value for a pool, it may happen
    that the new number of placement groups exceeds the allowed limit. For
    example
   </para>
<screen>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
   <para>
    The limit prevents extreme placement group splitting, and is derived from
    the <option>mon_osd_max_split_count</option> value.
   </para>
  </warning>

  <para>
   To determine the right new number of placement groups for a resized cluster
   is a complex task. One approach is to continuously grow the number of
   placement groups up to the state when the cluster performance is optimal. To
   determine the new incremented number of placement groups, you need to get
   the value of the <option>mon_osd_max_split_count</option> parameter, and add
   it to the current number of placement groups. To give you a basic idea, take
   a look at the following script:
  </para>

<screen>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
 pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
 echo "current pg_num value: $pg_num, max increment: $max_inc"
 next_pg_num="$(($pg_num+$max_inc))"
 echo "allowed increment of pg_num: $next_pg_num"</screen>

  <para>
   After finding out the next number of placement groups, increase it with
  </para>

<screen>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pool">
  <title>Adding a Pool</title>

  <para>
   After you first deploy a cluster, &ceph; uses the default pools to store
   data. You can later create a new pool with
  </para>

<screen>ceph osd pool create</screen>

  <para>
   For more information on cluster pool creation, see
   <xref linkend="ceph.pools.operate.add_pool"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.del_pool">
  <title>Deleting a Pool</title>

  <para>
   By deleting a pool, you permanently destroy all data stored in that pool.
   You can delete a previously created pool with
  </para>

<screen>ceph osd pool delete</screen>

  <para>
   For more information on cluster pool deletion, see
   <xref linkend="ceph.pools.operate.del_pool"/>.
  </para>
 </sect1>
</chapter>
