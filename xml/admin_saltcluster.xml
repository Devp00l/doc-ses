<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.salt.cluster">
 <title>&salt; Cluster Administration</title>
 <para>
  After you deploy &ceph; cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>Adding New Cluster Nodes</title>
  <para>
   The procedure of adding new nodes to the cluster is almost identical to
   initial cluster node deployment described in <xref
    linkend="ceph.install.saltstack"/>:
  </para>
  <procedure>
   <step>
    <para>
     Install &cephos; on the new node, configure its network setting so that it
     resolves the &smaster; host name correctly, and install the
     <systemitem>salt-minion</systemitem> package:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     If the &smaster;'s host name is different from <literal>salt</literal>, edit
     <filename>/etc/salt/minion</filename> and add the following:
    </para>
<screen>master:<replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Run the discovery stage (stage 1).
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
    <para>
     The stage creates a new profile for the node. The previous configuration
     is not overwritten.
    </para>
   </step>
   <step>
    <para>
     Optionally change <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> to
     include a new profile with the new node's disks.
    </para>
   </step>
   <step>
    <para>
     Run the configuration stage to apply changes to the profile.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     The &salt; pillar will be updated during this stage.
    </para>
   </step>
  </procedure>
  <tip>
   <para>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, we recommend adding all the OSDs you intend to add at the same time.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>Removing and Reinstalling Cluster Nodes</title>

  <note>
   <title>Removing OSDs from Your Cluster</title>
   <para>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </para>
  </note>

  <para>
   You may want remove a role from your minion, to do so use the Stage 5
   command:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.stage.5</screen>

  <para>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </para>

  <para>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </para>

  <note>
   <title>Preserving Partitions Created by Other Methods</title>
   <para>
    Disk drives previously configured by other methods, such as
    <command>ceph-deploy</command>, may still contain partitions. &deepsea;
    will not automatically destroy these. The administrator must
    reclaim these drives.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. Putting
    your &salt; master into a death spiral is a real risk.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration, placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
   <listitem>
    <para>
     The example reactor file
     <filename>/usr/share/doc/packages/deepsea/reactor.conf</filename> must be
     copied to <filename>/etc/salt/master.d/reactor.conf</filename>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, change the last line in the
   <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   to
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="Deepsea.restart">
  <title>Restarting &ceph; services using &deepsea;</title>

  <para>
   When you install updates, specifically ceph-&lt;mon,osd etc&gt; you need to
   restart the services to make use of the recently installed version. To do
   so, run:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>

  <para>
   The script iterates over all roles you have configured in the following
   order: MON, OSD, MDS, RGW, IGW. To keep the downtime low and to find
   potential issues as early as possible, nodes are restarted sequentially. For
   example, only one monitoring node is restarted at a time. The command also
   waits for the cluster to recover if the cluster is in a degraded unhealthy
   state.
  </para>

  <note>
   <title>Watching the Restarting</title>
   <para>
    The process of restarting the cluster may take some time. You can watch the
    events by using the &salt; event bus by running:
   </para>
<screen>&prompt.smaster;salt-run state.event pretty=True</screen>
  </note>
 </sect1>
</chapter>
