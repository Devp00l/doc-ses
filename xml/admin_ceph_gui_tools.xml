<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.oa">
<!-- ============================================================== -->
 <title>Managing &ceph; Cluster with GUI Tools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &storage; currently provides you with two graphical tools to manage and
  monitor your &ceph; cluster:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="ceph.oa" xrefstyle="select: title"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="ceph.install.calamari" xrefstyle="select: title"/>
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="ceph.oa">
  <title>&oa;</title>

  <para>
   &oa; is a central storage management system which supports &ceph; storage
   cluster. With &oa; you can control everything from a central management
   interface. It is no longer necessary to be familiar with the inner workings
   of the &ceph; storage tools. Cluster management tasks can be carried out by
   either using &oa;'s intuitive Web interface, or via its REST API.
  </para>

  <sect2 xml:id="ceph.oa.install.pkgs">
   <title>Installing Required Packages</title>
   <para>
    While you can install and run &oa; on any existing &ceph; cluster node, we
    recommend to install it on the admin node. &oa; is included in the
    &storage; extension. To install the required packages, run
   </para>
<screen>sudo zypper in openattic openattic-gui openattic-module-ceph</screen>
   <tip>
    <para>
     &oa; will work correctly only if it is the only Web-based application on
     the specific host. Do not share the host with another Web application such
     as Calamari.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.oa.install.oa">
   <title>&oa; Initial Setup</title>
   <para>
    After the packages are installed, run the actual &oa; setup:
   </para>
<screen>sudo oaconfig install</screen>
   <para>
    <command>oaconfig install</command> will start a number of services,
    initialize the &oa; database, and scan the system for pools and volumes to
    include.
   </para>
   <para>
    By default, <command>oaconfig</command> creates an administrative user
    account <systemitem>openattic</systemitem>, with the same password as the
    user name. As a security precaution, we strongly recommend to change this
    password immediately:
   </para>
<screen>sudo oaconfig changepassword openattic
Changing password for user 'openattic'
Password: &lt;enter password>
Password (again): &lt;re-enter password>
Password changed successfully for user 'openattic'
</screen>
   <para>
    Now your &oa; storage system can be managed by the Web user interface.
   </para>
  </sect2>

  <sect2 xml:id="ceph.oa.install.webui">
   <title>&oa; Web User Interface</title>
   <para>
    &oa; can be managed using a Web user interface. Open a Web browser and
    navigate to http://www.example.org/openattic. To log in, use the default
    user name <emphasis>openattic</emphasis> and the corresponding password.
   </para>
   <figure>
    <title>&oa; Login Screen</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_login.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_login.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    The &oa; user interface is graphically divided into a top menu pane and a
    content pane.
   </para>
   <para>
    The right part of the top pane includes a link to the current user
    settings, and a <guimenu>Logout</guimenu> link. The rest of the top pane
    includes the main &oa; menu.
   </para>
   <para>
    The content pane changes depending on which item menu is activated. By
    default, a <guimenu>Dashboard</guimenu> is displayed showing general &ceph;
    cluster statistics.
   </para>
   <figure>
    <title>&oa; Dashboard</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_dashboard.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_dashboard.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="ceph.oa.webui.ceph">
   <title>&ceph; Related Tasks</title>
   <para>
    If you click <guimenu>Ceph</guimenu> in the main &oa; menu, a submenu with
    &ceph; related tasks appear. Currently, the following tasks are relevant:
    <guimenu>OSDs</guimenu>, <guimenu>RBDs</guimenu>, <guimenu>Pools</guimenu>,
    and <guimenu>CRUSH Map</guimenu>.
   </para>
   <sect3>
    <title>Common Web UI Features</title>
    <para>
     In &oa; you often work with <emphasis>lists</emphasis>&mdash;for example
     lists of pools, OSD nodes, or RBD devices. The following common widgets
     help you manage or adjust these list:
    </para>
    <para>
     Click <inlinemediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_widget_reload.png" width="1.2em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_widget_reload.png" width="1.2em" format="png"/>
     </imageobject>
     </inlinemediaobject> to refresh the list of items.
    </para>
    <para>
     Click <inlinemediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_widget_columns.png" width="1.3em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_widget_columns.png" width="1.3em" format="png"/>
     </imageobject>
     </inlinemediaobject> to display or hide individual table columns.
    </para>
    <para>
     Click <inlinemediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_widget_rows.png" width="1.8em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_widget_rows.png" width="1.8em" format="png"/>
     </imageobject>
     </inlinemediaobject> and select how many rows to display on a single page.
    </para>
    <para>
     Click inside <inlinemediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_widget_search.png" width="5em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_widget_search.png" width="5em" format="png"/>
     </imageobject>
     </inlinemediaobject> and filter the rows by the typing the string to
     search for.
    </para>
    <para>
     Use <inlinemediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_widget_pager.png" width="6em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_widget_pager.png" width="6em" format="png"/>
     </imageobject>
     </inlinemediaobject> to change the currently displayed page if the list
     spans across multiple pages.
    </para>
   </sect3>
   <sect3>
    <title>Listing OSD Nodes</title>
    <para>
     To list all available OSD nodes, select
     <menuchoice><guimenu>Ceph</guimenu><guimenu>OSDs</guimenu></menuchoice>.
    </para>
    <para>
     The list shows each OSD's name, host name, status, and weight.
    </para>
    <figure>
     <title>List of OSD nodes</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_osds.png" width="90%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_osds.png" width="90%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   <sect3>
    <title>Managing Pools</title>
    <tip>
     <title>More Information on Pools</title>
     <para>
      For more general information about &ceph; pools, refer to
      <xref linkend="ceph.pools"/>. For information specific to erasure coded
      pools, refer to <xref linkend="cha.ceph.erasure"/>.
     </para>
    </tip>
    <para>
     To list all available pools, select
     <menuchoice><guimenu>Ceph</guimenu><guimenu>Pools</guimenu></menuchoice>.
    </para>
    <para>
     The list shows each pool's name, ID, the percentage of used space, the
     number of placemen groups, replica size, type ('replicated' or 'erasure'),
     erasure code profile, and the crush ruleset.
    </para>
    <figure>
     <title>List of Pools</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_pools.png" width="90%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_pools.png" width="90%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     To view more detailed information about a pool, activate its check box in
     the very left column:
    </para>
    <figure>
     <title>Pool Details</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_pools_status.png" width="40%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_pools_status.png" width="40%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <sect4>
     <title>Deleting Pools</title>
     <para>
      To delete a pool or a group of pools, activate their check boxes in the
      very left column and click <guimenu>Delete</guimenu> in the top left of
      the pools table:
     </para>
     <figure>
      <title>Deleting Pools</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="oa_pools_delete.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="oa_pools_delete.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </sect4>
    <sect4>
     <title>Adding Pools</title>
     <para>
      To add a new pool, click <guimenu>Add</guimenu> in the top left of the
      pools table and do the following on the <guimenu>Create Ceph
      pool</guimenu> screen:
     </para>
     <figure>
      <title>Adding a New Pool</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="oa_pools_add.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="oa_pools_add.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <procedure>
      <step>
       <para>
        Enter the name of the new pool. Refer to
        <xref linkend="sysreq.naming"/> for naming limitations.
       </para>
      </step>
      <step>
       <para>
        Select the cluster that will store the new pool.
       </para>
      </step>
      <step>
       <para>
        Select the pool type. Pools can be either replicated or erasure coded.
       </para>
      </step>
      <step>
       <para>
        Specify the number of the pool's placement groups.
       </para>
      </step>
      <step>
       <para>
        For a replicated pool, specify the replica size.
       </para>
      </step>
      <step>
       <para>
        Confirm with <guimenu>Create</guimenu>.
       </para>
      </step>
     </procedure>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.calamari">
  <title>Calamari</title>

  <para>
   Calamari is a management and monitoring system for &ceph; storage cluster.
   It provides a Web user interface that makes &ceph; cluster monitoring very
   simple and handy.
  </para>

  <para>
   The Calamari installation procedure differs according to the used deployment
   procedure. If you deployed your &ceph; by using
   <command>ceph-deploy</command>, refer to
   <xref linkend="ceph-deploy.calamari.installation"/>. If you deployed your
   cluster by using &crow;, refer to
   <xref linkend="crowbar.calamari.installation"/>.
  </para>

  <sect2 xml:id="ceph-deploy.calamari.installation">
   <title>Installing Calamari with <command>ceph-deploy</command></title>
   <para>
    To install Calamari, do the following:
   </para>
   <procedure>
    <step>
     <para>
      Install the client part of Calamari:
     </para>
<screen>sudo zypper in romana</screen>
    </step>
    <step>
     <para>
      Initialize Calamari installation. You will be asked for superuser user
      name and password. These will be needed when logging in to the Web
      interface after the setup is complete.
     </para>
<screen>sudo calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):
Email address:
Password:
Password (again):
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <tip>
      <para>
       In order for Calamari to work correctly, the admin keyring needs to be
       installed on each monitor node:
      </para>
<screen>&prompt.cephuser;ceph-deploy admin mon1 mon2 mon3</screen>
      <para>
       where <replaceable>mon1</replaceable>, <replaceable>mon2</replaceable>,
       or <replaceable>mon3</replaceable> are the host names of the monitors.
      </para>
     </tip>
     <para>
      Now open your Web browser and point it to the host name/IP address of the
      server where you installed Calamari. Log in with the credentials you
      entered when installing the Calamari client. A welcome screen appears,
      instructing you to enter the <command>ceph-deploy calamari
      connect</command> command. Switch to the terminal on the Calamari host
      and enter the following command. Note that the <option>--master</option>
      option specifies the host name of the Calamari server to which all the
      cluster nodes connect to:
     </para>
<screen>&prompt.cephuser;ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
     <para>
      After the command is successfully finished, reload the Web browser. Now
      you can monitor your &ceph; cluster, OSDs, pools, etc.
     </para>
     <important>
      <para>
       The Calamari dashboard screen shows the current status of the cluster.
       This updates regularly, so any change to the cluster state&mdash;for
       example if a node goes offline&mdash;should be reflected in Calamari
       within a few seconds. The <guimenu>Health</guimenu> panel includes a
       timer to indicate how long it has been since Calamari last saw heartbeat
       information from the cluster. Normally, this will not be more than one
       minute old, but in certain failure cases, for instance when a network
       outage occurs or if the cluster loses quorum (that is if more than half
       of the monitor nodes are down), Calamari will no longer be able to
       determine cluster state. In this case, the <guimenu>Health</guimenu>
       panel will indicate that the last update was more than one minute ago.
       After too long time with no updates, Calamari displays a warning at the
       top of the screen "Cluster Updates Are Stale. The Cluster is not
       updating Calamari." If this occurs, the other status information
       Calamari presents will not be correct so you should investigate further
       to check the status of your storage nodes and network.
      </para>
     </important>
     <tip>
      <para>
       They may be leftovers of the previous Calamari setup on the system. If
       after logging in to the Calamari application some nodes are already
       joined or registered, run the following on the Calamari host to trigger
       a re-run of salt on all &ceph; nodes, which should clear up any odd
       state or missing bits and pieces.
      </para>
      <note>
       <title>Salt Installed by Default with Calamari</title>
       <para>
        Even though you deployed your &ceph; cluster by using
        <command>ceph-deploy</command>, salt is installed along with Calamari.
        The <command>salt</command> command is thus installed even though you
        did not install salt manually.
       </para>
      </note>
<screen>salt '*' state.highstate</screen>
      <para>
       We also recommend to remove files from the previous Calamari setup, such
       as state files, configuration files, or PostgreSQL database files. At
       minimum, remove the files in the following directories:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         <filename>/etc/calamari/</filename>
        </para>
       </listitem>
       <listitem>
        <para>
         <filename>/etc/salt/</filename>
        </para>
       </listitem>
       <listitem>
        <para>
         <filename>/etc/graphite/</filename>
        </para>
       </listitem>
       <listitem>
        <para>
         <filename>/var/*/salt/</filename>
        </para>
       </listitem>
       <listitem>
        <para>
         <filename>/var/lib/graphite/</filename>
        </para>
       </listitem>
       <listitem>
        <para>
         <filename>/var/lib/pgsql/</filename>
        </para>
       </listitem>
      </itemizedlist>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="crowbar.calamari.installation">
   <title>Installing Calamari by Using &crow;</title>
   <note>
    <title>Conflicts in Combination with Deployment using &crow;</title>
    <para>
     If you used &crow; to install &storage;, install Calamari on a different
     server than &crow; as &crow; uses the same port as Calamari (port 80).
    </para>
   </note>
   <para>
    Use the &crow; UI to deploy Calamari as described in
    <xref linkend="sec.depl.ceph.ceph"/>.
   </para>
  </sect2>
 </sect1>
</chapter>
