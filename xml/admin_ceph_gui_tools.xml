<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.oa">
<!-- ============================================================== -->
 <title>Managing &ceph; Cluster with GUI Tools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>


 <para>
 &storage; currently provides you with two graphical tools to manage and monitor your &ceph; cluster:
 </para>
 <itemizedlist>
 	<listitem>
 		<para><xref linkend="ceph.install.calamari" xreflabel="Calamari"/>.</para>
 	</listitem>
 	<listitem>
 		<para><xref linkend="ceph.oa.install" xreflabel="openAttic"/>.</para>
 	</listitem>
 </itemizedlist>
 <sect1 xml:id="ceph.install.calamari">
  <title>Calamari Installation</title>

  <para>
   Calamari is a management and monitoring system for &ceph; storage cluster.
   It provides a Web user interface that makes &ceph; cluster monitoring very
   simple and handy.
  </para>
  <para>
  The Calamari installation procedure differs according to the used deployment procedure. If you deployed your &ceph; by using <command>ceph-deploy</command>, refer to
  <xref linkend="ceph-deploy.calamari.installation"/>. If you deployed your cluster by using &crow;, refer to <xref linkend="crowbar.calamari.installation"/>.
  </para>
<sect2 xml:id="ceph-deploy.calamari.installation">
<title>Installing Calamari with <literal>&ceph;-deploy</literal></title>
   <para>
   To install Calamari, run the following commands as &rootuser;:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen># zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen># calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):  
Email address: 
Password: 
Password (again): 
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      In order for Calamari to work correctly, the admin keyring needs to be
      installed on each monitor node:
     </para>
<screen>ceph-deploy admin mon1 mon2 mon3</screen>
     <para>
      where <replaceable>mon1</replaceable>, <replaceable>mon2</replaceable>,
      or <replaceable>mon3</replaceable> are the host names of the monitors.
     </para>
    </tip>
    <para>
     Now open your Web browser and point it to the host name/IP address of the
     server where you installed Calamari. Log in with the credentials you
     entered when installing the Calamari client. A welcome screen appears,
     instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host and
     enter the following command. Note that the <option>--master</option>
     option specifies the host name of the Calamari server to which all the
     cluster nodes connect to:
    </para>
<screen>ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your &ceph; cluster, OSDs, pools, etc.
    </para>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state&mdash;for
      example if a node goes offline&mdash;should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw heartbeat
      information from the cluster. Normally, this will not be more than one
      minute old, but in certain failure cases, for instance when a network
      outage occurs or if the cluster loses quorum (that is if more than half
      of the monitor nodes are down), Calamari will no longer be able to
      determine cluster state. In this case, the <guimenu>Health</guimenu>
      panel will indicate that the last update was more than one minute ago.
      After too long time with no updates, Calamari displays a warning at the
      top of the screen "Cluster Updates Are Stale. The Cluster is not updating
      Calamari." If this occurs, the other status information Calamari presents
      will not be correct so you should investigate further to check the status
      of your storage nodes and network.
     </para>
    </important>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to trigger a
      re-run of salt on all &ceph; nodes, which should clear up any odd state
      or missing bits and pieces.
     </para>
     <note>
     	<title>Salt Installed by Default with Calamari</title>
     	<para>
     	Even though you deployed your &ceph; cluster by using <literal>&ceph;-deploy</literal>, salt is installed along with calamari. The salt command is thus installed even though you did not install salt manually.
     	</para>
     </note>     
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup, such
      as state files, configuration files, or PostgreSQL database files. At
      minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
  </sect2>
  <sect2 xml:id="crowbar.calamari.installation">
  <title>Installing Calamari by Using &crow;</title>
  <note>
   <title>Conflicts in combination with deployment by using &crow;</title>
   <para>
    If you used &crow; to install &storage;, install Calamari on a different
    server than &crow; as &crow; uses the same port as Calamari (port 80).
   </para>
  </note>
	<para>
	Use the &crow; UI to deploy Calamari as described in <xref linkend="sec.depl.ceph.ceph"/>.
	</para>
	</sect2>
 </sect1>
 <sect1 xml:id="ceph.oa.install">
  <title>OpenAttic Installation</title>
 
 <para>
  &oa; is a central storage management system which supports &ceph; storage
  cluster. With &oa; you can control everything from a central management
  interface. It is no longer necessary to be familiar with the inner workings
  of the &ceph; storage tools. Cluster management tasks can be carried out by
  either using &oa;'s intuitive Web interface, or via its REST API.
 </para>
  
  <sect2 xml:id="ceph.oa.install.pkgs">
   <title>Installing Required Packages</title>
   <para>
    While you can install and run &oa; on any existing &ceph; cluster node, we
    recommend to install it on the admin node. &oa; is included in the
    &storage; extension. To install the required packages, run
   </para>
<screen>sudo zypper in openattic openattic-gui openattic-module-ceph</screen>
   <tip>
    <para>
     &oa; will work correctly only if it is the only Web-based application on
     the specific host. Do not share the host with another Web application such
     as Calamari.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.oa.install.oa">
   <title>&oa; Initial Setup</title>
   <para>
    After the packages are installed, run the actual &oa; setup:
   </para>
<screen>sudo oaconfig install</screen>
   <para>
    <command>oaconfig install</command> will start a number of services,
    initialize the &oa; database, and scan the system for pools and volumes to
    include.
   </para>
   <para>
    By default, <command>oaconfig</command> creates an administrative user
    account <systemitem>openattic</systemitem>, with the same password as the
    user name. As a security precaution, we strongly recommend to change this
    password immediately:
   </para>
<screen>sudo oaconfig changepassword openattic
Changing password for user 'openattic'
Password: &lt;enter password>
Password (again): &lt;re-enter password>
Password changed successfully for user 'openattic'
</screen>
   <para>
    Now your &oa; storage system can be managed by the Web user interface.
   </para>
  </sect2>

  <sect2 xml:id="ceph.oa.install.webui">
   <title>Accessing &oa; Web User Interface</title>
   <para>
    &oa; can be managed using a Web user interface. Open a Web browser and
    navigate to http://www.example.org/openattic. To log in, use the default user name <emphasis>openattic</emphasis>
    and password: <emphasis>openattic</emphasis>.
   </para>
  </sect2>
 </sect1>
</chapter>
