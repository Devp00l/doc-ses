<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.monitor">
 <title>Determining Cluster State</title>
 <para>
  Once you have a running cluster, you may use the <command>ceph</command> tool
  to monitor your cluster. Determining the cluster state typically involves
  checking the status of OSD, monitor, placement group and metadata server.
  <remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </para>

<screen>&prompt.root;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   The &ceph; cluster returns one of the following health codes:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </para>
     <para>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<filename>/var/log/ceph/ceph-osd.*</filename>) may contain debugging
      information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, for example
  OSD_HOST_DOWN</term>
    <listitem>
     <para>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </para>
<screen>&prompt.root;ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      The utilization thresholds for <emphasis>backfillfull</emphasis>,
      <emphasis>nearfull</emphasis>, <emphasis>full</emphasis>, and/or
      <emphasis>failsafe_full</emphasis> are not ascending. In particular, we
      expect <emphasis>backfillfull</emphasis> &lt;
      <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt;
      <emphasis>full</emphasis>, and <emphasis>full</emphasis> &lt;
      <emphasis>failsafe_full</emphasis>. The thresholds can be adjusted with:
     </para>
<screen>&prompt.root;ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
&prompt.root;ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
&prompt.root;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>full</emphasis> threshold and
      is preventing the cluster from servicing writes. Utilization by pool can
      be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
     <para>
      The currently defined <emphasis>full</emphasis> ratio can be seen with:
     </para>
<screen>&prompt.root;ceph osd dump | grep full_ratio</screen>
     <para>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </para>
<screen>&prompt.root;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>backfillfull</emphasis>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Utilization by pool
      can be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>nearfull</emphasis>
      threshold. This is an early warning that the cluster is approaching full.
      Utilization by pool can be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      One or more cluster flags of interest has been set. With the exception of
      <emphasis>full</emphasis>, these flags can be set or cleared with:
     </para>
<screen>&prompt.root;ceph osd set <replaceable>flag</replaceable>
&prompt.root;ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         The cluster is flagged as full and cannot service writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr </term>
       <listitem>
        <para>
         Paused reads or writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs are not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <emphasis>down</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs that were previously marked <emphasis>out</emphasis> will not be
         marked back <emphasis>in</emphasis> when they start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs will not automatically be marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Recovery or data rebalancing is suspended.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing is disabled.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Cache tiering activity is suspended.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSD is not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Failure reports for this OSD will be ignored.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         If this OSD was previously marked <emphasis>out</emphasis>
         automatically after a failure, it will not be marked
         <emphasis>in</emphasis> when it starts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         If this OSD is down, it will not be automatically marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Per-OSD flags can be set and cleared with:
     </para>
<screen>&prompt.root;ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
&prompt.root;ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <!-- 2017-08-17 tbazant: next is
   http://docs.ceph.com/docs/master/rados/operations/health-checks/#old-crush-tunables
   -->
  </variablelist>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.root;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Watching a Cluster</title>

  <para>
   To watch the cluster’s ongoing events, open a new terminal and enter:
  </para>

<screen>&prompt.root;ceph -w</screen>

  <para>
   &ceph; will print each event. For example, a tiny &ceph; cluster consisting
   of one monitor, and two OSDs may print the following:
  </para>

<screen>cluster:
  id:     6586341d-4565-3755-a4fd-b50f51bee248
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum blueshark1,blueshark2,blueshark3
  mgr: blueshark3(active), standbys: blueshark2, blueshark1
  osd: 15 osds: 15 up, 15 in

data:
  pools:   8 pools, 340 pgs
  objects: 537 objects, 1985 MB
  usage:   23881 MB used, 5571 GB / 5595 GB avail
  pgs:     340 active+clean

io:
  client:   100 MB/s rd, 26256 op/s rd, 0 op/s wr</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Checking a Cluster's Usage Stats</title>

  <para>
   To check a cluster’s data usage and data distribution among pools, you can
   use the <command>df</command> option. It is similar to Linux
   <command>df</command>. Execute the following:
  </para>

<screen>&prompt.root;ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    55886G     55826G       61731M          0.11
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    testpool     1          0         0        17676G           0
    ecpool       2      4077M      0.01        35352G        2102
    test1        3          0         0        17676G           0
    rbd          4         16         0        17676G           3
    rbd1         5         16         0        17676G           3
    ecpool1      6      5708M      0.02        35352G        2871</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
     Capacity</link> for additional details.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: The maximum available space in the given
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Checking a Cluster's Status</title>

  <para>
   To check a cluster's status, execute the following:
  </para>

<screen>&prompt.root;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.root;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.root;ceph osd tree</screen>

  <para>
   &ceph; will print out a CRUSH tree with a host, its OSDs, whether they are
   up and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD nodes allowing
   the cluster to redistribute data to the newly available storage.
  </para>

  <para>
   If you cannot start an OSD because it is full, you may delete some data by
   deleting some placement group directories in the full OSD.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Checking Monitor Status</title>

  <para>
   If your cluster has multiple monitors (likely), you should check the monitor
   quorum status after you start the cluster before reading and/or writing
   data. A quorum must be present when multiple monitors are running. You
   should also check monitor status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.root;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.root;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
   OSDs and Placement Groups.</link>
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to
   <link
   xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing
   a Configuration at Runtime</link>for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
</chapter>
