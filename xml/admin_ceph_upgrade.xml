<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster, you need to have both the underlying
   &sls; and &storage; correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &ceph; monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; &rgw;s.
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>ceph osd stat</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.to4">
  <title>Upgrade from &storage; 2.1/3 to 4</title>

  <para>
   This section includes steps specific to upgrading from &storage; version
   2.1/3 to 4.
  </para>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 2.1 or 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 2.1 or 3 cluster to version 4, follow these steps
   on each cluster node:
  </para>

  <procedure>
   <step>
    <warning>
     <title>Do Not Run <command>zypper dup</command> or Reboot the Node</title>
     <para>
      After you prepare for the upgrade to &sls; 12 SP2 as suggested later in
      this step, do <emphasis>not</emphasis> run <command>zypper dup</command>
      or reboot the node as its &ceph; related services may not start
      correctly.
     </para>
    </warning>
    <para>
     Upgrade the current &sls; to version 12 SP2. Refer to
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_update_sle.html"/>
     for more information on supported upgrade methods.
    </para>
   </step>
   <step>
    <para>
     List all the active services with <command>zypper ls</command>.
    </para>
<screen>zypper ls
#| Alias                                      | Name | Enabled | Refresh | Type
-+--------------------------------------------+------+---------+---------+------
1| SUSE_Enterprise_Storage_3_x86_64           | ...  | Yes     | Yes     | ris
2| SUSE_Linux_Enterprise_Server_12_SP2_x86_64 | ...  | Yes     | Yes     | ris
[...]</screen>
    <para>
     Verify that services related to &sls; 12 SP2 are present and enabled.
    </para>
   </step>
   <step>
    <para>
     Remove the current &storage; service. You can do it as follows:
    </para>
<screen>sudo zypper rs <replaceable>ID</replaceable></screen>
   </step>
   <step>
    <para>
     Activate &storage; 4 service. You can use <command>yast2 add-on</command>.
    </para>
   </step>
   <step>
    <para>
     Refresh new software repositories:
    </para>
<screen>sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Install the upgrade helper package:
    </para>
<screen>sudo zypper in ses-upgrade-helper</screen>
   </step>
   <step>
    <para>
     Run the upgrade script:
    </para>
<screen>sudo upgrade-ses.sh</screen>
    <para>
     The script does the distribution upgrade of the node. After reboot, the
     node comes up with &sls; 12 SP2 and &storage; 4 running.
    </para>
   </step>
  </procedure>

  <sect2 xml:id="ceph.upgrade.2.1to3.iscsi_up">
   <title>iSCSI Gateways Upgrade</title>
   <para>
    For iSCSI gateways, consider the following points during upgrades:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Check that the <systemitem>lrbd</systemitem> service is enabled so that
      the iSCSI gateway configuration is applied on reboot.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrade the iSCSI gateway nodes after the monitor and OSD nodes.
     </para>
     <tip>
      <para>
       If an iSCSI gateway includes OSD or monitor processes on the same node,
       then upgrade and restart these processes before the system is rebooted
       into the new kernel.
      </para>
     </tip>
    </listitem>
    <listitem>
     <para>
      Check that the &ceph; cluster health is <literal>HEALTH_OK</literal> when
      proceeding with the iSCSI gateway upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      iSCSI initiators (clients) that require access to storage throughout the
      upgrade need to be configured with multi-path I/O (MPIO).
     </para>
     <itemizedlist mark="opencircle">
      <listitem>
       <para>
        Before rebooting or taking an iSCSI gateway node offline, manually
        disable the corresponding initiator MPIO device paths on the client.
       </para>
      </listitem>
      <listitem>
       <para>
        Once the gateway is back online, enable the client MPIO device path(s).
       </para>
      </listitem>
      <listitem>
       <para>
        For all gateway nodes exposing a given iSCSI target, care should be
        taken to ensure that no more than one iSCSI gateway node is offline
        (rebooted for kernel update) at any moment during the upgrade.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="ceph.upgrade.2.1to3.iscsi">
    <title>Updated Behavior for iSCSI Gateways</title>
    <itemizedlist>
     <listitem>
      <para>
       The <option>rbd_name</option> is a backward compatibility option for
       setting the backstore name to only use the name of the image. Starting
       from &storage; 3, the default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
            {
                "host": "igw1",
                "tpg": [
                    {
                        "image": "archive",
                        "rbd_name": "simple"
                    }
                ]
            }
        ]
    }
]</screen>
     </listitem>
     <listitem>
      <para>
       Likewise, <option>wwn_generate</option> will use the original scheme of
       target and image name for setting the <option>vpn_unit_serial</option>.
       The current default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>target</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"targets": [
  {
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "wwn_generate": "original"
  }</screen>
      <para>
       For more information, see
       <filename>/usr/share/doc/lrbd/README.migration</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       The <option>rgw_region_root_pool</option> is deprecated for federated
       &rgw; deployments in &storage; 3. Replace it with the new
       <option>rgw_zonegroup_root_pool</option> option in
       <filename>ceph.conf</filename>
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
</chapter>
