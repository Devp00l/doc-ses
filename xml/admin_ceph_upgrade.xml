<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!-- influenced by http://ceph.com/docs/master/install/upgrading-ceph/ -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous release
  to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster itself, you need to add a new version of
   the &storage; product to the existing operating system, and update the
   software to the latest versions on each cluster node. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &ceph; monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; &rgw;s.
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>ceph osd stat</screen>
  </tip>

  <procedure xml:id="ceph.upgrade.general_steps">
   <title>General Upgrade Steps</title>
   <step>
    <para>
     Check that the latest patches are applied to the installed &sls;.
    </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper patch</screen>
   </step>
   <step>
    <para>
     Install &storage; extension to which you are upgrading with
     <menuchoice><guimenu>&yast;</guimenu><guimenu>Software</guimenu><guimenu>Add-On
     Products</guimenu><guimenu>Add</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     After the new product is added, refresh the installed software
     repositories.
    </para>
<screen>sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Upgrade the installed software to the new version.
    </para>
<screen>sudo zypper dup</screen>
   </step>
   <step>
    <para>
     After the last &ceph; node is upgraded, check the cluster status.
    </para>
<screen>ceph health</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.2.1to3">
  <title>Upgrade from &storage; 2.1 to 3</title>

  <para>
   This section includes steps specific to upgrading &storage; version 2.1 to
   3.
  </para>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 2.1
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   The basic procedure for upgrading from &storage; 2.1 to &storage; 3 is to
   install the <systemitem>ses-upgrade-helper</systemitem> package and run the
   <command>upgrade-to-ses3.sh</command> script as &rootuser; on each cluster
   node (except the admin node) in succession. This can be done while the
   cluster is running. The upgrade involves a number of caveats that are
   described below.
  </para>

  <tip>
   <para>
    The <systemitem>ses-upgrade-helper</systemitem> package is available in the
    &storage; 3 repositories. Therefore, to use the
    <command>upgrade-to-ses3.sh</command> command, you first need to install
    required &storage; repositories. Refer to
    <xref linkend="ceph.upgrade.general_steps"/>.
   </para>
  </tip>

  <sect2 xml:id="ceph.upgrade.2.1to3.ceph_uid">
   <title>'ceph' User and Group</title>
   <para>
    As of &storage; 3, the &ceph; daemons (MON, OSD, MDS, RGW) newly run as the
    unprivileged user <systemitem class="username">ceph</systemitem>. This is a
    major change from all previous versions, when all of the daemons (except
    RGW, which was a special case) ran as &rootuser;.
   </para>
   <para>
    Since the <systemitem class="username">ceph</systemitem> user will probably
    already be present on all your nodes, you need to either delete it or
    rename it to something else before installing &storage; 3. We recommend
    renaming it to <systemitem class="username">cephadm</systemitem> on all
    nodes including the admin node. On the cluster nodes, the
    <command>upgrade-to-ses3.sh</command> script takes care of this for you. On
    the admin node, it can be done manually:
   </para>
<screen>sudo usermod -l cephadm ceph</screen>
   <tip>
    <para>
     The <command>usermod -l</command> command only renames the user and does
     nothing else. You will probably also need to set up a home directory for
     the &cephuser; user and modify the <filename>/etc/sudoers</filename> file
     using the <command>visudo</command> command.
    </para>
   </tip>
   <para>
    During the product upgrade, a new
    <systemitem class="username">ceph</systemitem> user and group are created
    with the default UID/GID 167 (if available). If UID/GID 167 are not
    available in your environment, the first available UID/GID will be used
    when creating the <systemitem class="username">ceph</systemitem> user and
    group. Since this can complicate migration of OSD disks from one node to
    another, it is good practice to choose (and impose) a cluster-wide UID/GID
    for the <systemitem class="username">ceph</systemitem> user and group. But
    this is only necessary if 167, the preferred UID/GID, is already used for
    some other user/group.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.varlibceph">
   <title>Change Ownership of <filename>/var/lib/ceph</filename></title>
   <para>
    The &ceph; daemons store their state in <filename>/var/lib/ceph</filename>.
    In &storage; version 1, 2 and 2.1 the daemons run as &rootuser;, and the
    <filename>/var/lib/ceph</filename> directory is owned by &rootuser;.
    Upgrading from any of these versions to &storage; 3 requires manually
    changing the ownership to ensure that the upgraded daemons can access this
    directory tree.
   </para>
   <para>
    Before upgrading the &ceph; related packages, make sure that
    <option>CEPH_AUTO_RESTART_ON_UPGRADE</option> is set to "no" in
    <filename>/etc/sysconfig/ceph</filename>.
   </para>
   <para>
    After upgrading the &ceph; related packages, do <emphasis>not</emphasis>
    restart <systemitem>ceph.target</systemitem>. Instead, follow these steps
    on each node:
   </para>
   <procedure>
    <step>
     <para>
      Stop &storage; 2.1 daemons:
     </para>
<screen>sudo systemctl stop ceph.target</screen>
    </step>
    <step>
     <para>
      Change the ownership of <filename>/var/lib/ceph</filename>:
     </para>
<screen>sudo chown -R ceph:ceph /var/lib/ceph</screen>
    </step>
    <step>
     <para>
      Start &storage; 3 daemons:
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
   <tip>
    <para>
     You do not need to perform this step manually if you are using the
     <command>upgrade-to-ses3.sh</command> script from the
     <systemitem>ses-upgrade-helper</systemitem> package.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.rgw">
   <title>&rgw; Instance Name Change</title>
   <para>
    In &storage; 2.1 and earlier, <command>ceph-deploy</command> prefixed &rgw;
    instance names with the string <literal>radosgw.</literal> and this prefix
    was hard-coded in the &rgw; &systemd; unit file.
   </para>
   <para>
    As of &storage; 3, this prefix is no longer hard-coded in the unit file.
   </para>
   <para>
    To ensure a smooth upgrade, it is necessary to disable the &rgw; service
    (using the instance name without the prefix) before the upgrade and
    re-enable it using the full instance name after the upgrade. In other
    words, before upgrading your &rgw; nodes, do the following:
   </para>
<screen>systemctl disable ceph-radosgw@instance_name.service</screen>
   <para>
    And after the upgrade, do:
   </para>
<screen>sudo systemctl enable ceph-radosgw@radosgw.instance_name.service</screen>
   <para>
    If you are using the <command>upgrade-to-ses3.sh</command> script from the
    <systemitem>ses-upgrade-helper</systemitem> package (recommended), this is
    taken care of. The script parses <filename>ceph.conf</filename> and runs
    these pre- and post-commands for you.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.rgwlog">
   <title>&rgw; Log File</title>
   <para>
    In &storage; 2.1, each &rgw; instance has a section named
    <literal>[client.radosgw.<replaceable>instance_name</replaceable>]</literal>
    in <filename>/etc/ceph/ceph.conf</filename>, where
    <replaceable>instance_name</replaceable> is the name of your &rgw;
    instance. This section contains &rgw;-specific configuration parameters.
    One of these parameters is
   </para>
<screen>log_file = /var/log/ceph-radosgw/ceph.client.radosgw.<replaceable>instance_name</replaceable>.log</screen>
   <para>
    In &storage; 3, &rgw; no longer logs to
    <filename>/var/log/ceph-radosgw</filename>. Instead, the log file with the
    same name is expected to be in <filename>/var/log/ceph</filename> along
    with the log files of the other &ceph; daemons.
   </para>
   <para>
    The <command>upgrade-to-ses3.sh</command> script removes this line from
    <filename>/etc/ceph/ceph.conf</filename> if it is present. If you are
    upgrading manually, you will need to do this yourself.
   </para>
   <para>
    After the upgrade, &rgw; instances will log to
    <filename>/var/log/ceph</filename>, along with the other &ceph; daemons.
    Their log files will be rotated by the standard &ceph; logrotate
    configuration file <filename>/etc/logrotate.d/ceph</filename>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.tunables">
   <title>CRUSH Tunables</title>
   <para>
    During the upgrade, you may see the cluster go into "HEALTH_WARN" state and
    complain about "legacy tunables". This is not dangerous, and can be
    rectified after the entire cluster is running &storage; 3.
   </para>
   <para>
    If your cluster is still complaining about "legacy tunables" after all
    nodes have been upgraded, you have three options:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Option 1: suppress the warning
     </para>
     <para>
      You can stay on legacy tunables and suppress the warning by adding the
      following option to your <filename>ceph.conf</filename> [mon] section:
     </para>
<screen>mon warn on legacy crush tunables = false</screen>
     <para>
      For the change to take effect, you will need to restart the monitors, or
      apply the option to running monitors with:
     </para>
<screen>sudo ceph tell mon.\* injectargs --no-mon-warn-on-legacy-crush-tunables</screen>
    </listitem>
    <listitem>
     <para>
      Option 2: conservative tunables setting
     </para>
     <para>
      Adjusting the tunables will result in some data movement, which can
      affect cluster performance. Choose a time when the cluster is not under
      heavy load, and run:
     </para>
<screen>sudo ceph osd crush tunables firefly</screen>
     <para>
      Although the "firefly" tunables profile may not be the 'latest and
      greatest', some supported clients and kernels may not be able to work
      with the newer tunables.
     </para>
    </listitem>
    <listitem>
     <para>
      Option 3: optimal tunables setting
     </para>
     <warning>
      <title>Not Supported</title>
      <para>
       Note that the optimal tunables setting is not recommended or supported,
       and you take such a step at your own risk.
      </para>
     </warning>
     <para>
      If you are certain that all your clients and kernels support the latest
      tunables&mdash;the client OS is either &sls; 12 SP1 or newer for the
      kernel-based client, or &storage; 3 or newer is installed on the client
      host for the user-space client&mdash;you can migrate the cluster to them
      by running:
     </para>
<screen>sudo ceph osd crush tunables optimal</screen>
     <tip>
      <para>
       Since adjusting CRUSH tunables can cause significant data movement
       within the cluster, consider doing it when the cluster's load is low.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.guid">
   <title>OSD Partition GUID Codes</title>
   <para>
    In &storage; 3, &ceph; relies on partition GUID codes to set the right
    permissions on OSD data and journal devices on each boot. In &storage; 2.1,
    <command>ceph-deploy</command> sets these partition GUID codes correctly.
    However, it is possible that pre-existing partitions were used and in this
    case the partition GUID codes might be wrong. (This does not pose a problem
    in &storage; 2.1 because the OSD daemons run as &rootuser;.)
   </para>
   <para>
    If you think this may apply in your case&mdash;for example, you put OSD
    journals on a separate disk and you set them up manually&mdash;see
    <xref linkend="bp.osd_on_exisitng_partitions"/> for details on
    verifying/fixing the partition GUID codes before you upgrade your cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.calamari">
   <title>Calamari Server Upgrade</title>
   <para>
    To upgrade the Calamari server, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Stop &smaster; on the node where Calamari is installed:
     </para>
<screen>sudo rcsalt-master stop</screen>
    </step>
    <step>
     <para>
      Check that there are no stale <systemitem>salt-master</systemitem>
      processes running:
     </para>
<screen>ps -e | grep salt-master</screen>
    </step>
    <step>
     <para>
      Start &smaster;:
     </para>
<screen>sudo rcsalt-master start</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.iscsi_up">
   <title>iSCSI Gateways Upgrade</title>
   <para>
    For iSCSI gateways, consider the following points during upgrades:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Check that the <systemitem>lrbd</systemitem> service is enabled so that
      the iSCSI gateway configuration is applied on reboot.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrade the iSCSI gateway nodes after the monitor and OSD nodes.
     </para>
     <tip>
      <para>
       If an iSCSI gateway includes OSD or monitor processes on the same node,
       then upgrade and restart these processes before the system is rebooted
       into the new kernel.
      </para>
     </tip>
    </listitem>
    <listitem>
     <para>
      Check that the &ceph; cluster health is <literal>HEALTH_OK</literal> when
      proceeding with the iSCSI gateway upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      iSCSI initiators (clients) that require access to storage throughout the
      upgrade need be configured with multi-path I/O (MPIO).
     </para>
     <itemizedlist mark="opencircle">
      <listitem>
       <para>
        Before rebooting or taking an iSCSI gateway node offline, manually
        disable the corresponding initiator MPIO device paths on the client.
       </para>
      </listitem>
      <listitem>
       <para>
        Once the gateway is back online, enable the client MPIO device path(s).
       </para>
      </listitem>
      <listitem>
       <para>
        For all gateway nodes exposing a given iSCSI target, care should be
        taken to ensure that no more than one iSCSI gateway node is offline
        (rebooted for kernel update) at any moment during the upgrade.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="ceph.upgrade.2.1to3.iscsi">
    <title>Updated Behavior for iSCSI Gateways</title>
    <itemizedlist>
     <listitem>
      <para>
       The <option>rbd_name</option> is a backward compatibility option for
       setting the backstore name to only use the name of the image. Starting
       from &storage; 3, the default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
            {
                "host": "igw1",
                "tpg": [
                    {
                        "image": "archive",
                        "rbd_name": "simple"
                    }
                ]
            }
        ]
    }
]</screen>
     </listitem>
     <listitem>
      <para>
       Likewise, <option>wwn_generate</option> will use the original scheme of
       target and image name for setting the <option>vpn_unit_serial</option>.
       The current default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>target</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"targets": [
  {
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "wwn_generate": "original"
  }</screen>
      <para>
       For more information, see
       <filename>/usr/share/doc/lrbd/README.migration</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       The <option>rgw_region_root_pool</option> is deprecated for federated
       &rgw; deployments in &storage; 3. Replace it with the new
       <option>rgw_zonegroup_root_pool</option> option in
       <filename>ceph.conf</filename>
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.upgrade.2.1to3.admin_node">
   <title>The Admin Node</title>
   <para>
    In addition to upgrading the cluster nodes using the
    <command>upgrade-to-ses3.sh</command> script, it is also necessary to
    upgrade the admin node to get the latest versions of tools and services,
    such as <command>ceph-deploy</command> or Calamari.
   </para>
   <para>
    There is no special procedure for upgrading the admin node. Just register
    the &storage; 3 product and run <command>zypper dup</command>.
   </para>
  </sect2>
 </sect1>
</chapter>
