<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; from the previous
  release(s) to version &productnumber;.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package> ,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Consider the following items before starting the upgrade procedure:
  </para>

  <variablelist>
   <varlistentry xml:id="upgrade.order">
    <term>Upgrade Order</term>
    <listitem>
     <para>
      Before upgrading the &ceph; cluster, you need to have both the underlying
      &sls; and &productname; correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example &ceph;
      &rgw;s depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
      upgrading in this order:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        &mon;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mgr;s
       </para>
      </listitem>
      <listitem>
       <para>
        &osd;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mds;s
       </para>
      </listitem>
      <listitem>
       <para>
        &rgw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &igw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha;
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Delete Unnecessary Operating System Snapshots</term>
    <listitem>
     <para>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Check Cluster Health</term>
    <listitem>
     <para>
      We recommend to check the cluster health before starting the upgrade
      procedure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Upgrade One by One</term>
    <listitem>
     <para>
      We recommend upgrading all the daemons of a specific type&mdash;for
      example all monitor daemons or all OSD daemons&mdash;one by one to ensure
      that they are all on the same release. We also recommend that you upgrade
      all the daemons in your cluster before you try to exercise new
      functionality in a release.
     </para>
     <para>
      After all the daemons of a specific type are upgraded, check their
      status.
     </para>
     <para>
      Ensure each monitor has rejoined the quorum after all monitors are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph mon stat</screen>
     <para>
      Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Set <option>require-osd-release luminous</option> Flag</term>
    <listitem>
     <para>
      When the last OSD is upgraded to &productname; 5, the monitor nodes will
      detect that all OSDs are running the 'luminous' version of &ceph; and
      they may complain that the <option>require-osd-release luminous</option>
      osdmap flag is not set. In that case, you need to set this flag manually
      to acknowledge that&mdash;now that the cluster has been upgraded to
      'luminous'&mdash;it cannot be downgraded back to &ceph; 'jewel'. Set the
      flag by running the following command:
     </para>
<screen>&prompt.cephuser;ceph osd require-osd-release luminous</screen>
     <para>
      After the command completes, the warning disappears.
     </para>
     <para>
      On fresh installs of &productname; 5, this flag is set automatically when
      the &ceph; monitors create the initial osdmap, so no end user action is
      needed.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds.migrate.osd.encrypted">
  <title>Encrypting OSDs during Upgrade</title>

  <para>
   Since &productname; 5, OSDs are by default deployed using &bluestore;
   instead of &filestore;. Although &bluestore; supports encryption, &osd;s are
   deployed unencrypted by default. The following procedure describes steps to
   encrypt OSDs during the upgrade process. Let us assume that both data and
   WAL/DB disks to be used for OSD deployment are clean with no partitions. If
   the disk were previously used, wipe them following the procedure described
   in <xref linkend="deploy.wiping.disk"/>.
  </para>

  <important>
   <title>One OSD at a Time</title>
   <para>
    You need to deploy encrypted OSDs one by one, not simultaneously. The
    reason is that OSD's data is drained, and the cluster goes through several
    iterations of rebalancing.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Determine the <option>bluestore block db size</option> and
     <option>bluestore block wal size</option> values for your deployment and
     add them to the
     <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
     file on the &smaster;. The values need to be specified in bytes.
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     For more information on customizing the <filename>ceph.conf</filename>
     file, refer to <xref linkend="ds.custom.cephconf"/>.
    </para>
   </step>
   <step>
    <para>
     Run &deepsea; Stage 3 to distribute the changes:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Verify that the <filename>ceph.conf</filename> file is updated on the
     relevant OSD nodes:
    </para>
<screen>
&prompt.sminion;cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     Edit the *.yml files in the
     <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename>
     directory that are relevant to the OSDs you are encrypting. Double check
     their path with the one defined in the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> file to ensure
     that you modify the correct *.yml files.
    </para>
    <important>
     <title>Long Disk Identifiers</title>
     <para>
      When identifying OSD disks in the
      <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>
      files, use long disk identifiers.
     </para>
    </important>
    <para>
     An example of an OSD configuration follows. Note that because we need
     encryption, the <option>db_size</option> and <option>wal_size</option>
     options are removed:
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     Deploy the new &blockstore; OSDs with encryption by running &deepsea;
     Stages 2 and 3:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    <para>
     You can watch the progress with <command>ceph -s</command> or
     <command>ceph osd tree</command>. It is critical that you let the cluster
     rebalance before repeating the process on the next OSD node.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>Upgrade from &productname; 5 to 6</title>

  <important xml:id="u4to5.softreq">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP3
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 5
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order, based on their assigned role as recommended by &ceph; upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent the prescribed order from being
      violated if a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply, for example, new kernel versions. To
      reduce waiting I/O operations, we recommend declining incoming requests
      for the duration of the upgrade process.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Since &ceph; Luminous, the <option>osd crush location</option>
      configuration option is no longer supported. Update your &deepsea;
      configuration files to use <command>crush location</command> before
      upgrading.
     </para>
    </listitem>
    <listitem>
     <para>
      There are two ways to obtain &cephos; and &productname; &productnumber;
      update repositories:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If your cluster nodes are registered with SUSEConnect and use SCC/SMT,
        you will use the <command>zypper migration</command> method and the
        update repositories will be assigned automatically.
       </para>
      </listitem>
      <listitem>
       <para>
        If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
        Media-ISO or other package source, you will use the <command>zypper
        dup</command> method. In this case, you need to add the following
        repositories to all cluster nodes manually: SLE15-SP1 Base, SLE15-SP1
        Update, SES6 Base, and SES6 Update. You can do so using the
        <command>zypper</command> command. First remove all existing software
        repositories, then add the required new ones, and finally refresh the
        repositories sources:
       </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Products/Storage/6/x86_64/product/ SES6-POOL
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Updates/Storage/6/x86_64/update/ SES6-UPDATES
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Products/SLE-SERVER/15-SP1/x86_64/product/ SLES15-SP1-POOL
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Updates/SLE-SERVER/15-SP1/x86_64/update/ SLES15-SP1-UPDATES
&prompt.root;zypper ref
</screen>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &productname; 5 cluster to version &productnumber;, follow
   these steps:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the &smaster; node to &cephos; and &productname; &productnumber;.
     Depending on your upgrade method, use either <command>zypper
     migration</command> or <command>zypper dup</command>.
    </para>
   </step>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.cephuser;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     If your cluster nodes are <emphasis role="bold">not</emphasis> registered
     with SUSEConnect and do not use SCC/SMT, you will use the <command>zypper
     dup</command> method. Change your Pillar data in order to use the
     different strategy. Edit
    </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
    <para>
     and add the following line:
    </para>
<screen>upgrade_init: zypper-dup</screen>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     Update your Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     See <xref linkend="ds.minion.targeting"/> for details about &sminion;s
     targeting.
    </para>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Upgrade &sminion;s:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to
     <xref linkend="ds.minion.targeting"/> of <xref linkend="ds.depl.stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <para>
     Refer to <xref linkend="ceph.maintenance.upgrade_details" /> for more
     information.
    </para>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrading to &productname; 5, &filestore; OSDs need approximately
    five minutes longer to start as the OSD will do a one-off conversion of its
    on-disk files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade&mdash;you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, and &sls;, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>OSD Migration to &bluestore;</title>
   <para>
    OSD &bluestore; is a new back end for the OSD daemons. It is the default
    option since &productname; 5. Compared to &filestore;, which stores objects
    as files in an XFS file system, &bluestore; can deliver increased
    performance because it stores objects directly on the underlying block
    device. &bluestore; also enables other features, such as built-in
    compression and EC overwrites, that are unavailable with &filestore;.
   </para>
   <para>
    Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a &bluestore; OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on faster/different
    media.
   </para>
   <para>
    In SES5, both &filestore; and &bluestore; are supported and it is possible
    for &filestore; and &bluestore; OSDs to co-exist in a single cluster.
    During the &productname; upgrade procedure, &filestore; OSDs are not
    automatically converted to &bluestore;. Be aware that the
    &bluestore;-specific features will not be available on OSDs that have not
    been migrated to &bluestore;.
   </para>
   <para>
    Before converting to &bluestore;, the OSDs need to be running &productname;
    5. The conversion is a slow process as all data gets re-written twice.
    Though the migration process can take a long time to complete, there is no
    cluster outage and all clients can continue accessing the cluster during
    this period. However, do expect lower performance for the duration of the
    migration. This is caused by rebalancing and backfilling of cluster data.
   </para>
   <para>
    Use the following procedure to migrate &filestore; OSDs to &bluestore;:
   </para>
   <tip>
    <title>Turn Off Safety Measures</title>
    <para>
     &salt; commands needed for running the migration are blocked by safety
     measures. In order to turn these precautions off, run the following
     command:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      Migrate hardware profiles:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.policy</screen>
     <para>
      This runner migrates any hardware profiles currently in use by the
      <filename>policy.cfg</filename> file. It processes
      <filename>policy.cfg</filename>, finds any hardware profile using the
      original data structure, and converts it to the new data structure. The
      result is a new hardware profile named
      'migrated-<replaceable>original_name</replaceable>'.
      <filename>policy.cfg</filename> is updated as well.
     </para>
     <para>
      If the original configuration had separate journals, the &bluestore;
      configuration will use the same device for the 'wal' and 'db' for that
      OSD.
     </para>
    </step>
    <step>
     <para>
      &deepsea; migrates OSDs by setting their weight to 0 which 'vacuums' the
      data until the OSD is empty. You can either migrate OSDs one by one, or
      all OSDs at once. In either case, when the OSD is empty, the
      orchestration removes it and then re-creates it with the new
      configuration.
     </para>
     <tip>
      <title>Recommended Method</title>
      <para>
       Use <command>ceph.migrate.nodes</command> if you have a large number of
       physical storage nodes or almost no data. If one node represents less
       than 10% of your capacity, then the
       <command>ceph.migrate.nodes</command> may be marginally faster moving
       all the data from those OSDs in parallel.
      </para>
      <para>
       If you are not sure about which method to use, or the site has few
       storage nodes (for example each node has more than 10% of the cluster
       data), then select <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        To migrate OSDs one at a time, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        To migrate all OSDs on each node in parallel, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       As the orchestration gives no feedback about the migration progress, use
      </para>
<screen>&prompt.cephuser;ceph osd tree</screen>
      <para>
       to see which OSDs have a weight of zero periodically.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    After the migration to &bluestore;, the object count will remain the same
    and disk usage will be nearly the same.
   </para>
  </sect2>

  <sect2 xml:id="ceph.maintenance.upgrade_details">
   <title>Details on the <command>salt <replaceable>target</replaceable> ceph.maintenance.upgrade</command> Command</title>
   <para>
    During an upgrade via <command>salt <replaceable>target</replaceable>
    ceph.maintenance.upgrade</command>, &deepsea; applies all available
    updates/patches on all servers in the cluster in parallel without rebooting
    them. After these updates/patches are applied, the actual upgrade begins:
   </para>
   <procedure>
    <step>
     <para>
      The admin node is upgraded to &cephos;. This also upgrades the
      <package>salt-master</package> and <package>deepsea</package> packages.
     </para>
    </step>
    <step>
     <para>
      All &sminion;s are upgraded to a version that corresponds to the
      &smaster;.
     </para>
    </step>
    <step>
     <para>
      The migration is performed sequentially on all cluster nodes in the
      recommended order (the &mon;s first, see
      <xref linkend="upgrade.order" />) using the preferred method. As a
      consequence, the <package>ceph</package> package is upgraded.
     </para>
    </step>
    <step>
     <para>
      After updating all &mon;s, their services are restarted but the nodes are
      <emphasis role="bold">not rebooted</emphasis>. This way we ensure that
      all running &mon;s have identical version.
     </para>
     <important>
      <title>Do Not Reboot Monitor Nodes</title>
      <para>
       If the cluster monitor nodes host OSDs, <emphasis role="bold">do not
       reboot</emphasis> the nodes during this stage because the shared OSDs
       will not join the cluster after the reboot.
      </para>
     </important>
    </step>
    <step>
     <para>
      All the remaining cluster nodes are updated and rebooted in the
      recommended order.
     </para>
    </step>
    <step>
     <para>
      After all nodes are on the same patch-level, the following command is
      run:
     </para>
<screen><command>ceph require osd release <replaceable>RELEASE</replaceable></command></screen>
    </step>
   </procedure>
   <para>
    In case this process is interrupted by an accident or intentionally by the
    administrator, <emphasis role="bold">never reboot</emphasis> the nodes
    manually because after rebooting the first OSD node and OSD daemon, it will
    not be able to join the cluster anymore.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>Upgrade from &productname; 4 to 6</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Direct upgrade from &productname; 4 to 6 is not supported. First you need to
   upgrade &productname; 4 to 5 following the instructions in
   <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/cha_ceph_upgrade.html"/>.
   Then upgrade &productname; 5 to 6 as described in
   <xref linkend="ceph.upgrade.4to5" />.
  </para>
 </sect1>
</chapter>
