<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.install">
 <title>Installation of Basic &ceph; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter outlines procedures to deploy the &ceph; cluster. Currently we
  support the following methods of deployment:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="ceph.install.ceph-deploy"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="ceph.install.crowbar"/>
   </para>
  </listitem>
 </itemizedlist>
 <important>
  <title>Do Not Mix Installation Methods</title>
  <para>
   You cannot mix the supported installation methods. For example if you decide
   to deploy the cluster with &crow;, you cannot later make changes to its
   settings with <command>ceph-deploy</command> and vice versa.
  </para>
 </important>
 <sect1 xml:id="ceph.install.ceph-deploy">
  <title>Deploying with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to ease the way you
   deploy &ceph; cluster in small scale setups.
  </para>

  <sect2 xml:id="ceph.install.ceph-deploy.layout">
   <title>&ceph; Layout</title>
   <para>
    For testing purposes, a minimal &ceph; cluster can be made to run on a
    single node. However, in a production setup we recommend using at least
    four nodes: one admin node and three cluster nodes, each running one
    monitor daemon and some number of object storage daemons (OSDs).
   </para>
   <figure>
    <title>Minimal &ceph; Setup</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <para>
     Although &ceph; nodes can be virtual machines, real hardware is strongly
     recommended for the production environment.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.network">
   <title>Network Recommendations</title>
   <para>
    The network environment where you intend to run &ceph; should ideally be a
    bonded set of at least two network interfaces that is logically split into
    a public part and trusted internal part using VLANs. The bonding mode is
    recommended to be 802.3ad when possible to provide maximum bandwidth and
    resiliency.
   </para>
   <para>
    The public VLAN serves for providing the service to the customers, the
    internal part provides for the authenticated &ceph; network communication.
    The main reason is that although &ceph; authentication and protection
    against attacks once secret keys are in place, the messages used to
    configure these keys may be transferred open and are vulnerable.
   </para>
   <tip>
    <title>Nodes Configured via DHCP</title>
    <para>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various &ceph; daemons start. If this happens, the &ceph; MONs and OSDs
     will not start correctly (running <command>systemctl status
     ceph\*</command> will result in "unable to bind" errors), and Calamari may
     be unable to display graphs. To avoid this issue, we recommend increasing
     the DHCP client timeout to at least 30 seconds on each node in your
     storage cluster. This can be done by changing the following settings on
     each node:
    </para>
    <para>
     In <filename>/etc/sysconfig/network/dhcp</filename> set
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     In <filename>/etc/sysconfig/network/config</filename> set
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.eachnode">
   <title>Preparing Each &ceph; Node</title>
   <para>
    Before deploying the &ceph; cluster, apply the following steps for each
    &ceph; node as &rootuser;:
   </para>
   <procedure>
    <step>
     <para>
      Install &sle; 12 SP2 and add the &storage; extension.
     </para>
     <figure>
      <title>&storage; Extension Selection</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="install_extension.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="install_extension.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      On the <guimenu>Installation Settings</guimenu> screen, click
      <guimenu>Software</guimenu>. On the <guimenu>Software Selection and
      System Tasks</guimenu> screen, there are several tasks related to
      &storage;. For OSDs, monitors, or the admin server, be sure to choose
      &storage; server packages and confirm with <guimenu>OK</guimenu>.
     </para>
     <figure>
      <title>&storage; Related Installation Tasks</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="install_soft_sel.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="install_soft_sel.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      For more information on the extension installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     </para>
     <important>
      <title>&ceph; Runs as <systemitem>ceph</systemitem> User and Group</title>
      <para>
       In &storage; version 3.0, &ceph; runs as <systemitem>ceph</systemitem>
       user and group, both with default UID/GID 167. If UID/GID 167 is not
       available across your cluster, you may need to create the
       <systemitem>ceph</systemitem> user and group manually. For more
       information, see <xref linkend="ceph.upgrade.2.1to3.ceph_uid"/>.
      </para>
     </important>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is on, either turn it off with
     </para>
<screen>sudo /sbin/SuSEfirewall2 off</screen>
     <para>
      or, if you want to keep it on, enable the appropriate set of ports. You
      can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Make sure that network settings are correct: each &ceph; node needs to
      route to all other &ceph; nodes, and each &ceph; node needs to resolve
      all other &ceph; nodes by their short host names (without the domain
      suffix). If these two conditions are not met, &ceph; fails.
     </para>
    </step>
    <step>
     <para>
      Install and set up NTP&mdash;the time synchronization tool. We strongly
      recommend using NTP within the &ceph; cluster. The reason is that &ceph;
      daemons pass critical messages to each other, which must be processed
      before daemons reach a timeout threshold. If the clocks in &ceph;
      monitors are not synchronized, it can lead to a number of anomalies, such
      as daemons ignoring received messages.
     </para>
     <para>
      Even though clock drift may still be noticeable with NTP, it is not yet
      harmful.
     </para>
     <para>
      To install NTP, run the following:
     </para>
<screen>sudo zypper in ntp yast2-ntp-client</screen>
     <para>
      To configure NTP, go to <menuchoice><guimenu>&yast;</guimenu>
      <guimenu>Network Services</guimenu> <guimenu>NTP
      Configuration</guimenu></menuchoice>. Make sure to enable the NTP service
      (<command>systemctl enable ntpd.service &amp;&amp; systemctl start
      ntpd.service</command>). Find more detailed information on NTP in the
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">
      SLES Administration Guide</link>.
     </para>
    </step>
    <step>
     <para>
      Install SSH server. &ceph; uses SSH to log in to all cluster nodes. Make
      sure SSH is installed (<command>zypper in openssh</command>) and enabled
      (<command>systemctl enable sshd.service &amp;&amp; systemctl start
      sshd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Add a &cephuser; user account, and set password for it. The admin node
      will log in to &ceph; nodes as this particular &cephuser; user .
     </para>
<screen>sudo useradd -m &cephuser.plain; &amp;&amp; passwd &cephuser.plain;</screen>
    </step>
    <step>
     <para>
      The admin node needs to have passwordless SSH access to all &ceph; nodes.
      When <command>ceph-deploy</command> logs in to a &ceph; node as a
      &cephuser; user, this user must have passwordless <command>sudo</command>
      privileges.
     </para>
     <para>
      Edit the <filename>/etc/sudoers</filename> file (with
      <command>visudo</command>) and add the following line to add the
      <command>sudo</command> command for the &cephuser; user:
     </para>
<screen>&cephuser.plain; ALL = (root) NOPASSWD:ALL</screen>
     <tip>
      <title>Disable <literal>requiretty</literal></title>
      <para>
       You may receive an error while trying to execute
       <command>ceph-deploy</command> commands. If
       <literal>requiretty</literal> is set by default, disable it by executing
       <command>sudo visudo</command> and locate the <literal>Defaults
       requiretty</literal> setting. Change it to<literal>
       Defaults:&cephuser.plain; !requiretty</literal> to ensure that
       <command>ceph-deploy</command> can connect using the &cephuser; user and
       execute commands with <command>sudo</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      On the admin node, become the &cephuser; user, and enable passwordless
      SSH access to all other &ceph; nodes:
     </para>
<screen>su - &cephuser.plain;
&prompt.cephuser;ssh-keygen</screen>
     <para>
      You will be asked several questions. Leave the values at their defaults,
      and the passphrase empty.
     </para>
     <para>
      Copy the key to each &ceph; node:
     </para>
<screen>ssh-copy-id &cephuser.plain;@<replaceable>node1</replaceable>
ssh-copy-id &cephuser.plain;@<replaceable>node2</replaceable>
ssh-copy-id &cephuser.plain;@<replaceable>node3</replaceable></screen>
     <tip>
      <title>Running <command>ceph-deploy</command> from a Different User Account Than &cephuser;</title>
      <para>
       It is possible to run the <command>ceph-deploy</command> command even if
       you are logged in as a different user than &cephuser;. For this purpose,
       you need to set up an SSH alias in your
       <filename>~/.ssh/config</filename> file:
      </para>
<screen>[...]
Host ceph-node1
  Hostname ceph-node1
  User &cephuser.plain;</screen>
      <para>
       After this change, <command>ssh ceph-node1</command> automatically uses
       the &cephuser; user to log in.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.purge">
   <title>Cleaning Previous &ceph; Environment</title>
   <para>
    If at any point during the &ceph; deployment you run into trouble and need
    to start over, or you want to make sure that any previous &ceph;
    configuration is removed, execute the following commands as &cephuser; user
    to purge the previous &ceph; configuration.
   </para>
   <warning>
    <para>
     Be aware that <emphasis>purging</emphasis> previous &ceph; installation
     destroys stored data and access settings.
    </para>
   </warning>
<screen>&prompt.cephuser;ceph-deploy purge <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
&prompt.cephuser;ceph-deploy purgedata <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
&prompt.cephuser;ceph-deploy forgetkeys</screen>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.cephdeploy">
   <title>Running <command>ceph-deploy</command></title>
   <para>
    After you prepared each &ceph; node as described in
    <xref linkend="ceph.install.ceph-deploy.eachnode"/>, you are ready to
    deploy &ceph; from the admin node with <command>ceph-deploy</command>. Note
    that <command>ceph-deploy</command> will not successfully install an OSD on
    disks that have been previously used, unless you first 'zap' them. Be aware
    that 'zapping' erases the entire disk content:
   </para>
<screen>&prompt.cephuser;ceph-deploy disk zap <replaceable>node:vdb</replaceable></screen>
   <procedure>
    <step>
     <para>
      Install <command>ceph</command> and <command>ceph-deploy</command>:
     </para>
<screen>sudo zypper in ceph ceph-deploy</screen>
    </step>
    <step>
     <para>
      Disable IPv6. Open <filename>/etc/sysctl.conf</filename>, edit the
      following lines, and reboot the admin node:
     </para>
<screen>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</screen>
    </step>
    <step>
     <para>
      Because it is not recommended to run <command>ceph-deploy</command> as
      &rootuser;, become the &cephuser; user:
     </para>
<screen>su - &cephuser.plain;</screen>
    </step>
    <step>
     <para>
      Run <command>ceph-deploy</command> to install &ceph; on each node:
     </para>
<screen>&prompt.cephuser;ceph-deploy install <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <tip>
      <para>
       <command>ceph-deploy</command> creates important files in the directory
       where you run it from. It is best to run <command>ceph-deploy</command>
       in an empty directory.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Set up the monitor nodes. Create keys and local configuration. The keys
      are used to authenticate and protect the communication between &ceph;
      nodes.
     </para>
<screen>&prompt.cephuser;ceph-deploy new <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <para>
      During this step, <command>ceph-deploy</command> creates local
      configuration files. It is recommended to inspect the configuration files
      in the current directory.
     </para>
     <tip>
      <title>Monitor Nodes on Different Subnets</title>
      <para>
       If the monitor nodes are not in the same subnet, you need to modify the
       <filename>ceph.conf</filename> in the current directory. For example, if
       the nodes have IP addresses
      </para>
<screen>10.121.9.186
10.121.10.186
10.121.11.186</screen>
      <para>
       add the following line to the global section of
       <filename>ceph.conf</filename>:
      </para>
<screen>public network = 10.121.0.0/16</screen>
      <para>
       Since you are likely to experience problems with IPv6 networking,
       consider modifying the IPv6 mon_host settings, as in the following
       example:
      </para>
<screen>mon_host = [2620:...10:121:9:186,2620:...10:121:10:186,2620:...10:121:11:186]</screen>
      <para>
       into its IPv4 equivalent:
      </para>
<screen>mon_host = 10.121.9.186, 10.121.10.186, 10.121.11.186</screen>
     </tip>
    </step>
    <step>
     <para>
      Create the initial monitor service on already created monitor nodes:
     </para>
<screen>&prompt.cephuser;ceph-deploy mon create-initial</screen>
    </step>
    <step>
     <para>
      Any node from which you need to run &ceph; command line tools needs a
      copy of the admin keyring. To copy the admin keyring to a node or set of
      nodes, run
     </para>
<screen>&prompt.cephuser;ceph-deploy admin <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <important>
      <para>
       Because the <literal>client.admin</literal>'s keyring file is readable
       by &rootuser; only, you need to use <command>sudo</command> when running
       the <command>ceph</command> command.
      </para>
     </important>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Create OSD daemons. Although you can use a directory as a storage, we
      recommend to create a separate disk dedicated to a &ceph; node. To find
      out the name of the disk device, run
     </para>
<screen>cat /proc/partitions
major minor  #blocks  name

 254        0   12582912 vda
 254        1    1532928 vda1
 254        2   11048960 vda2
  11        0    2831360 sr0
 254       16    4194304 vdb</screen>
     <para>
      In our case the <systemitem>vdb</systemitem> disk has no partitions, so
      it is most likely our newly created disk.
     </para>
     <para>
      Now set up the disk for &ceph;:
     </para>
<screen>&prompt.cephuser;ceph-deploy osd prepare <replaceable>node:vdb</replaceable></screen>
     <tip>
      <title>Using Existing Partitions</title>
      <para>
       If you need to create OSDs on already existing partitions, you need to
       set their GUIDs correctly. See
       <xref linkend="bp.osd_on_exisitng_partitions"/> for more details.
      </para>
     </tip>
     <tip>
      <para>
       If the disk was already used before, add the <option>--zap</option>
       option.
      </para>
<screen>&prompt.cephuser;ceph-deploy osd prepare --zap <replaceable>node:vdb</replaceable></screen>
      <para>
       Be aware that 'zapping' erases the entire disk content.
      </para>
     </tip>
     <note>
<!-- bnc#912479 -->
      <title>Default File System for OSDs</title>
      <para>
       The default and only supported file system for OSDs is
       <literal>xfs</literal>.
      </para>
     </note>
     <para>
      Optionally, activate the OSD:
     </para>
<screen>&prompt.cephuser;ceph-deploy osd activate <replaceable>node:vdb1</replaceable></screen>
     <tip>
      <para>
       To join the functionality of <command>ceph-deploy osd prepare</command>
       and <command>ceph-deploy osd activate</command>, use
       <command>ceph-deploy osd create</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      To test the status of the cluster, run
     </para>
<screen>sudo ceph -k ceph.client.admin.keyring health</screen>
    </step>
   </procedure>
   <tip>
    <title>Non-default Cluster Name</title>
    <para>
     If you need to install the cluster with <command>ceph-deploy</command>
     using a name other than the default <literal>cluster</literal> name, you
     need to initially specify it with <option>--cluster</option>, and then
     specify it in each <command>ceph-deploy</command> command related to that
     cluster:
    </para>
<screen>ceph-deploy --cluster my_cluster new [...]
ceph-deploy --ceph-conf my_cluster.conf mon create-initial
ceph-deploy --ceph-conf my_cluster.conf osd prepare [...]
ceph-deploy --ceph-conf my_cluster.conf osd activate [...]</screen>
    <para>
     Note that using a name other than default cluster name is not supported by
     SUSE.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.crowbar">
  <title>Deploying with &crow;</title>

  <para>
   &crow; (<link xlink:href="http://crowbar.github.io/"/>) is a framework to
   build complete deployments. It helps you transform groups of bare-metal
   nodes into an operational cluster within relatively short time.
  </para>

  <para>
   The deployment process consists of two basic steps: first you need to
   install and set up the &crow; admin server, then use it to deploy the
   available OSD/monitor nodes.
  </para>

  <sect2 xml:id="ceph.install.crowbar.admin_server">
   <title>Installing and Setting Up the &crow; Admin Server</title>
   <para>
    &crow; admin server is a stand-alone host with &sls; 12 SP2 installed,
    operating in the same network as the &ceph; OSD/MON nodes to be deployed.
    You need to configure the &crow; admin server so that it provides software
    repositories required for &ceph; deployment via TFTP protocol and PXE
    network boot.
   </para>
   <procedure>
    <step>
     <para>
      Install and register &sls; 12 SP2 on the &crow; admin server. Optionally,
      you can install and register the &storage; 4 extension at the same time.
      For more information on &sls; installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_inst.html"/>.
      For more information on the extensions installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     </para>
     <tip>
      <para>
       &crow; admin server does not require any graphical interface. To save
       the system resources and disk space, it is enough to install the
       <guimenu>Base System</guimenu>, <guimenu>Minimal System</guimenu> and,
       if you chose to install the &storage; 4 extension, <guimenu>SUSE
       Enterprise Storage Crowbar</guimenu> patterns from the <guimenu>Software
       Selection and System Tasks</guimenu> window. If you plan to synchronize
       repositories (see
       <xref linkend="ceph.install.crowbar.admin_server.repos"/>) with &smt;,
       add the <guimenu>Subscription Management Tool</guimenu> pattern as well.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Configure network settings for the &crow; admin server. The server has to
      have a static IP address assigned, and the full host name including the
      domain name specified (for example
      <literal>crowbar-admin.example.com</literal>). Check with
      <command>hostname -f</command> if the host name resolves correctly. The
      local network where you deploy the cluster needs to have the DHCP server
      disabled as the &crow; admin server runs its own.
     </para>
     <tip>
      <para>
       &crow; admin server default IP address is 192.168.124.10. If it is
       possible to keep that IP in your network environment, you can save some
       time on reconfiguring the &crow; network settings.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Configure NTP to keep the server's time synchronized. See
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"/>
      for more information on the NTP protocol.
     </para>
    </step>
    <step>
     <para>
      Make sure that SSH is enabled and started on the server.
     </para>
    </step>
    <step>
     <para>
      Install and register the &storage; 4 extension if you did not install it
      in step 1. For more information on extension installation, see
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
      Then install the <guimenu>SUSE Enterprise Storage Crowbar</guimenu>
      pattern in &yast;. If you prefer the command line, run <command>sudo
      zypper in -t pattern ses_admin</command>.
     </para>
    </step>
    <step>
     <para>
      Mount software repositories required for &ceph; nodes deployment with
      &crow;. See <xref linkend="ceph.install.crowbar.admin_server.repos"/> for
      more detailed information.
     </para>
    </step>
    <step>
     <para>
      If you need to further customize the &crow; admin server settings, refer
      to the <emphasis>Crowbar Setup</emphasis> chapter of the current
      <emphasis>&ocloud; Deployment Guide</emphasis> at
      <link xlink:href="https://www.suse.com/documentation"/>.
     </para>
    </step>
    <step>
     <para>
      Run the <command>install-ses-admin</command> script to complete the
      &crow; admin server setup. The script outputs a lot of information to the
      <filename>/var/log/crowbar/install.log</filename> log file which can be
      examined in the case of failure. Run it in the
      <systemitem>screen</systemitem> environment for safety reasons, as the
      network will be reconfigured during its run and interrupts may occur.
     </para>
<screen>screen install-ses-admin</screen>
     <para>
      Be patient as the script takes several minutes to finish.
     </para>
    </step>
    <step>
     <para>
      After the script successfully finishes, you can view the &crow; admin
      server Web UI by pointing your Web browser to the &crow; admin server IP
      address (http://192.168.124.10 by default).
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ceph.install.crowbar.admin_server.repos">
    <title>Prepare Software Repositories</title>
    <para>
     &crow; admin server needs to provide several software repositories so that
     the &ceph; nodes can install required packages from them on PXE boot.
     These repositories need to be mounted/synchronized under
     <filename>/srv/tftpboot/suse-12.2</filename>. The following description is
     based on the x86_64 architecture.
    </para>
    <tip>
     <title>Synchronizing Repositories</title>
     <para>
      There are several ways to provide the content in the repository
      directories. You can, for example, run your local &smt; instance,
      synchronize the repositories, and then export them via NFS and mount them
      on the &crow; admin server.
     </para>
    </tip>
    <variablelist>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.2/x86_64/install</term>
      <listitem>
       <para>
        This directory needs to contain the contents of the SLES 12 SP2 DVD
        disc #1. &ceph; nodes need it for the base SLES 12 SP2 installation.
        You can either mount the downloaded .iso image as a loop device, or
        copy its content with <command>rsync</command>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Pool</term>
      <listitem>
       <para>
        Base software repository for SLES 12 SP2.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Updates</term>
      <listitem>
       <para>
        Repository containing updates for SLES 12 SP2.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Pool</term>
      <listitem>
       <para>
        Base software repository for SES 4.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Updates</term>
      <listitem>
       <para>
        Repository containing updates for SES 4.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.install.crowbar.deploy_nodes">
   <title>Deploying the &ceph; Nodes</title>
   <para>
    The &crow; &wi; runs on the &admserv;. It provides an overview of the most
    important deployment details, including a view on the nodes and which roles
    are deployed on which nodes, and on the &barcl; proposals that can be
    edited and deployed. In addition, the &crow; &wi; shows details about the
    networks and switches in your cluster. It also provides graphical access to
    some tools with which you can manage your repositories, back up or restore
    the &admserv;, export the &chef; configuration, or generate a
    <literal>supportconfig</literal> TAR archive with the most important log
    files.
   </para>
   <sect3 xml:id="sec.depl.crow.login">
    <title>Logging In</title>
    <para>
     The &crow; &wi; uses the HTTP protocol and port <literal>80</literal>.
    </para>
    <procedure xml:id="pro.depl.crow.login">
     <title>Logging In to the &crow; Web Interface</title>
     <step>
      <para>
       On any machine, start a Web browser and make sure that JavaScript and
       cookies are enabled.
      </para>
     </step>
     <step>
      <para>
       As URL, enter the IP address of the &admserv;, for example:
      </para>
<screen>http://192.168.124.10/</screen>
     </step>
     <step>
      <para>
       Log in as user <systemitem class="username">crowbar</systemitem>. If you
       have not changed the password, it is <literal>crowbar</literal> by
       default.
      </para>
     </step>
    </procedure>
<!--taroth 2016-02-05: https://bugzilla.suse.com/show_bug.cgi?id=950482-->
    <procedure xml:id="pro.depl.crow.password">
     <title>Changing the Password for the &crow; Web Interface</title>
     <step>
      <para>
       After being logged in to the &crow; &wi;, select <menuchoice>
       <guimenu>&Barcl;</guimenu> <guimenu>Crowbar</guimenu> </menuchoice>.
      </para>
     </step>
     <step>
      <para>
       Select the <literal>&crow;</literal> &barcl; entry and
       <guimenu>Edit</guimenu> the proposal.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>Attributes</guimenu> section, click
       <guimenu>Raw</guimenu> to edit the configuration file.
      </para>
     </step>
     <step>
      <para>
       Search for the following entry:
      </para>
<screen>"crowbar": {
     "password": "crowbar"</screen>
     </step>
     <step>
      <para>
       Change the password.
      </para>
     </step>
     <step>
      <para>
       Confirm your change by clicking <guimenu>Save</guimenu> and
       <guimenu>Apply</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.depl.inst.nodes.install">
    <title>Node Installation</title>
    <para>
     The &ceph; nodes represent the actual clister infrastructure. Node
     installation and service deployment is done automatically from the
     &admserv;. Before deploying the &ceph; service, &cephos; will be installed
     on all nodes.
    </para>
    <para>
     To install a node, you need to boot it first using PXE. It will be booted
     with an image that enables the &admserv; to discover the node and make it
     available for installation. When you have allocated the node, it will boot
     using PXE again and the automatic installation will start.
    </para>
    <procedure>
     <step>
      <para>
       Boot all nodes that you want to deploy using PXE. The nodes will boot
       into the <quote>SLEShammer</quote> image, which performs the initial
       hardware discovery.
      </para>
      <important>
       <title>Limit the Number of Concurrent Boots using PXE</title>
       <para>
        Booting many nodes using PXE at the same time will cause heavy load on
        the TFTP server, because all nodes will request the boot image at the
        same time. It is recommended to boot the nodes time-delayed.
       </para>
      </important>
     </step>
     <step>
      <para>
       Open a browser and point it to the &crow; Web interface on the
       &admserv;, for example <literal>http://192.168.124.10/</literal>. Log in
       as user <systemitem class="username">crowbar</systemitem>. The password
       is <literal>crowbar</literal> by default, if you have not changed it.
      </para>
      <para>
       Click <menuchoice> <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu>
       </menuchoice> to open the <guimenu>Node Dashboard</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Each node that has successfully booted will be listed as being in state
       <literal>Discovered</literal>, indicated by a yellow bullet. The nodes
       will be listed with their MAC address as a name. Wait until all nodes
       are listed as being <literal>Discovered</literal> before proceeding. In
       case a node does not report as being <literal>Discovered</literal>, it
       may need to be rebooted manually.
      </para>
      <figure>
       <title>Discovered Nodes</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="100%" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="75%" format="png"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
     <step>
      <para>
       Although this step is optional, it is recommended to properly group your
       nodes at this stage, since it lets you clearly arrange all nodes.
       Grouping the nodes by role would be one option, for example monitor
       nodes and OSD nodes.
      </para>
      <substeps performance="required">
       <step>
        <para>
         Enter the name of a new group into the <guimenu>New Group</guimenu>
         text box and click <guimenu>Add Group</guimenu>.
        </para>
       </step>
       <step>
        <para>
         Drag and drop a node onto the title of the newly created group. Repeat
         this step for each node you want to put into the group.
        </para>
        <figure>
         <title>Grouping Nodes</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_node_dashboard_groups_initial.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_node_dashboard_groups_initial.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To allocate all nodes, click <menuchoice><guimenu>Nodes</guimenu>
       <guimenu>Bulk Edit</guimenu></menuchoice>. To allocate a single node,
       click the name of a node, then click <guimenu>Edit</guimenu>.
      </para>
      <figure>
       <title>Editing a Single Node</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="depl_node_edit.png" width="100%" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="depl_node_edit.png" width="75%" format="png"/>
        </imageobject>
       </mediaobject>
      </figure>
      <important>
       <title>Limit the Number of Concurrent Node Deployments</title>
       <para>
        Deploying many nodes in bulk mode will cause heavy load on the
        &admserv;. The subsequent concurrent &chef; client runs triggered by
        the nodes will require a lot of RAM on the &admserv;.
       </para>
       <para>
        Therefore it is recommended to limit the number of concurrent
        <quote>Allocations</quote> in bulk mode. The maximum number depends on
        the amount of RAM on the &admserv;&mdash;limiting concurrent
        deployments to five up to ten is recommended.
       </para>
      </important>
     </step>
     <step>
      <para>
       In single node editing mode, you can also specify the
       <guimenu>Filesystem Type</guimenu> for the node. By default, it is set
       to <literal>ext4</literal> for all nodes. It is recommended to keep this
       default.
      </para>
     </step>
     <step>
      <para>
       Provide a meaningful <guimenu>Alias</guimenu>, <guimenu>Public
       Name</guimenu> and a <guimenu>Description</guimenu> for each node and
       check the <guimenu>Allocate</guimenu> box. You can also specify the
       <guimenu>Intended Role</guimenu> for the node. This optional setting is
       used to make reasonable proposals for the &barcl;s.
      </para>
      <para>
       By default <guimenu>Target Platform</guimenu> is set to <guimenu>SLES 12
       SP2</guimenu>.
      </para>
      <tip>
       <title>Alias Names</title>
       <para>
        Providing an alias name will change the default node names (MAC
        address) to the name you provided, making it easier to identify the
        node. Furthermore, this alias will also be used as a DNS
        <literal>CNAME</literal> for the node in the admin network. As a
        result, you can access the node via this alias when, for example,
        logging in via SSH.
       </para>
      </tip>
      <figure>
       <title>Bulk Editing Nodes</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="depl_node_bulk_edit_allocate.png" width="100%" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="depl_node_bulk_edit_allocate.png" width="75%" format="png"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
     <step>
      <para>
       When you have filled in the data for all nodes, click
       <guimenu>Save</guimenu>. The nodes will reboot and commence the
       &ay;-based &sls; installation (or installation of other target
       platforms, if selected) via a second boot using PXE. Click <menuchoice>
       <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu> </menuchoice> to
       return to the <guimenu>Node Dashboard</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Nodes that are being installed are listed with the status
       <literal>Installing</literal> (yellow/green bullet). When the
       installation of a node has finished, it is listed as being
       <literal>Ready</literal>, indicated by a green bullet. Wait until all
       nodes are listed as being <literal>Ready</literal> before proceeding.
      </para>
      <figure>
       <title>All Nodes Have Been Installed</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="depl_node_dashboard_groups_installed.png" width="100%" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="depl_node_dashboard_groups_installed.png" width="75%" format="png"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.depl.ceph.barclamps">
    <title>&Barcl;s</title>
    <para>
     The &ceph; service is automatically installed on the nodes by using
     so-called &barcl;s&mdash;a set of recipes, templates, and installation
     instructions. A &barcl; is configured via a so-called proposal. A proposal
     contains the configuration of the service(s) associated with the blowlamp
     and a list of machines onto which the &barcl; should be deployed.
    </para>
    <para>
     All existing &barcl;s can be accessed from the &crow; Web interface by
     clicking <guimenu>Barclamps</guimenu>. To create or edit &barcl; proposals
     and deploy them, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Open a browser and point it to the &crow; Web interface available on the
       &admserv;, for example <literal>http://192.168.124.10/</literal>. Log in
       as user <systemitem class="username">crowbar</systemitem>. The password
       is <literal>crowbar</literal> by default, if you have not changed it.
      </para>
      <para>
       Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
       Barclamps</guimenu> menu. Alternatively you may filter the list to
       <guimenu>Crowbar</guimenu> or <guimenu>OpenStack</guimenu> &barcl;s by
       choosing the respective option from <guimenu>Barclamps</guimenu>. The
       <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for setting
       up and configuring all nodes, while the <guimenu>OpenStack</guimenu>
       &barcl;s are dedicated to &ceph; service deployment and configuration.
      </para>
     </step>
     <step>
      <para>
       You can either <guimenu>Create</guimenu> a proposal or
       <guimenu>Edit</guimenu> an existing one.
      </para>
      <para>
       Most &ceph; &barcl;s consist of two sections: the
       <guimenu>Attributes</guimenu> section lets you change the configuration,
       and the <guimenu>Node Deployment</guimenu> section lets you choose onto
       which nodes to deploy the &barcl;.
      </para>
     </step>
     <step>
      <para>
       To edit the <guimenu>Attributes</guimenu> section, change the values via
       the Web form. Alternatively you can directly edit the configuration file
       by clicking <guimenu>Raw</guimenu>.
      </para>
      <warning>
       <title>Raw Mode</title>
       <para>
        If you switch between <guimenu>Raw</guimenu> mode and Web form
        (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
        your changes before switching, otherwise they will be lost.
       </para>
      </warning>
     </step>
     <step>
      <para>
       To assign nodes to a role, use the <guimenu>Deployment</guimenu> section
       of the &barcl;. It shows the <guimenu>Available Nodes</guimenu> that you
       can assign to the roles belonging to the &barcl;.
      </para>
      <para>
       One or more nodes are usually automatically pre-selected for available
       roles. If this pre-selection does not meet your requirements, click the
       <guimenu>Remove</guimenu> icon next to the role to remove the
       assignment. Assign a node or cluster of your choice by selecting the
       respective entry from the list of <guimenu>Available Nodes</guimenu>,
       <guimenu>Available Clusters</guimenu>, or <guimenu>Available Clusters
       with Remote Nodes</guimenu>. Drag it to the desired role and drop it
       onto the <emphasis>role name</emphasis>. Do <emphasis>not</emphasis>
       drop a node or cluster onto the text box&mdash;this is used to filter
       the list of available nodes or clusters!
      </para>
     </step>
     <step>
      <para>
       To save and deploy your edits, click <guimenu>Apply</guimenu>. To save
       your changes without deploying them, click <guimenu>Save</guimenu>. To
       remove the complete proposal, click <guimenu>Delete</guimenu>. A
       proposal that already has been deployed can only be deleted manually,
       see <xref linkend="sec.depl.ceph.barclamps.delete"/> for details.
      </para>
      <para>
       If you deploy a proposal onto a node where a previous one is still
       active, the new proposal will overwrite the old one.
      </para>
      <note>
       <title>Wait Until a Proposal has been Deployed</title>
       <para>
        Deploying a proposal might take some time (up to several minutes). It
        is strongly recommended to always wait until you see the note
        <quote>Successfully applied the proposal</quote> before proceeding on
        to the next proposal.
       </para>
      </note>
     </step>
    </procedure>
    <sect4 xml:id="sec.depl.ceph.barclamps.delete">
     <title>Delete a Proposal That Already Has Been Deployed</title>
     <para>
      To delete a proposal that already has been deployed, you first need to
      <guimenu>Deactivate</guimenu> it in the &crow; Web interface.
      Deactivating a proposal removes the chef role from the nodes, so the
      routine that installed and set up the services is not executed anymore.
      After a proposal has been deactivated, you can <guimenu>Delete</guimenu>
      it in the &crow; Web interface to remove the barclamp configuration data
      from the server.
     </para>
     <para>
      Deactivating and deleting a barclamp that already had been deployed does
      <emphasis>not</emphasis> remove packages installed when the &barcl; was
      deployed. Nor does it stop any services that were started during the
      &barcl; deployment. To undo the deployment on the affected node, you need
      to stop (<command>systemctl stop
      <replaceable>service</replaceable></command>) the respective services and
      disable (<command>systemctl disable
      <replaceable>service</replaceable></command>) them. Uninstalling packages
      should not be necessary.
     </para>
    </sect4>
    <sect4 xml:id="sec.depl.ceph.barclamps.queues">
     <title>Queuing/Dequeuing Proposals</title>
     <para>
      When a proposal is applied to one or more nodes that are nor yet
      available for deployment (for example because they are rebooting or have
      not been fully installed, yet), the proposal will be put in a queue. A
      message like
     </para>
<screen>Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</screen>
     <para>
      will be shown when having applied the proposal. A new button
      <guimenu>Dequeue</guimenu> will also become available. Use it to cancel
      the deployment of the proposal by removing it from the queue.
     </para>
    </sect4>
   </sect3>
   <sect3 xml:id="sec.depl.ceph.ceph">
    <title>Deploying &ceph;</title>
    <para>
     For &ceph; at least four nodes are required. If deploying the optional
     Calamari server for &ceph; management and monitoring, an additional node
     is required.
    </para>
    <para>
     The &ceph; &barcl; has the following configuration options:
    </para>
    <variablelist>
     <varlistentry>
      <term><guimenu>Disk Selection Method</guimenu>
      </term>
      <listitem>
       <para>
        Choose whether to only use the first available disk or all available
        disks. <quote>Available disks</quote> are all disks currently not used
        by the system. Note that one disk (usually
        <filename>/dev/sda</filename>) of every block storage node is already
        used for the operating system and is not available for &ceph;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Number of Replicas of an Object</guimenu>
      </term>
      <listitem>
       <para>
        For data security, stored objects are not only stored once, but
        redundantly. Specify the number of copies that should be stored for
        each object with this setting. The number includes the object itself.
        If you for example want the object plus two copies, specify 3.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>SSL Support for RadosGW</guimenu>
      </term>
      <listitem>
       <para>
        Choose whether to encrypt public communication
        (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If
        choosing <guimenu>HTTPS</guimenu>, you need to specify the locations
        for the certificate key pair files.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Calamari Credentials</guimenu>
      </term>
      <listitem>
       <para>
        Calamari is a Web front-end for managing and analyzing the &ceph;
        cluster. Provide administrator credentials (user name, password, e-mail
        address) in this section. When &ceph; has bee deployed you can log in
        to Calamari with these credentials. Deploying Calamari is
        optional&mdash;leave these text boxes empty when not deploying
        Calamari.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <figure>
     <title>The &ceph; &Barcl;</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="depl_barclamp_ceph.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_ceph.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     The &ceph; service consists of the following different roles:
    </para>
    <variablelist>
     <varlistentry>
      <term><guimenu>ceph-osd</guimenu>
      </term>
      <listitem>
       <para>
        The virtual block storage service. Install this role on all dedicated
        &ceph; &stornode;s (at least three), but not on any other node.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>ceph-mon</guimenu>
      </term>
      <listitem>
       <para>
        Cluster monitor daemon for the &ceph; distributed file system.
        <guimenu>ceph-mon</guimenu> needs to be installed on three or five
        &stornode;s running <guimenu>ceph-osd</guimenu>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>ceph-calamari</guimenu>
      </term>
      <listitem>
       <para>
        Sets up the Calamari Web interface which lets you manage the &ceph;
        cluster. Deploying it is optional. The Web interface can be accessed
        via http://<replaceable>IP-ADDRESS</replaceable>/ (where
        <replaceable>IP-ADDRESS</replaceable> is the address of the machine
        where <guimenu>ceph-calamari</guimenu> is deployed on).
        <guimenu>ceph-calamari</guimenu> needs to be installed on a dedicated
        node&mdash;it is <emphasis>not</emphasis> possible to install it on a
        nodes running other services.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>ceph-radosgw</guimenu>
      </term>
      <listitem>
       <para>
        The HTTP REST gateway for &ceph;. Install it on a &stornode; running
        <guimenu>ceph-osd</guimenu>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <figure>
     <title>The &ceph; &Barcl;: Node Deployment Example</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.saltstack">
  <title>Deploying with &salt;</title>

  <warning>
   <title>Technology Preview</title>
   <para>
    As of &storage; 3, &salt; is considered a technology preview and is not
    supported.
   </para>
  </warning>

  <para>
   &salt; is a <emphasis>stack</emphasis> of components that help you deploy
   and manage server infrastructure. It is very scalable, fast, and relatively
   easy to get running. Read the following considerations before you start
   deploying the cluster with &salt;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
     deployment. Dedicate all the host resources to the &smaster; services. Do
     not install &ceph; on the host where you want to run &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
     the &ceph; environment, &sminion; is typically an OSD or monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name. Therefore, we recommend to set the &smaster;'s host name to
     <systemitem>salt</systemitem>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To deploy &ceph; cluster using &salt;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Install and register &sls; 12 SP2 together with &storage; 4 extension on
     each node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install &smaster;:
    </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
   </step>
   <step>
    <para>
     Install &sminion;s on each node you want to include in the &ceph; cluster:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
   </step>
   <step>
    <para>
     Configure all &sminion;s to connect to the &smaster;. If your &smaster; is
     not reachable by the DNS name <literal>salt</literal>, you will need to
     edit the <filename>/etc/salt/minion</filename> file on each &sminion; and
     set
    </para>
<screen>master:<replaceable>dns_name_of_salt_master</replaceable></screen>
   </step>
   <step>
    <para>
     Set up salt keys.
    </para>
    <substeps>
     <step>
      <para>
       Enable and start all &sminion;s:
      </para>
<screen>&prompt.sminion;systemctl enable salt-minion
&prompt.sminion;systemctl start salt-minion</screen>
     </step>
     <step>
      <para>
       Enable and start &smaster;:
      </para>
<screen>&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
     </step>
     <step>
      <para>
       Accept the keys from the minions:
      </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
      <note>
       <para>
        Both all &sminion;s and &smaster; have a key to do a symmetric
        key-based communication. The master must accept the minions into its
        control while minions with other keys are not accepted.
       </para>
      </note>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Install the <systemitem class="resource">salt-ceph</systemitem> package:
    </para>
<screen>&prompt.smaster;zypper in salt-ceph</screen>
   </step>
   <step>
    <para>
     Distribute the <systemitem>ceph</systemitem> execution interface to the
     minions:
    </para>
<screen>&prompt.smaster;salt "*" saltutil.sync_all
&prompt.smaster;salt '*' pkg.install python-ceph-cfg</screen>
    <tip>
     <para>
      Use "*" in the <command>salt</command> command to specify all controlled
      minions.
     </para>
    </tip>
    <para>
     The <filename>ceph</filename> directory will be synchronized to each of
     your nodes. You can test the result with:
    </para>
<screen>&prompt.smaster;salt "*" ceph_cfg -d</screen>
    <para>
     If this returns no results, check and repeat the proceeding step.
    </para>
   </step>
   <step>
    <para>
     Create a directory on &smaster;:
    </para>
<screen>&prompt.smaster;mkdir /srv/salt/osceph/</screen>
   </step>
   <step>
    <para>
     Create a new <filename>ceph.conf</filename> file and copy it into the new
     directory <filename>/srv/salt/osceph/</filename>. A reasonable minimal
     <filename>ceph.conf</filename> file contains the following settings:
    </para>
<screen>[global]
fsid = 6de3876f-f4dd-4987-90bf-44fc1b2148d5
mon_initial_members = ses-1, ses-2, ses-3
mon_host = 192.168.12.11,192.168.12.12,192.168.12.13
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx</screen>
    <note>
     <para>
      You must generate a unique FSID for your &ceph; cluster by running
      <command>uuidgen</command> from a terminal, then use that FSID in the
      <literal>fsid</literal> line in <filename>ceph.conf</filename> .
     </para>
     <para>
      The <literal>mon_initial_members</literal> and
      <literal>mon_host</literal> lines need to contain the actual host names
      and IP addresses of the hosts you will initially deploy as monitors. The
      order of host names and IP addresses on these lines needs to match.
     </para>
    </note>
   </step>
   <step>
    <para>
     Install &ceph; on all nodes:
    </para>
<screen>&prompt.smaster;salt '*' pkg.install ceph</screen>
   </step>
   <step>
    <para>
     If you wish to deploy &rgw;, install the
     <systemitem>ceph-radosgw</systemitem> package on any node(s) you wish to
     use for this service. The following example command will install
     <systemitem>ceph-radosgw</systemitem> on any node whose host name starts
     with "rgw":
    </para>
<screen>&prompt.smaster;salt 'rgw*' pkg.install ceph-radosgw</screen>
   </step>
   <step>
    <tip>
     <para>
      Salt uses <emphasis>Salt State</emphasis> files (or SLS files) that
      represent the state in which the cluster should be. SLS files use
      specific simple syntax. You can either use one complex file, or split it
      into several logical parts and include them from the main file.
     </para>
    </tip>
    <para>
     Get the example SLS file, modify it to your needs, and copy it into the
     <filename>/srv/salt/</filename> directory:
    </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_buildup.sls /srv/salt</screen>
    <tip>
     <para>
      You can find the example SLS file with comments in
      <xref linkend="app.storage.sls"/>.
     </para>
    </tip>
    <important>
     <para>
      The example SLS file includes preset authentication keys and will deploy
      a monitor, an MDS and a &rgw; on any node you apply it to. It will
      additionally try to turn <filename>/dev/vdb</filename> and
      <filename>/dev/vdc</filename> into OSDs. Thus it is suitable for a small
      test cluster of virtual machines. For a very simple test deployment,
      delete the various "*rgw*" and "*mds*" sections, and adjust the
      "prepare_" and "activate_" sections to match the actual device names of
      the disks you wish to turn into OSDs. Make sure these disks are
      completely blank (with no partitions) before applying this state, or
      those disks will not become OSDs.
     </para>
    </important>
    <note>
     <para>
      Do not use the example SLS file in production. At a minimum you need to
      replace the authentication keys with keys of your own creation.
     </para>
    </note>
   </step>
   <step>
    <para>
     Configure &ceph; following the SLS file you modified:
    </para>
    <tip>
     <para>
      The <command>state.sls</command> directive instructs Salt to apply the
      language included in the specified file.
     </para>
    </tip>
<screen>&prompt.smaster;cd /srv/salt
&prompt.smaster;salt '*' state.sls cluster_buildup</screen>
    <tip>
     <para>
      If you experience problems during this step, it is probably because of
      the settings, such as number of disks. To adjust these, try editing your
      <filename>cluster_buildup.sls</filename> file.
     </para>
    </tip>
   </step>
   <step>
    <para>
     For a production deployment, you should create your own authentication
     keys for each type of service (MON, OSD, MDS, RGW), then use them in the
     salt state files, in place of the keys in the example file. To create a
     minimal set of keys for admin, MON and OSD, run the following commands
     against any one of the &sminion;s:
    </para>
<screen>&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=admin
&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=mon
&prompt.smaster;salt minion-hostname ceph_cfg.keyring_create keyring_type=osd</screen>
    <para>
     Take the key from the output of each of these commands and use it for the
     'secret' value of the appropriate section in your state file (see the
     various 'keyring_*_save' sections of
     <filename>cluster_buildup.sls</filename> for examples).
    </para>
   </step>
   <step>
    <para>
     For a larger cluster, with many nodes with different roles (for example
     three MONs, several OSDs, at least one &rgw; and one MDS), it is best to
     create multiple salt state files, one for each role. Then apply those
     states to the nodes with the relevant roles, rather than using the same
     state file for all nodes.
    </para>
    <para>
     See <xref linkend="app.storage.sls"/> for an example set of SLS files.
    </para>
   </step>
  </procedure>

  <para>
   You should now have a running &ceph; setup.
  </para>

  <tip>
   <title>Uninstalling &ceph;</title>
   <para>
    To uninstall the &ceph; cluster, copy
    <filename>cluster_teardown.sls</filename> to <filename>/srv/salt</filename>
   </para>
<screen>cp /usr/share/doc/packages/salt-ceph/examples/cluster_teardown.sls /srv/salt</screen>
   <para>
    change to <filename>/srv/salt</filename> and run
   </para>
<screen>&prompt.smaster;salt '*' state.sls cluster_teardown</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.install.calamari">
  <title>Install Calamari</title>

  <para>
   Calamari is a management and monitoring system for &ceph; storage cluster.
   It provides a Web user interface that makes &ceph; cluster monitoring very
   simple and handy.
  </para>

  <note>
   <title>Conflicts in combination with deployment by using &crow;</title>
   <para>
    If you used &crow; to install &storage;, install Calamari on a different
    server than &crow; as &crow; uses the same port as Calamari (port 80). Bear
    in mind that Calamari must be installed on a node of the &storage; cluster.
   </para>
  </note>

  <para>
   To install Calamari, run the following commands as &rootuser;:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen># zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen># calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):  
Email address: 
Password: 
Password (again): 
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      In order for Calamari to work correctly, the admin keyring needs to be
      installed on each monitor node:
     </para>
<screen>ceph-deploy admin mon1 mon2 mon3</screen>
     <para>
      where <replaceable>mon1</replaceable>, <replaceable>mon2</replaceable>,
      or <replaceable>mon3</replaceable> are the host names of the monitors.
     </para>
    </tip>
    <para>
     Now open your Web browser and point it to the host name/IP address of the
     server where you installed Calamari. Log in with the credentials you
     entered when installing the Calamari client. A welcome screen appears,
     instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host and
     enter the following command. Note that the <option>--master</option>
     option specifies the host name of the Calamari server to which all the
     cluster nodes connect to:
    </para>
<screen>ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your &ceph; cluster, OSDs, pools, etc.
    </para>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state&mdash;for
      example if a node goes offline&mdash;should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw heartbeat
      information from the cluster. Normally, this will not be more than one
      minute old, but in certain failure cases, for instance when a network
      outage occurs or if the cluster loses quorum (that is if more than half
      of the monitor nodes are down), Calamari will no longer be able to
      determine cluster state. In this case, the <guimenu>Health</guimenu>
      panel will indicate that the last update was more than one minute ago.
      After too long time with no updates, Calamari displays a warning at the
      top of the screen "Cluster Updates Are Stale. The Cluster is not updating
      Calamari." If this occurs, the other status information Calamari presents
      will not be correct so you should investigate further to check the status
      of your storage nodes and network.
     </para>
    </important>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to trigger a
      re-run of salt on all &ceph; nodes, which should clear up any odd state
      or missing bits and pieces.
     </para>
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup, such
      as state files, configuration files, or PostgreSQL database files. At
      minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
 </sect1>
</chapter>
