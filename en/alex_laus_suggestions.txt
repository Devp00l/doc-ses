$>rados bench 180 --no-cleanup -p data write

Maintaining 16 concurrent writes of 4194304 bytes for up to 180 seconds or 0 objects
Object prefix: benchmark_data_ODS1_18023
sec Cur ops started finished avg MB/s cur MB/s last lat av

2015-07-30 23:01:58.607951min lat: 4.83066 max lat: 58.1505 avg lat: 24.2551
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
220 7 133 126 2.29025 0 - 24.2551
221 7 133 126 2.27988 0 - 24.2551
Total time run: 221.903941
Total writes made: 133
Write size: 4194304
Bandwidth (MB/sec): 2.397 

Stddev Bandwidth: 2.33237
Max bandwidth (MB/sec): 12
Min bandwidth (MB/sec): 0
Average Latency: 25.66
Stddev Latency: 13.227
Max latency: 58.1505
Min latency: 4.83066

$>rados bench 180 --no-cleanup -p data seq
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
0 16 16 0 0 0 - 0
1 16 70 54 212.319 216 0.563077 0.220467
2 16 119 103 204.215 196 0.961257 0.264081
Total time run: 2.602962
Total reads made: 133
Read size: 4194304
Bandwidth (MB/sec): 204.383 

Average Latency: 0.31128
Max latency: 1.16971
Min latency: 0.008047


Page 40 to 45 

Cache Tiering could be complicated, the main reason for it is if there are multiple disk in the same host. the default $>ceph osd create will add all the disk understand the same host. 
So one single host have all SSD vs all host having 1 ssd each ... I'm not sure which is better but physically distributed cache over SSD in different host seem to make more sense to me. 

Anyway, the instruction missing how to modify if the same physical "host" can write in different HOST in CRUSH map. And the id = -1 .... -5 are not really clearly state how to create or modify. 
Just don't not change ... is really confusing for the beginner. 

So I added a SSD physical disk into the VM as sdb following and sda is regular virtual disk over VMDK

$> sudo /sbin/hdparm -tT /dev/sdb

/dev/sdb:
Timing cached reads: 32664 MB in 1.99 seconds = 16443.27 MB/sec
Timing buffered disk reads: 504 MB in 3.00 seconds = 167.81 MB/sec
$> sudo /sbin/hdparm -tT /dev/sda

/dev/sda:
Timing cached reads: 31572 MB in 1.99 seconds = 15884.76 MB/sec
Timing buffered disk reads: 116 MB in 3.01 seconds = 38.56 MB/sec

Page 46 

The rbd call with erasure pool "ecpool" has not been create until Chapter 6, I would suggest this page put toward the end of Chapter 6 instead. Then the referencing Chapter 5 of Caching made more sense anyway. 

To compare the write on cold is even slower ( which is correct and read is faster then data pool ) 

$>rados bench 180 --no-cleanup -p cold-storage write 
...
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
260 6 114 108 1.66136 0 - 32.79
261 6 114 108 1.655 0 - 32.79
262 6 114 108 1.64868 0 - 32.79
263 3 114 111 1.68803 2 122.362 34.4984
264 3 114 111 1.68164 0 - 34.4984
265 3 114 111 1.67529 0 - 34.4984
266 3 114 111 1.66899 0 - 34.4984
267 3 114 111 1.66274 0 - 34.4984
268 3 114 111 1.65654 0 - 34.4984
269 3 114 111 1.65038 0 - 34.4984
Total time run: 270.019286
Total writes made: 114
Write size: 4194304
Bandwidth (MB/sec): 1.689 

Stddev Bandwidth: 2.41664
Max bandwidth (MB/sec): 32
Min bandwidth (MB/sec): 0
Average Latency: 36.5215
Stddev Latency: 30.1191
Max latency: 132.733
Min latency: 1.71362

$>rados bench 180 --no-cleanup -p cold-storage seq
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
0 0 0 0 0 0 - 0
1 11 114 103 411.947 412 0.286138 0.078033
Total time run: 1.082883
Total reads made: 114
Read size: 4194304
Bandwidth (MB/sec): 421.098 

Average Latency: 0.129733
Max latency: 0.683775
Min latency: 0.014514



$>rados bench 180 --no-cleanup -p hot-storage write 
... 
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
180 16 1313 1297 28.8194 60 1.54959 2.21265
181 16 1314 1298 28.6823 4 1.20986 2.21188
182 8 1314 1306 28.7005 32 1.78533 2.21118
Total time run: 182.020927
Total writes made: 1314
Write size: 4194304
Bandwidth (MB/sec): 28.876 

Stddev Bandwidth: 18.073
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average Latency: 2.21313
Stddev Latency: 1.45746
Max latency: 12.996
Min latency: 0.171323

$>rados bench 180 --no-cleanup -p hot-storage seq
...
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
20 16 728 712 142.386 160 0.07297 0.444536
21 16 782 766 145.89 216 0.724205 0.428411
22 16 843 827 150.349 244 0.016374 0.417248
23 16 946 930 161.723 412 0.632435 0.391206
24 16 1066 1050 174.983 480 0.080683 0.365147
25 16 1314 1298 207.66 992 0.028114 0.307508
Total time run: 25.145185
Total reads made: 1314
Read size: 4194304
Bandwidth (MB/sec): 209.026 

Average Latency: 0.306068
Max latency: 1.813
Min latency: 0.009599


Then we compare the erasure write on hot the result is about the same as hot, since my ssd is only single hardware with multiple partitions. If multiple ssd may generate better result.

$>rados bench 180 --no-cleanup -p ec-hot-storage write
...
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
180 16 1288 1272 28.2637 36 2.66814 2.25268
181 16 1289 1273 28.1296 4 1.79701 2.25232
Total time run: 181.022246
Total writes made: 1289
Write size: 4194304
Bandwidth (MB/sec): 28.483 

Stddev Bandwidth: 17.0279
Max bandwidth (MB/sec): 68
Min bandwidth (MB/sec): 0
Average Latency: 2.24696
Stddev Latency: 1.12037
Max latency: 6.12836
Min latency: 0.184802

$>rados bench 180 --no-cleanup -p ec-hot-storage seq
...
sec Cur ops started finished avg MB/s cur MB/s last lat avg lat
20 16 683 667 133.387 116 0.092489 0.46219
21 16 701 685 130.464 72 0.18129 0.476636
22 16 733 717 130.351 128 0.049422 0.484128
23 16 761 745 129.553 112 0.722009 0.483281
24 16 794 778 129.654 132 0.302523 0.47941
25 16 830 814 130.227 144 0.258459 0.483447
26 16 866 850 130.757 144 0.036974 0.475755
27 16 898 882 130.654 128 0.01109 0.476342
28 16 949 933 133.273 204 0.375561 0.475692
29 16 1013 997 137.504 256 0.705495 0.462853
30 16 1053 1037 138.254 160 0.350638 0.457128
31 16 1214 1198 154.566 644 0.300812 0.412308
32 16 1289 1273 159.11 300 0.071944 0.39486
Total time run: 32.035612
Total reads made: 1289
Read size: 4194304
Bandwidth (MB/sec): 160.946 

Average Latency: 0.397563
Max latency: 2.3747
Min latency: 0.009994

Page 50 
Qemu by default is not install even with YaST install with KVM virtualization. 
So we need to install qemu-block-rbd or else the Qemu will report Unknown protocol 

$> qemu-img: Could not open 'rbd:ecpool/': Unknown protocol

Need to add the following before using Qemu with rbd
$>sudo zypper install qemu-block-rbd


