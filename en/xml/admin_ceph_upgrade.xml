<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!-- influenced by http://ceph.com/docs/master/install/upgrading-ceph/ -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster itself, you need to add a new version
   of the &storage; product to the existing operating system, and update the
   software to the latest versions on each cluster node. You can upgrade
   daemons in your cluster while the cluster is online and in service.
   Certain types of daemons depend upon others. For example &ceph; &rgw;s
   depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
   upgrading in this order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &ceph; monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; &rgw;s.
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for
    example all monitor daemons or all OSD daemons&mdash;one by one to
    ensure that they are all on the same release. We also recommend that you
    upgrade all the daemons in your cluster before you try to exercise new
    functionality in a release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their
    status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs
    are upgraded:
   </para>
<screen>ceph osd stat</screen>
  </tip>

  <procedure>
   <title>General Upgrade Steps</title>
   <step>
    <para>
     Check that the latest patches are applied to the installed &sls;.
    </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper patch</screen>
   </step>
   <step>
    <para>
     Install &storage; extension to which you are upgrading with
     <menuchoice><guimenu>&yast;</guimenu><guimenu>Software</guimenu><guimenu>Add-On
     Products</guimenu><guimenu>Add</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     After the new product is added, refresh the installed software
     repositories.
    </para>
<screen>sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Upgrade the installed software to the new version.
    </para>
<screen>sudo zypper dup</screen>
   </step>
   <step>
    <para>
     After the last &ceph; node is upgraded, check the cluster status.
    </para>
<screen>ceph health</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.2.1to3">
  <title>Upgrade from &storage; 2.1 to 3</title>

  <para>
   This section includes steps specific to upgrading &storage; version 2.1
   to 3.
  </para>

  <sect2 xml:id="ceph.upgrade.2.1to3.ceph_uid">
   <title>'ceph' User and Group</title>
   <para>
    In &storage; prior to version 3.0, the <systemitem>ceph</systemitem> user
    was used for administering and installing the cluster with
    <command>ceph-deploy</command>, while the &ceph; daemons ran with
    &rootuser; privileges. In &storage; version 3.0, the &ceph; daemons run as
    an unprivileged user and group <systemitem>ceph</systemitem>.
   </para>
   <para>
    This is why you need to rename the existing
    <systemitem>ceph</systemitem> user and possibly create a new one with
    specific parameters, on each cluster node:
   </para>
   <procedure>
    <step>
     <para>
      Rename the existing <systemitem>ceph</systemitem> user to &cephuser;:
     </para>
<screen>sudo usermod -l &cephuser; ceph </screen>
    </step>
    <step>
     <para>
      During the product upgrade, a new <systemitem>ceph</systemitem> user
      and group are created with the default UID/GID 167 (if available).
      Depending on whether you need to have control over the new
      <systemitem>ceph</systemitem> user creation, there are the following
      options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If UID/GID 167 <emphasis>are not</emphasis> used across the cluster,
        the <systemitem>ceph</systemitem> user will be automatically created
        during the product installation and there are no manual steps
        needed.
       </para>
      </listitem>
      <listitem>
       <para>
        If UID/GID 167 <emphasis>are</emphasis> already used, you need to
        choose UID and GID for the <systemitem>ceph</systemitem> user and
        group, and create them manually:
       </para>
       <procedure>
        <step>
         <para>
          Create <systemitem>ceph</systemitem> group:
         </para>
<screen>sudo groupadd -r -g <replaceable>GID</replaceable> ceph</screen>
        </step>
        <step>
         <para>
          Add <systemitem>ceph</systemitem> user and assign to the
          <systemitem>ceph</systemitem> group:
         </para>
<screen>sudo useradd -r -g ceph -u <replaceable>UID</replaceable> -s /sbin/nologin -d /var/lib/ceph ceph</screen>
        </step>
       </procedure>
      </listitem>
      <listitem>
       <para>
        If UID/GID 167 <emphasis>are</emphasis> already used but you do not
        want to create the <systemitem>ceph</systemitem> user and group
        manually, they will be created manually, but the UID/GID may vary
        across the cluster nodes. This can cause troubles later on when you
        want to move an OSD disk from one node to another.
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Restart &ceph;-related services:
     </para>
<screen>sudo sytemctl restart ceph.target</screen>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="ceph.upgrade.2.1to3.varlibceph">
   <title>Manually Change Ownership of <filename>/var/lib/ceph</filename></title>
   <para>
    The &ceph; daemons store their state in <filename>/var/lib/ceph</filename>. In &storage; version 1, 2 and 2.1 
    the daemons run as &rootuser;, and the <filename>/var/lib/ceph</filename> directory is owned by &rootuser;.
    Upgrading from any of these versions to &storage; 3 requires manually changing the
    ownership to ensure that the upgraded daemons can access this directory
    tree. 
   </para>
   <para>
    Before upgrading the &ceph; related packages, make sure that <option>CEPH_AUTO_RESTART_ON_UPGRADE</option>
    is set to "no" in <filename>/etc/sysconfig/ceph</filename>. 
   </para>
   <para>
    After upgrading the &ceph; related packages, do <emphasis>not</emphasis> restart <systemitem>ceph.target</systemitem>.
    Instead, follow these steps on each node:
   </para>
   <procedure>
    <step>
     <para>
     Stop &storage; 2.1 daemons: 
    </para>
    <screen>sudo systemctl stop ceph.target</screen>
   </step>
   <step>
    <para>
     Change the ownership of <filename>/var/lib/ceph</filename>:
    </para>
    <screen>sudo chown -R ceph:ceph /var/lib/ceph</screen>
   </step>
   <step>
    <para>
     Start &storage; 3 daemons:
    </para>
    <screen>sudo systemctl start ceph.target</screen>
   </step>
   </procedure>
  </sect2>
  <sect2 xml:id="ceph.upgrade.2.1to3.rgw">
   <title>&rgw; Instance Name Change</title>
   <para>
    If one or more instances of &rgw; are installed on the &ceph; cluster nodes,
    run the following commands on each of them to reflect the change in &rgw; instance name:
   </para>
   <procedure>
    <step>
     <para>
      Disable &storage; 2.1 &rgw; service:
     </para>
     <screen>sudo systemctl disable ceph-radosgw@<replaceable>instance_name</replaceable>.service</screen>
    </step>
    <step>
     <para>
      Enable &storage; 3 &rgw; service:
     </para>
     <screen>sudo systemctl enable ceph-radosgw@<replaceable>instance_name</replaceable>.service</screen>
    </step>
    <step>
     <para>
      Restart &ceph; services on each cluster node:
     </para>
     <screen>sudo systemctl restart ceph.target</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.1to2">
  <title>Upgrade from &storage; 1.0 to 2</title>

  <para>
   This section includes steps specific to upgrading &storage; version 1.0
   to 2.
  </para>

  <sect2 xml:id="ceph.upgrade.apache2civetweb">
   <title>Apache to Civetweb Migration</title>
   <para>
    While &storage; 1.0 used Apache as an application server for &rgw;
    client/server communication, &storage; 2 introduced a new application
    server called Civetweb. Civetweb is a lightweight embedded Web server,
    which makes &rgw; configuration easier.
   </para>
   <para>
    Although &rgw; in &storage; 2 still works well with Apache, support for
    Apache will end in future &storage; releases. You can migrate to
    Civetweb either manually, or using <command>ceph-deploy</command>.
   </para>
   <para>
    To migrate your existing Apache to Civetweb manually, follow these
    steps:
   </para>
   <procedure>
    <step>
     <para>
      Check if Apache is running on your system:
     </para>
<screen>sudo systemctl status apache2.service</screen>
    </step>
    <step>
     <para>
      Stop Apache if it is running:
     </para>
<screen>sudo systemctl stop apache2.service</screen>
    </step>
    <step>
     <para>
      Disable Apache automatic start:
     </para>
<screen>sudo systemctl disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.radosgw.gateway]
rgw frontends = "civetweb port=80"</screen>
    </step>
    <step>
     <para>
      Restart &rgw;:
     </para>
<screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway_name</replaceable>.service</screen>
    </step>
   </procedure>
   <para>
    To migrate your existing Apache to Civetweb using
    <command>ceph-deploy</command>, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      List &rgw; instances within the &ceph; cluster to get the instance
      name:
     </para>
<screen>ceph-deploy --overwrite-conf rgw list
node1:rgw1</screen>
    </step>
    <step>
     <para>
      Delete the existing &rgw; instance you want to migrate:
     </para>
<screen>ceph-deploy --overwrite-conf rgw delete node1:rgw1</screen>
    </step>
    <step>
     <para>
      Create a new CivetWeb based &rgw; instance:
     </para>
<screen>ceph-deploy --overwrite-conf rgw create node1:rgw1</screen>
     <tip>
      <para>
       If you still need to deploy Apache based &rgw; instead of the
       recommended CivetWeb, add the <option>--cgi</option> option:
      </para>
<screen>ceph-deploy --overwrite-conf rgw --cgi create node1:rgw1</screen>
      <note>
       <para>
        Apache listens on port 80 by default. If you need
        to deploy it on a different port, you need to manually cancel
        listening on the default port 80. You can do it by editing the
        <filename>/etc/apache2/listen.conf</filename> file, commenting the
        line that contains <literal>Listen 80</literal>, and restarting the
        service:
       </para>
<screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway</replaceable>.service</screen>
      </note>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
