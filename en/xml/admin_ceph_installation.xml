<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ceph.install">
 <title>Installation of Basic &ceph; Cluster</title>
 <para>
  This chapter outlines procedures to install and deploy the &ceph; cluster.
  The main focus is on deploying &ceph; either with the
  <command>ceph-deploy</command> command line utility, or the &crow;
  framework.
 </para>
 <sect1 id="ceph.install.ceph-deploy">
  <title>Deploying with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to ease the way
   you deploy &ceph; cluster in your small scale setups.
  </para>

  <sect2 id="ceph.install.ceph-deploy.layout">
   <title>&ceph; Layout</title>
   <para>
    Even if &ceph; can run with a minimum setup of two nodes (mainly for
    testing purposes), we recommend a setup of at least four nodes: three
    monitor and four OSD nodes.
   </para>
   <tip>
    <para>
     Although &ceph; nodes can be virtual machines, real hardware is
     strongly recommended for a production environment.
    </para>
   </tip>
  </sect2>

  <sect2 id="ceph.install.ceph-deploy.network">
   <title>Network Recommendations</title>
   <para>
    The network environment where you intend to run &ceph; should be split
    into a public part and trusted internal part. While the public part
    serves for providing the service to the customers, the internal part
    provides for the authenticated &ceph; network communication.
   </para>
   <para>
    The main reason is that although &ceph; ceph provides authentication and
    protection against attacks once secret keys are in place, the messages
    used to configure these keys may be transferred open and are vulnerable.
   </para>
  </sect2>

  <sect2 id="ceph.install.ceph-deploy.eachnode">
   <title>Preparing Each &ceph; Node</title>
   <para>
    Before deploying the &ceph; cluster, apply the following steps for each
    &ceph; node as &rootuser;:
   </para>
   <procedure>
    <step>
     <para>
      Install &sle; 12 and add the &storage; extension. It provides a
      software repository with the software needed to run &ceph;.
     </para>
    </step>
    <step>
     <para>
      Install NTP. We recommend installing the NTP time synchronization
      service on &ceph; nodes to prevent issues arising from clock drift:
     </para>
<screen>sudo zypper in ntp yast2-ntp-client</screen>
     <para>
      To configure NTP, go to <menuchoice><guimenu>&yast;</guimenu>
      <guimenu>Network Services</guimenu> <guimenu>NTP
      Configuration</guimenu></menuchoice>. Make sure to enable the NTP
      service (<command>systemctl enable ntpd.service &amp;&amp; systemctl
      start ntpd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Install SSH server. &ceph; uses SSH to log in to all cluster nodes.
      Make sure SSH is installed (<command>zypper in openssh</command>) and
      enabled (<command>systemctl enable sshd.service &amp;&amp; systemctl
      start sshd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Add a <systemitem>ceph</systemitem> user account, and set password for
      it. The admin node will log in to &ceph; nodes as this particular user
      <systemitem>ceph</systemitem>.
     </para>
<screen>useradd -m ceph &amp;&amp; passwd ceph</screen>
    </step>
    <step>
     <para>
      The admin node needs to have a passwordless SSH access to all &ceph;
      nodes. When <command>ceph-deploy</command> logs in to a &ceph; node as
      a <systemitem>ceph</systemitem> user, this user must have passwordless
      <command>sudo</command> privileges.
     </para>
     <para>
      Edit the <filename>/etc/sudoers</filename> file (with
      <command>visudo</command>) and add the following line to add the
      <command>sudo</command> command for the <literal>ceph</literal> user:
     </para>
<screen>ceph ALL = (root) NOPASSWD:ALL</screen>
    </step>
    <step>
     <para>
      On the admin node, become the <literal>ceph</literal> user, and enable
      passwordless ssh access to all other &ceph; nodes:
     </para>
<screen>su - ceph
ssh-keygen</screen>
     <para>
      You will be asked several questions. Leave the values at their
      defaults, and the passphrase empty.
     </para>
     <para>
      Copy the key to each &ceph; node:
     </para>
<screen>ssh-copy-id ceph@<replaceable>node1</replaceable>
ssh-copy-id ceph@<replaceable>node2</replaceable>
ssh-copy-id ceph@<replaceable>node3</replaceable></screen>
     <remark>2014-11-05 tbazant: how about ssh-agent and ssh add?</remark>
    </step>
   </procedure>
  </sect2>

  <sect2 id="ceph.install.ceph-deploy.purge">
   <title>Cleaning Previous &ceph; Environment</title>
   <para>
    If at any point during the &ceph; deployment you run into trouble and
    need to start over, or you want to make sure that any previous &ceph;
    configuration is removed, execute the following commands as
    <systemitem>ceph</systemitem> user to purge the previous &ceph;
    configuration.
   </para>
   <warning>
    <para>
     Be aware that <emphasis>purging</emphasis> previous &ceph; installation
     destroys stored data and access settings.
    </para>
   </warning>
<screen>ceph-deploy purge <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
ceph-deploy purgedata <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
ceph-deploy forgetkeys</screen>
   <para>
    As &rootuser;, run the following:
   </para>
<screen>zypper rm $(zypper --disable-system-resolvables -s 0 packages \
 -r SUSE-Storage | grep -v '^-' | tail -n +4 | cut -d'|' -f3 | sort -u)</screen>
  </sect2>

  <sect2 id="ceph.install.ceph-deploy.cephdeploy">
   <title>Running <command>ceph-deploy</command></title>
   <para>
    After you prepared each &ceph; node as described in
    <xref
  linkend="ceph.install.ceph-deploy.eachnode"/>, you are ready to
    deploy &ceph; with <command>ceph-deploy</command>. Note that the
    <command>ceph-deploy</command> utility is run from the admin node.
   </para>
   <tip>
    <title>Non-default Cluster Name</title>
    <para>
     If you need to install the cluster with <command>ceph-deploy</command>
     using other than the default <literal>cluster</literal> name, you have
     to initially specify it with <option>--cluster</option>, and then
     specify it in each <command>ceph-deploy</command> command related to
     that cluster:
    </para>
<screen>ceph-deploy --cluster my_cluster new [...]
ceph-deploy --ceph-conf my_cluster.conf mon create-initial
ceph-deploy --ceph-conf my_cluster.conf osd prepare [...]
ceph-deploy --ceph-conf my_cluster.conf osd activate [...]</screen>
    <para>
     Please note that using other than default cluster name is not supported
     by SUSE.
    </para>
   </tip>
   <procedure>
    <step>
     <para>
      Install <command>ceph-deploy</command>:
     </para>
<screen>sudo zypper in ceph-deploy</screen>
    </step>
    <step>
     <para>
      Disable IPv6. Open <filename>/etc/sysctl.conf</filename>, edit the
      following lines, and reboot the admin node:
     </para>
<screen>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</screen>
    </step>
    <step>
     <para>
      Because it is not recommended to run <command>ceph-deploy</command> as
      &rootuser;, become the <literal>ceph</literal> user:
     </para>
<screen>su - ceph</screen>
    </step>
    <step>
     <para>
      Run <command>ceph-deploy</command> to install &ceph; on each node:
     </para>
<screen>ceph-deploy install <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <tip>
      <para>
       <command>ceph-deploy</command> creates important files in the
       directory where you run it from. It is best to run
       <command>ceph-deploy</command> in an empty directory.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Set up the monitor nodes. Create keys and local configuration. The
      keys are used to authenticate and protect the communication between
      &ceph; nodes.
     </para>
<screen>ceph-deploy new <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <para>
      During this step, <command>ceph-deploy</command> creates local
      configuration files. It is recommended to inspect the configuration
      files in the current directory.
     </para>
     <tip>
      <title>Monitor Nodes on Different Subnets</title>
      <para>
       If the monitor nodes are not in the same subnet, you need to modify
       the <filename>ceph.conf</filename> in the current directory. For
       example, if the nodes have IP addresses
      </para>
<screen>10.121.9.186
10.121.10.186
10.121.11.186</screen>
      <para>
       add the following line to the global section of
       <filename>ceph.conf</filename>:
      </para>
<screen>public network = 10.121.0.0/16</screen>
      <para>
       Since you are likely to experience problems with IPv6 networking,
       please consider modifying the IPv6 mon_host settings, as in the
       following example:
      </para>
<screen>mon_host = [2620:...10:121:9:186,2620:...10:121:10:186,2620:...10:121:11:186]</screen>
      <para>
       into its IPv4 equivalent:
      </para>
<screen>mon_host = 10.121.9.186, 10.121.10.186, 10.121.11.186</screen>
     </tip>
    </step>
    <step>
     <para>
      Create the initial monitor service on already created monitor nodes:
     </para>
<screen>ceph-deploy mon create-initial</screen>
    </step>
    <step>
     <para>
      To test the status of the cluster, run
     </para>
<screen>ceph -k ceph.client.admin.keyring health</screen>
     <tip>
      <para>
       If you specify which node is the admin node, then you need not
       specify the keyring file each time you run the
       <command>ceph</command> command:
      </para>
<screen>ceph-deploy admin <replaceable>node</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information on firewall settings for &ceph; in
      <xref
     linkend="storage.bp.net.firewall"/>
     </para>
    </step>
    <step>
     <para>
      Create OSD daemons. Although you can use a directory as a storage, we
      recommend to create a separate disk dedicated to a &ceph; node. To
      find out the name of the disk device, run
     </para>
<screen>cat /proc/partitions
major minor  #blocks  name

 254        0   12582912 vda
 254        1    1532928 vda1
 254        2   11048960 vda2
  11        0    2831360 sr0
 254       16    4194304 vdb</screen>
     <para>
      In our case the <systemitem>vdb</systemitem> disk has no partitions,
      so it is most likely our newly created disk.
     </para>
     <para>
      Now set up the disk for &ceph;:
     </para>
<screen>ceph-deploy osd prepare <replaceable>node:vdb</replaceable></screen>
     <tip>
      <para>
       If there are already one or more partitions on the disk you want to
       use, you need to run
      </para>
<screen>ceph-deploy disk zap node:vdb</screen>
      <para>
       before running the <command>ceph-deploy osd prepare</command>
       command.
      </para>
     </tip>
     <note>
<!-- bnc#912479 -->
      <title>Default Filesystem for OSDs</title>
      <para>
       The default and recommended filesystem for OSDs is
       <literal>xfs</literal>. Additionally, only <literal>btrfs</literal>
       is supported. Use the <option>--fs-type</option> option to specify a
       filesystem other than the default:
      </para>
<screen>ceph-deploy osd prepare --fs-type btrfs <replaceable>node:vdb</replaceable></screen>
     </note>
     <para>
      Finally activate the OSD:
     </para>
<screen>ceph-deploy osd activate <replaceable>node:vdb1</replaceable></screen>
     <tip>
      <para>
       To join the functionality of <command>ceph-deploy osd
       prepare</command> and <command>ceph-deploy osd activate</command>,
       use <command>ceph-deploy osd create</command>.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 id="ceph.install.calamari">
   <title>Install Calamari</title>
   <para>
    Calamari is a management and monitoring system for &ceph; storage
    cluster. It provides a Web user interface that makes &ceph; cluster
    monitoring very simple and handy.
   </para>
   <para>
    To install Calamari, run the following commands as &rootuser;:
   </para>
   <procedure>
    <step>
     <para>
      Install the client part of Calamari:
     </para>
<screen># zypper in calamari-clients</screen>
    </step>
    <step>
     <para>
      Initialize Calamari installation. You will be asked for superuser
      username and password. These will be needed when logging in to the Web
      interface after the setup is complete.
     </para>
<screen># calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):  
Email address: 
Password: 
Password (again): 
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information on firewall settings for &ceph; in
      <xref
     linkend="storage.bp.net.firewall"/>
     </para>
    </step>
    <step>
     <para>
      Now open your Web browser and point it to the hostname/IP address of
      the server where you installed Calamari. Log in with the credentials
      you entered when installing the Calamari client. A welcome screen
      appears, instructing you to enter the <command>ceph-deploy calamari
      connect</command> command. Switch to the terminal on the Calamari host
      and enter the following command:
     </para>
<screen>ceph-deploy calamari --master <replaceable>master_host</replaceable> connect <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
     <para>
      After the command is successfully finished, reload the Web browser.
      Now you can monitor your &ceph; cluster, OSDs, pools, etc.
     </para>
     <tip>
      <para>
       They may be leftovers of the previous Calamari setup on the system.
       If, after logging in the Calamari application, some nodes are already
       joined or registered, run the following on the Calamari host to
       trigger a re-run of salt on all &ceph; nodes, which should clear up
       any odd state or missing bits and pieces.
      </para>
<screen>salt '*' state.highstate</screen>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
