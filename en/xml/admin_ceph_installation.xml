<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.install">
 <title>Installation of Basic &ceph; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter outlines procedures to install and deploy the &ceph; cluster.
  The main focus is on deploying &ceph; with the
  <command>ceph-deploy</command> command line utility.
 </para>
 <sect1 xml:id="ceph.install.ceph-deploy">
  <title>Deploying with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to ease the way
   you deploy &ceph; cluster in your small scale setups.
  </para>

  <sect2 xml:id="ceph.install.ceph-deploy.layout">
   <title>&ceph; Layout</title>
   <para>
    For testing purposes, a minimal &ceph; cluster can be made to run on a
    single node. However, in a production setup we recommend using at least
    four nodes: one admin node and three cluster nodes, each running one
    monitor daemon and some number of object storage daemons (OSDs).
   </para>
   <figure>
    <title>Minimal &ceph; Setup</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <para>
     Although &ceph; nodes can be virtual machines, real hardware is
     strongly recommended for the production environment.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.network">
   <title>Network Recommendations</title>
   <para>
    The network environment where you intend to run &ceph; should be split
    into a public part and trusted internal part. While the public part
    serves for providing the service to the customers, the internal part
    provides for the authenticated &ceph; network communication.
   </para>
   <para>
    The main reason is that although &ceph; authentication and protection
    against attacks once secret keys are in place, the messages used to
    configure these keys may be transferred open and are vulnerable.
   </para>
   <tip>
    <title>Nodes Configured via DHCP</title>
    <para>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various &ceph; daemons start. If this happens, the &ceph; MONs and OSDs
     will not start correctly (running <command>systemctl status
     ceph\*</command> will result in "unable to bind" errors), and Calamari
     may be unable to display graphs. To avoid this issue, we recommend
     increasing the DHCP client timeout to at least 30 seconds on each node
     in your storage cluster. This can be done by changing the following
     settings on each node:
    </para>
    <para>
     In <filename>/etc/sysconfig/network/dhcp</filename> set
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     In <filename>/etc/sysconfig/network/config</filename> set
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.eachnode">
   <title>Preparing Each &ceph; Node</title>
   <para>
    Before deploying the &ceph; cluster, apply the following steps for each
    &ceph; node as &rootuser;:
   </para>
   <procedure>
    <step>
     <para>
      Install &sle; 12 and add the &storage; extension. It provides a
      software repository with the software needed to run &ceph;.
     </para>
    </step>
    <step>
     <para>
      Install NTP. We recommend installing the NTP time synchronization
      service on &ceph; nodes to prevent issues arising from clock drift:
     </para>
<screen>sudo zypper in ntp yast2-ntp-client</screen>
     <para>
      To configure NTP, go to <menuchoice><guimenu>&yast;</guimenu>
      <guimenu>Network Services</guimenu> <guimenu>NTP
      Configuration</guimenu></menuchoice>. Make sure to enable the NTP
      service (<command>systemctl enable ntpd.service &amp;&amp; systemctl
      start ntpd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Install SSH server. &ceph; uses SSH to log in to all cluster nodes.
      Make sure SSH is installed (<command>zypper in openssh</command>) and
      enabled (<command>systemctl enable sshd.service &amp;&amp; systemctl
      start sshd.service</command>).
     </para>
    </step>
    <step>
     <para>
      Add a <systemitem>ceph</systemitem> user account, and set password for
      it. The admin node will log in to &ceph; nodes as this particular
      <systemitem>ceph</systemitem> user .
     </para>
<screen>useradd -m ceph &amp;&amp; passwd ceph</screen>
    </step>
    <step>
     <para>
      The admin node needs to have passwordless SSH access to all &ceph;
      nodes. When <command>ceph-deploy</command> logs in to a &ceph; node as
      a <systemitem>ceph</systemitem> user, this user must have passwordless
      <command>sudo</command> privileges.
     </para>
     <para>
      Edit the <filename>/etc/sudoers</filename> file (with
      <command>visudo</command>) and add the following line to add the
      <command>sudo</command> command for the <literal>ceph</literal> user:
     </para>
<screen>ceph ALL = (root) NOPASSWD:ALL</screen>
     <tip>
      <title>Disable <literal>requiretty</literal></title>
      <para>
       You may receive an error while trying to execute
       <command>ceph-deploy</command> commands. If
       <literal>requiretty</literal> is set by default, disable it by
       executing <command>sudo visudo</command> and locate the
       <literal>Defaults requiretty</literal> setting. Change it to<literal>
       Defaults:ceph !requiretty</literal> to ensure that
       <command>ceph-deploy</command> can connect using the
       <systemitem>ceph</systemitem> user and execute commands with
       <command>sudo</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      On the admin node, become the <literal>ceph</literal> user, and enable
      passwordless SSH access to all other &ceph; nodes:
     </para>
<screen>su - ceph
ssh-keygen</screen>
     <para>
      You will be asked several questions. Leave the values at their
      defaults, and the passphrase empty.
     </para>
     <para>
      Copy the key to each &ceph; node:
     </para>
<screen>ssh-copy-id ceph@<replaceable>node1</replaceable>
ssh-copy-id ceph@<replaceable>node2</replaceable>
ssh-copy-id ceph@<replaceable>node3</replaceable></screen>
     <tip>
      <title>Running <command>ceph-deploy</command> FROM a Different User Account Than <systemitem>ceph</systemitem></title>
      <para>
       It is possible to run the <command>ceph-deploy</command> command even
       if you are logged in as a different user than
       <systemitem>ceph</systemitem>. For this purpose, you need to set up
       an ssh alias in your <filename>~/.ssh/config</filename> file:
      </para>
<screen>[...]
Host ceph-node1
  Hostname ceph-node1
  User ceph</screen>
      <para>
       After this change, <command>ssh ceph-node1</command> automatically
       uses the <systemitem>ceph</systemitem> user to log in.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.purge">
   <title>Cleaning Previous &ceph; Environment</title>
   <para>
    If at any point during the &ceph; deployment you run into trouble and
    need to start over, or you want to make sure that any previous &ceph;
    configuration is removed, execute the following commands as
    <systemitem>ceph</systemitem> user to purge the previous &ceph;
    configuration.
   </para>
   <warning>
    <para>
     Be aware that <emphasis>purging</emphasis> previous &ceph; installation
     destroys stored data and access settings.
    </para>
   </warning>
<screen>ceph-deploy purge <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
ceph-deploy purgedata <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
ceph-deploy forgetkeys</screen>
<!-- 2015-11-09 tbazant: stage not needed
     (https://bugzilla.suse.com/show_bug.cgi?id=946762)
   <para>
    As &rootuser;, run the following:
   </para>
<screen>zypper rm $(zypper \-\-disable-system-resolvables -s 0 packages \
 -r SUSE-Enterprise-Storage-2-Pool SUSE-Enterprise-Storage-2-Updates \
 | grep -v '^-' | tail -n +4 | cut -d'|' -f3 | sort -u)</screen>
 -->
  </sect2>

  <sect2 xml:id="ceph.install.ceph-deploy.cephdeploy">
   <title>Running <command>ceph-deploy</command></title>
   <para>
    After you prepared each &ceph; node as described in
    <xref linkend="ceph.install.ceph-deploy.eachnode"/>, you are ready to
    deploy &ceph; with <command>ceph-deploy</command>. Note that the
    <command>ceph-deploy</command> utility is run from the admin node.
   </para>
   <procedure>
    <step>
     <para>
      Install <command>ceph</command> and <command>ceph-deploy</command>:
     </para>
<screen>sudo zypper in ceph ceph-deploy</screen>
    </step>
    <step>
     <para>
      Disable IPv6. Open <filename>/etc/sysctl.conf</filename>, edit the
      following lines, and reboot the admin node:
     </para>
<screen>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</screen>
    </step>
    <step>
     <para>
      Because it is not recommended to run <command>ceph-deploy</command> as
      &rootuser;, become the <literal>ceph</literal> user:
     </para>
<screen>su - ceph</screen>
    </step>
    <step>
     <para>
      Run <command>ceph-deploy</command> to install &ceph; on each node:
     </para>
<screen>ceph-deploy install <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <tip>
      <para>
       <command>ceph-deploy</command> creates important files in the
       directory where you run it from. It is best to run
       <command>ceph-deploy</command> in an empty directory.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Set up the monitor nodes. Create keys and local configuration. The
      keys are used to authenticate and protect the communication between
      &ceph; nodes.
     </para>
<screen>ceph-deploy new <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
     <para>
      During this step, <command>ceph-deploy</command> creates local
      configuration files. It is recommended to inspect the configuration
      files in the current directory.
     </para>
     <tip>
      <title>Monitor Nodes on Different Subnets</title>
      <para>
       If the monitor nodes are not in the same subnet, you need to modify
       the <filename>ceph.conf</filename> in the current directory. For
       example, if the nodes have IP addresses
      </para>
<screen>10.121.9.186
10.121.10.186
10.121.11.186</screen>
      <para>
       add the following line to the global section of
       <filename>ceph.conf</filename>:
      </para>
<screen>public network = 10.121.0.0/16</screen>
      <para>
       Since you are likely to experience problems with IPv6 networking,
       consider modifying the IPv6 mon_host settings, as in the following
       example:
      </para>
<screen>mon_host = [2620:...10:121:9:186,2620:...10:121:10:186,2620:...10:121:11:186]</screen>
      <para>
       into its IPv4 equivalent:
      </para>
<screen>mon_host = 10.121.9.186, 10.121.10.186, 10.121.11.186</screen>
     </tip>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is on, either turn it off with
     </para>
<screen>sudo /sbin/SuSEfirewall2 off</screen>
     <para>
      or, if you want to keep it on, enable the appropriate set of ports.
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Create the initial monitor service on already created monitor nodes:
     </para>
<screen>ceph-deploy mon create-initial</screen>
    </step>
    <step>
     <para>
      Check the firewall status
     </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
     <para>
      and if it is off, check its configuration and turn it on with
     </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
     <para>
      You can find detailed information in
      <xref linkend="storage.bp.net.firewall"/>.
     </para>
    </step>
    <step>
     <para>
      Create OSD daemons. Although you can use a directory as a storage, we
      recommend to create a separate disk dedicated to a &ceph; node. To
      find out the name of the disk device, run
     </para>
<screen>cat /proc/partitions
major minor  #blocks  name

 254        0   12582912 vda
 254        1    1532928 vda1
 254        2   11048960 vda2
  11        0    2831360 sr0
 254       16    4194304 vdb</screen>
     <para>
      In our case the <systemitem>vdb</systemitem> disk has no partitions,
      so it is most likely our newly created disk.
     </para>
     <para>
      Now set up the disk for &ceph;:
     </para>
<screen>ceph-deploy osd prepare <replaceable>node:vdb</replaceable></screen>
     <tip>
      <para>
       If there are already one or more partitions on the disk you want to
       use, you need to run
      </para>
<screen>ceph-deploy disk zap node:vdb</screen>
      <para>
       before running the <command>ceph-deploy osd prepare</command>
       command.
      </para>
     </tip>
     <note>
<!-- bnc#912479 -->
      <title>Default File System for OSDs</title>
      <para>
       The default and recommended file system for OSDs is
       <literal>xfs</literal>. Additionally, only <literal>btrfs</literal>
       is supported. Use the <option>--fs-type</option> option to specify a
       file system other than the default:
      </para>
<screen>ceph-deploy osd prepare --fs-type btrfs <replaceable>node:vdb</replaceable></screen>
     </note>
     <para>
      Optionally, activate the OSD:
     </para>
<screen>ceph-deploy osd activate <replaceable>node:vdb1</replaceable></screen>
     <tip>
      <para>
       To join the functionality of <command>ceph-deploy osd
       prepare</command> and <command>ceph-deploy osd activate</command>,
       use <command>ceph-deploy osd create</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      To test the status of the cluster, run
     </para>
<screen>ceph -k ceph.client.admin.keyring health</screen>
     <tip>
      <para>
       If you specify which node is the admin node, then you need not
       specify the keyring file each time you run the
       <command>ceph</command> command:
      </para>
<screen>ceph-deploy admin <replaceable>node</replaceable></screen>
     </tip>
   <tip>
    <title>Non-default Cluster Name</title>
    <para>
     If you need to install the cluster with <command>ceph-deploy</command>
     using a name other than the default <literal>cluster</literal> name,
     you need to initially specify it with <option>--cluster</option>, and
     then specify it in each <command>ceph-deploy</command> command related
     to that cluster:
    </para>
<screen>ceph-deploy --cluster my_cluster new [...]
ceph-deploy --ceph-conf my_cluster.conf mon create-initial
ceph-deploy --ceph-conf my_cluster.conf osd prepare [...]
ceph-deploy --ceph-conf my_cluster.conf osd activate [...]</screen>
    <para>
     Note that using a name other than default cluster name is not supported
     by SUSE.
    </para>
   </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.calamari">
  <title>Install Calamari</title>

  <para>
   Calamari is a management and monitoring system for &ceph; storage
   cluster. It provides a Web user interface that makes &ceph; cluster
   monitoring very simple and handy.
  </para>

  <para>
   To install Calamari, run the following commands as &rootuser;:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen># zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen># calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):  
Email address: 
Password: 
Password (again): 
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </step>
   <step>
    <para>
     Now open your Web browser and point it to the host name/IP address of
     the server where you installed Calamari. Log in with the credentials
     you entered when installing the Calamari client. A welcome screen
     appears, instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host
     and enter the following command:
    </para>
<screen>ceph-deploy calamari --master <replaceable>master_host</replaceable> connect <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your &ceph; cluster, OSDs, pools, etc.
    </para>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state&mdash;for
      example if a node goes offline&mdash;should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw
      heartbeat information from the cluster. Normally, this will not be
      more than one minute old, but in certain failure cases, for instance
      when a network outage occurs or if the cluster loses quorum (that is
      if more than half of the monitor nodes are down), Calamari will no
      longer be able to determine cluster state. In this case, the
      <guimenu>Health</guimenu> panel will indicate that the last update was
      more than one minute ago. After too long time with no updates,
      Calamari displays a warning at the top of the screen "Cluster Updates
      Are Stale. The Cluster is not updating Calamari." If this occurs, the
      other status information Calamari presents will not be correct so you
      should investigate further to check the status of your storage nodes
      and network.
     </para>
    </important>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to
      trigger a re-run of salt on all &ceph; nodes, which should clear up
      any odd state or missing bits and pieces.
     </para>
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup,
      such as state files, configuration files, or PostgreSQL database
      files. At minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
 </sect1>
</chapter>
