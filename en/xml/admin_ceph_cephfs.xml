<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.cephfs">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>Clustered File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that
  uses a &ceph; storage cluster to store its data. The &cephfs; uses the
  same cluster system as &ceph; block devices, &ceph; object storage with
  its S3 and Swift APIs, or native bindings
  (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>&ceph; Metadata Server</title>

  <para>
   A &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph;
   block devices and &ceph; object storage <emphasis>do not</emphasis> use
   MDS. MDS's make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adding a Metadata Server</title>
   <para>
    After you deploy OSD's and monitors, you can deploy metadata servers.
    Although MDS service can share a node with an OSD and/or monitor one,
    you are encouraged to deploy it on a separate cluster node for
    performance reasons.
   </para>
<screen>ceph-deploy mds create <replaceable>host-name</replaceable>:<replaceable>daemon-name</replaceable></screen>
   <para>
    You may optionally specify a daemon instance name if you need to run
    multiple daemons on a single server.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mdf.config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file. For detailed list of
    MDS related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
<!-- http://docs.ceph.com/docs/master/cephfs/createfs/ -->

  <title>&cephfs;</title>

  <para>
   Once you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you may create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2>
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>.
    When configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data
      loss in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as
      this will directly affect the observed latency of file system
      operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information on managing pools, see
    <xref linkend="ceph.pools"/>.
   </para>
   <para>
    To create the two required pools 'cephfs_data' and 'cephfs_metadata'
    with default settings for use with &cephfs;, run the following commands:
   </para>
<screen>&prompt.cephuser;ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
&prompt.cephuser;ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
  </sect2>
 </sect1>
</chapter>
