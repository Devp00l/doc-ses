<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.cephx">
 <title>Authentication with &cephx;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  To identify clients and protect against man-in-the-middle attacks, &ceph;
  provides its &cephx; authentication system. <emphasis>Clients</emphasis>
  in this context are either human users&mdash;such as the admin
  user&mdash;or &ceph;-related services/daemons, for example OSDs, monitors,
  or &rgw;s.
 </para>
 <note>
  <para>
   The &cephx; protocol does not address data encryption in transport, such
   as SSL/TLS.
  </para>
 </note>
 <sect1 xml:id="storage.cephx.arch">
<!-- http://docs.ceph.com/docs/master/architecture/#high-availability-authentication -->

  <title>Authentication Architecture</title>

  <para>
   &cephx; uses shared secret keys for authentication, meaning both the
   client and the monitor cluster have a copy of the client’s secret key.
   The authentication protocol enables both parties to prove to each other
   that they have a copy of the key without actually revealing it. This
   provides mutual authentication, which means the cluster is sure the user
   possesses the secret key, and the user is sure that the cluster has a
   copy of the secret key as well.
  </para>

  <para>
   A key scalability feature of &ceph; is to avoid a centralized interface
   to the &ceph; object store. This means that &ceph; clients can interact
   with OSDs directly. To protect data, &ceph; provides its &cephx;
   authentication system, which authenticates &ceph; clients.
  </para>

  <para>
   Each monitor can authenticate clients and distribute keys, so there is no
   single point of failure or bottleneck when using &cephx;. The monitor
   returns an authentication data structure that contains a session key for
   use in obtaining &ceph; services. This session key is itself encrypted
   with the client’s permanent secret key, so that only the client can
   request services from the &ceph; monitors. The client then uses the
   session key to request its desired services from the monitor, and the
   monitor provides the client with a ticket that will authenticate the
   client to the OSDs that actually handle data. &ceph; monitors and OSDs
   share a secret, so the client can use the ticket provided by the monitor
   with any OSD or metadata server in the cluster. &cephx; tickets expire,
   so an attacker cannot use an expired ticket or session key obtained
   wrongfully. This form of authentication will prevent attackers with
   access to the communications medium from either creating bogus messages
   under another client’s identity or altering another client’s
   legitimate messages, as long as the client secret key is not revealed
   before it expires.
  </para>

  <para>
   To use &cephx;, an administrator must set up clients/users first. In the
   following diagram, the
   <systemitem
    class="username">client.admin</systemitem> user invokes
   <command>ceph auth get-or-create-key</command> from the command line to
   generate a user name and secret key. &ceph;’s <command>auth</command>
   subsystem generates the user name and key, stores a copy with the
   monitor(s) and transmits the user’s secret back to the
   <systemitem class="username">client.admin</systemitem> user. This means
   that the client and the monitor share a secret key.
  </para>

  <figure>
   <title>Basic &cephx; Authentication</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   To authenticate with the monitor, the client passes in the user name to
   the monitor. The monitor generates a session key and encrypts it with the
   secret key associated to the user name and transmits the encrypted ticket
   back to the client. The client then decrypts the data with the shared
   secret key to retrieve the session key. The session key identifies the
   user for the current session. The client then requests a ticket related
   to the user, which is signed by the session key. The monitor generates a
   ticket, encrypts it with the user’s secret key and transmits it back to
   the client. The client decrypts the ticket and uses it to sign requests
   to OSDs and metadata servers throughout the cluster.
  </para>

  <figure>
   <title>&cephx; Authentication</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &cephx; protocol authenticates ongoing communications between the
   client machine and the &ceph; servers. Each message sent between a client
   and a server after the initial authentication is signed using a ticket
   that the monitors, OSDs and metadata servers can verify with their shared
   secret.
  </para>

  <figure>
   <title>&cephx; Authentication - MDS and OSD</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <important>
   <para>
    The protection offered by this authentication is between the &ceph;
    client and the &ceph; cluster hosts. The authentication is not extended
    beyond the &ceph; client. If the user accesses the &ceph; client from a
    remote host, &ceph; authentication is not applied to the connection
    between the user’s host and the client host.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="storage.cephx.keymgmt">
<!-- http://docs.ceph.com/docs/master/rados/operations/user-management/ -->

  <title>Key Management</title>

  <para>
   This section describes &ceph; client users and their authentication and
   authorization with the &ceph; storage cluster. <emphasis>Users</emphasis>
   are either individuals or system actors such as applications, which use
   &ceph; clients to interact with the &ceph; storage cluster daemons.
  </para>

  <para>
   When &ceph; runs with authentication and authorization enabled (enabled
   by default), you must specify a user name and a keyring containing the
   secret key of the specified user (usually via the command line). If you
   do not specify a user name, &ceph; will use
   <systemitem class="username">client.admin</systemitem> as the default
   user name. If you do not specify a keyring, &ceph; will look for a
   keyring via the keyring setting in the &ceph; configuration file. For
   example, if you execute the <command>ceph health</command> command
   without specifying a user name or keyring, &ceph; interprets the command
   like this:
  </para>

<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>

  <para>
   Alternatively, you may use the <literal>CEPH_ARGS</literal> environment
   variable to avoid reentering of the user name and secret.
  </para>

  <sect2 xml:id="storage.cephx.keymgmt.backgrnd">
   <title>Background Information</title>
   <para>
    Regardless of the type of &ceph; client (e.g., block device, object
    storage, file system, native API, etc.), &ceph; stores all data as
    objects within <emphasis>pools</emphasis>. &ceph; users need to have
    access to pools in order to read and write data. Additionally, &ceph;
    users must have execute permissions to use &ceph;'s administrative
    commands. The following concepts will help you understand &ceph; user
    management.
   </para>
   <sect3>
    <title>User</title>
    <para>
     A user is either an individual or a system actor such as an
     application. Creating users allows you to control who (or what) can
     access your &ceph; storage cluster, its pools, and the data within
     pools.
    </para>
    <para>
     &ceph; uses <emphasis>types</emphasis> of users. For the purposes of
     user management, the type will always be <literal>client</literal>.
     &ceph; identifies users in period (.) delimited form, consisting of the
     user type and the user ID. For example, <literal>TYPE.ID</literal>,
     <literal>client.admin</literal>, or <literal>client.user1</literal>.
     The reason for user typing is that &ceph; monitors, OSDs, and metadata
     servers also use the cephx protocol, but they are not clients.
     Distinguishing the user type helps to distinguish between client users
     and other users, streamlining access control, user monitoring and
     traceability.
    </para>
    <note>
     <para>
      A &ceph; storage cluster user is not the same as a &ceph; object
      storage user or a &ceph; file system user. The &ceph; object gateway
      uses a &ceph; storage cluster user to communicate between the gateway
      daemon and the storage cluster, but the gateway has its own user
      management functionality for end users. The &ceph; file system uses
      POSIX semantics. The user space associated with it is not the same as
      a &ceph; storage cluster user.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Authorization and Capabilities</title>
    <para>
     &ceph; uses the term 'capabilities' (caps) to describe authorizing an
     authenticated user to exercise the functionality of the monitors, OSDs
     and metadata servers. Capabilities can also restrict access to data
     within a pool or a namespace within a pool. A &ceph; administrative
     user sets a user's capabilities when creating or updating a user.
    </para>
    <para>
     Capability syntax follows the form:
    </para>
<screen><replaceable>daemon-type</replaceable> 'allow <replaceable>capability</replaceable>' [<replaceable>daemon-type</replaceable> 'allow <replaceable>capability</replaceable>']</screen>
    <para>
     Following is a list of capabilities for each service type:
    </para>
    <variablelist>
     <varlistentry>
      <term>Monitor capabilities</term>
      <listitem>
       <para>
        include <literal>r</literal>, <literal>w</literal>,
        <literal>x</literal> and <literal>allow profile
        <replaceable>cap</replaceable></literal>.
       </para>
<screen>mon 'allow rwx'
mon 'allow profile osd'</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>OSD capabilities</term>
      <listitem>
       <para>
        include <literal>r</literal>, <literal>w</literal>,
        <literal>x</literal>, <literal>class-read</literal>, 
        <literal>class-write</literal> and <literal>profile osd</literal>.
        Additionally, OSD capabilities also allow for pool and namespace settings.
       </para>
       <screen>osd 'allow <replaceable>capability</replaceable>' [pool=<replaceable>poolname</replaceable>] [namespace=<replaceable>namespace-name</replaceable>]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>MDS capability</term>
      <listitem>
       <para>
        simply requires <literal>allow</literal>, or blank. 
       </para>
       <screen>mds 'allow'</screen>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The following entries describe each capability:
    </para>
    <variablelist>
     <varlistentry>
      <term>allow</term>
      <listitem>
       <para>
        Precedes access settings for a daemon. Implies <literal>rw</literal> for MDS only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>r</term>
      <listitem>
       <para>
        Gives the user read access. Required with monitors to retrieve the CRUSH map.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>w</term>
      <listitem>
       <para>
        Gives the user write access to objects.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>x</term>
      <listitem>
       <para>
        Gives the user the capability to call class methods (both read and write) and to conduct <literal>auth</literal> operations on monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>class-read</term>
      <listitem>
       <para>
        Gives the user the capability to call class read methods. Subset of <literal>x</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>class-write</term>
      <listitem>
       <para>
        Gives the user the capability to call class write methods. Subset of <literal>x</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>*</term>
      <listitem>
       <para>
        Gives the user read, write and execute permissions for a particular daemon/pool, and the ability to execute admin commands.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile osd</term>
      <listitem>
       <para>
        Gives a user permissions to connect as an OSD to other OSDs or monitors. Conferred on OSDs to enable OSDs to handle replication heartbeat traffic and status reporting.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile mds</term>
      <listitem>
       <para>
        Gives a user permissions to connect as a MDS to other MDSs or monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile bootstrap-osd</term>
      <listitem>
       <para>
        Gives a user permissions to bootstrap an OSD. Delegated to deployment tools such as <command>ceph-disk</command>, <command>ceph-deploy</command> so that they have permissions to add keys when bootstrapping an OSD.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile bootstrap-mds</term>
      <listitem>
       <para>
        Gives a user permissions to bootstrap a metadata server. Delegated to deployment tools such as ceph-deploy so they have permissions to add keys when bootstrapping a metadata server.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Pools</title>
    <para> A pool is a logical partition where users store data. In &ceph;
     deployments, it is common to create a pool as a logical partition for
     similar types of data. For example, when deploying &ceph; as a back-end for
     &ostack;, a typical deployment would have pools for volumes, images,
     backups and virtual machines, and users such as <systemitem class="username">client.glance</systemitem> or
     <systemitem class="username">client.cinder</systemitem>.
    </para>
   </sect3>
  </sect2>
  <sect2 xml:id="storage.cephx.keymgmt.usermgmt">
   <title>Managing Users</title>
   <para>
    User management functionality provides &ceph; cluster administrators with the ability to create, update and delete users
    directly in the &ceph; cluster.
   </para>
   <para>
    When you create or delete users in the &ceph; cluster, you may need to distribute keys to clients so that they can be added to keyrings. See <xref linkend="storage.cephx.keymgmt.keyringmgmt"/> for details.
   </para>
   <sect3>
    <title>List Users</title>
    <para>
     To list the users in your cluster, execute the following:
    </para>
    <screen>ceph auth list</screen>
    <para>
     &ceph; will list out all users in your cluster. For example, in a cluster with two nodes, <command>ceph auth list</command>
     output looks similar to this: 
    </para>
    <screen>installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
