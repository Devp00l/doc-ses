<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ceph.erasure">
 <title>Erasure Code</title>
 <para>
  A &ceph; pool is associated to a type to sustain the loss of an OSD (that is
  a disk since most of the time there is one OSD per disk). The default
  choice when creating a pool is replicated, meaning every object is copied
  on multiple disks. The Erasure Code pool type can be used instead to save
  space.
 </para>
 <para>
  For background information on Erasure Code, see
  <ulink
  url="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   You cannot access erasure coded pools with the rbd interface unless you
   have a cache tier set up. Refer to
   <xref linkend="ceph.tier.erasure"/> for more details.
  </para>
 </note>
 <sect1>
  <title>Creating a Sample Erasure Coded Pool</title>

  <para>
   The simplest erasure coded pool is equivalent to RAID5 and requires at
   least three hosts:
  </para>

<screen><prompt>> </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created
<prompt>> </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt>> </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The <literal>12</literal> in the <command>pool create</command> command
   stands for the number of placement groups.
  </para>
 </sect1>
 <sect1>
  <title>Erasure Code Profiles</title>

  <para>
   Some terminology hints:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      when the encoding function is called, it returns chunks of the same
      size. Data chunks which can be concatenated to reconstruct the
      original object and coding chunks which can be used to rebuild a lost
      chunk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      the number of data chunks, that is the number of chunks in which the
      original object is divided. For instance if <literal>k = 2</literal> a
      10KB object will be divided into <literal>k</literal> objects of 5KB
      each.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The default erasure code profile sustains the loss of a single OSD. It is
   equivalent to a replicated pool of size two but requires 1.5TB instead of
   2TB to store 1TB of data. The default profile can be displayed with:
  </para>

<screen><prompt>> </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
ruleset-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Choosing the right profile is important because it cannot be modified
   after the pool is created: a new pool with a different profile needs to
   be created and all objects from the previous pool moved to the new.
  </para>

  <para>
   The most important parameters of the profile are <literal>k</literal>,
   <literal>m</literal> and <literal>ruleset-failure-domain</literal>
   because they define the storage overhead and the data durability. For
   instance, if the desired architecture must sustain the loss of two racks
   with a storage overhead of 40% overhead, the following profile can be
   defined:
  </para>

<screen><prompt>> </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   ruleset-failure-domain=rack
<prompt>> </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt>> </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt>> </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The NYAN object will be divided in three (<literal>k=3</literal>) and two
   additional chunks will be created (<literal>m=2</literal>). The value of
   <literal>m</literal> defines how many OSD can be lost simultaneously
   without losing any data. The
   <literal>ruleset-failure-domain=rack</literal> will create a CRUSH
   ruleset that ensures no two chunks are stored in the same rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   For more information about the erasure code profiles, see
   <ulink
   url="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/>.
  </para>
 </sect1>
</chapter>
