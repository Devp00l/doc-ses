<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.storage.bestpractice">
 <title>Best Practice</title>
 <para>
  This chapter introduces a list of selected topics which you can encounter
  when managing &ceph; environment. To every topic there is a recommended
  solution that helps you understand or fix the existing problem. The topics
  are sorted into relevant categories.
 </para>
 <sect1 id="storage.bp.hwreq">
  <title>Hardware Requirements</title>

  <para></para>

  <sect2 id="ses.bp.diskshare">
   <title>OSD and Monitor Sharing One Server</title>
   <para>
    Although it is technically possible to run OSDs and monitor nodes on the
    same server in test environments, we strongly recommend having a
    separate server for each monitor node in production. The main reason is
    performance - the more OSDs the cluster has, the more I/O operations the
    monitor nodes have to perform. And when one server is shared between a
    monitor node and OSD(s), the OSD(s) I/O operations are a limiting factor
    for the monitor node.
   </para>
   <para>
    Another aspect is whether to share disks between an OSD, a monitor node,
    and the operating system on the server. The answer is simple: if
    possible, dedicate a separate disk to OSD, and a separate server to a
    monitor node.
   </para>
   <para>
    Although &ceph; supports directory-based OSDs, OSD should always have a
    dedicated disk other than the operating system one.
   </para>
   <tip>
    <para>
     If it is <emphasis>really</emphasis> necessary to run OSD and monitor
     node on the same server, run the monitor on a separate disk by mounting
     the disk to the <filename>/var/lib/ceph/mon</filename> directory for
     slightly better performance.
    </para>
   </tip>
  </sect2>

  <sect2 id="ses.bp.numofdisks">
   <title>How Many Disks Can I Have in a Server</title>
   <para>
    You can have as many disks in one server as it allows. There are few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for
      the disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> For optimum performance, reserve at least
      2GB of RAM per each terabyte of the disk space installed.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails,
      the more disks it has, the more OSDs the cluster temporarily loses.
      Moreover, to keep the replication rules running, you need to copy all
      the data from the failed server between the other nodes in the
      cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.inst">
  <title>Installation</title>

  <para></para>

  <sect2 id="storage.bp.inst.add_osd_cephdisk">
   <title>Adding OSDs with <command>ceph-disk</command></title>
   <para>
    <command>ceph-disk</command> is a utility that can prepare and activate
    a disk, partition or directory as a &ceph; OSD. It automates the
    multiple steps involved in manual creation and start of an OSD into two
    steps of preparing and activating the OSD by using the subcommands
    <command>prepare</command> and <command>activate</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>prepare</command>
     </term>
     <listitem>
      <para>
       Prepares a directory, disk or drive for a &ceph; OSD. It creates a
       GPT partition, marks the partition with &ceph; type uuid, creates a
       file system, marks the file system as ready for &ceph; consumption,
       uses entire partition and adds a new partition to the journal disk.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>activate</command>
     </term>
     <listitem>
      <para>
       Activate the &ceph; OSD. It mounts the volume in a temporary
       location, allocates an OSD id (if needed), remounts in the correct
       location
       <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></filename>
       and starts <command>ceph-osd</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following example shows steps to adding an OSD with
    <command>ceph-osd</command>.
   </para>
   <procedure>
    <step>
     <para>
      Make sure a new disk is physically present on the node where you want
      to add the OSD. In our example, it is <emphasis>node1</emphasis>
      belonging to cluster <emphasis>ceph</emphasis>.
     </para>
    </step>
    <step>
     <para>
      <command>ssh</command> to node1.
     </para>
    </step>
    <step>
     <para>
      Generate a unique identification for the new OSD:
     </para>
<screen>uuidgen
c70c032a-6e88-4962-8376-4aa119cb52ee</screen>
    </step>
    <step>
     <para>
      Prepare the disk:
     </para>
<screen>sudo ceph-disk prepare --cluster ceph \
--cluster-uuid c70c032a-6e88-4962-8376-4aa119cb52ee --fs-type xfs /dev/hdd1</screen>
    </step>
    <step>
     <para>
      Activate the OSD:
     </para>
<screen>sudo ceph-disk activate /dev/hdd1</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="storage.bp.inst.add_osd_cephdeploy">
   <title>Adding OSDs with <command>ceph-deploy</command></title>
   <para>
    <command>ceph-deploy</command> is a command line utility to ease the
    installation and configuration of a &ceph; cluster. It can be used to
    add or remove OSDs as well. To add a new OSD to a node
    <literal>node2</literal> with <command>ceph-deploy</command>, follow
    these steps:
   </para>
   <tip>
    <para>
     <command>ceph-deploy</command> is usually run from the administration
     node, from which you installed the cluster.
    </para>
   </tip>
   <procedure>
    <step>
     <para>
      Lists available disks on a node:
     </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/vdc other, unknown</screen>
     <para>
      <filename>/dev/vdc</filename> seems to be unused, so let us focus on
      adding it as an OSD.
     </para>
    </step>
    <step>
     <para>
      Zap the disk. Zapping deletes the disk's partition table.
     </para>
<screen>ceph-deploy disk zap node2:vdc</screen>
     <warning>
      <para>
       Zapping deletes all data from the disk
      </para>
     </warning>
    </step>
    <step>
     <para>
      Prepare the OSD. The <command>prepare</command> expects you to specify
      the disk for data, and optionally the disk for its journal. We
      recommend storing the journal on a separate drive to maximize
      throughput.
     </para>
<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
    </step>
    <step>
     <para>
      Activate the OSD. The <command>activate</command> command will cause
      your OSD to come <literal>up</literal> and be placed
      <literal>in</literal> the cluster. The <command>activate</command>
      command uses the path to the partition created when running the
      <command>prepare</command> command.
     </para>
<screen>ceph-deploy osd activate node2:/dev/vdc1:/dev/ssd1</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="storage.bp.inst.add_rm_monitor">
   <title>Adding and Removing Monitors</title>
   <para>
    With <command>ceph-deploy</command>, adding and removing monitors is a
    simple task. Also, take into account the following
    restrictions/recommendation.
   </para>
   <important>
    <itemizedlist>
     <listitem>
      <para>
       <command>ceph-deploy</command> restricts you to only install one
       monitor per host.
      </para>
     </listitem>
     <listitem>
      <para>
       We do not recommend mixing monitors and OSDs on the same host.
      </para>
     </listitem>
     <listitem>
      <para>
       For high availability, you should run a production &ceph; cluster
       with <emphasis>at least</emphasis> three monitors.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <sect3 id="storage.bp.inst.add_rm_monitor.addmon">
    <title>Adding a Monitor</title>
    <para>
     Once you create a cluster and install &ceph; packages to the monitor
     host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
     information), you may deploy the monitors to the monitor hosts. You may
     specify more monitor hostnames in the same command.
    </para>
<screen>ceph-deploy mon create <replaceable>host-name</replaceable></screen>
    <note>
     <para>
      When adding a monitor on a host that was not in hosts initially
      defined with the <command>ceph-deploy new</command> command, a
      <option>public network</option> statement needs to be added to the
      <filename>ceph.conf</filename> file.
     </para>
    </note>
   </sect3>
   <sect3 id="storage.bp.inst.add_rm_monitor.rmmon">
    <title>Removing a Monitor</title>
    <para>
     If you have a monitor in your cluster that you want to remove, you may
     use the destroy option. You may specify more monitor hostnames in the
     same command.
    </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
    <note>
     <para>
      Ensure that if you remove a monitor, the remaining monitors will be
      able to establish a consensus. If that is not possible, consider
      adding a monitor before removing the monitor you would like to take
      offline.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.monitoring">
  <title>Monitoring</title>

  <para></para>

  <sect2 id="storage.bp.monitoring.osd">
   <title>Checking if OSD Daemons are Running on a Node</title>
   <para>
    To check the status of OSD services on a specific node, log in to the
    node, and run the following:
   </para>
<screen>sudo systemctl status ceph-osd*
ceph-osd@0.service - Ceph object storage daemon
   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
   Active: active (running) since Fri 2015-02-20 11:13:18 CET; 2 days ago
 Main PID: 1822 (ceph-osd)
   CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
           └─1822 /usr/bin/ceph-osd -f --cluster ceph --id 0</screen>
   <para>
    For more information, see <xref linkend="ceph.operating.services"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.monitoring.mon">
   <title>Checking if Monitor Daemons are Running on a Node</title>
   <para>
    To check the status of monitor services on a specific node, log in to
    the node, and run the following:
   </para>
<screen>sudo systemctl status ceph-mon*
ceph-mon@doc-ceph1.service - Ceph cluster monitor daemon
   Loaded: loaded (/usr/lib/systemd/system/ceph-mon@.service; enabled)
   Active: active (running) since Wed 2015-02-18 16:57:17 CET; 4 days ago
 Main PID: 1203 (ceph-mon)
   CGroup: /system.slice/system-ceph\x2dmon.slice/ceph-mon@doc-ceph1.service
           └─1203 /usr/bin/ceph-mon -f --cluster ceph --id doc-ceph1</screen>
   <para>
    For more information, see <xref linkend="ceph.operating.services"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.monitoring.diskfails">
   <title>What Happens When a Disk Fails?</title>
   <para>
    When a disk with a stored cluster data has a hardware problem and fails
    to operate, here is what happens:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The related OSD crashed and is automatically removed from the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The failed disk's data are replicated to another OSD in the cluster
      from other copies of the same data stored in other OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Then you should remove the disk from the cluster &crushmap;, and
      physically from the host hardware.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="storage.bp.monitoring.journalfails">
   <title>What Happens When a Journal Disk Fails?</title>
   <para>
    &ceph; OSDs use journaling file systems (see
    <ulink
   url="http://en.wikipedia.org/wiki/Journaling_file_system"/>
    for more information) to store data. When a disk dedicated to a journal
    fails, the related OSD(s) fail as well (see
    <xref
   linkend="storage.bp.monitoring.diskfails"/>.
   </para>
   <warning>
    <title>Hosting Multiple Journals on One Disk</title>
    <para>
     For performance boost, you can use a fast disk (such as SSD) to store
     journal partitions for several OSDs. We do not recommend to host
     journals for more than 4 OSDs on one disk, because in case of the
     journals disk failure, you risk losing stored data for all the related
     OSDs' disks.
    </para>
   </warning>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.disk">
  <title>Disk Management</title>

  <para></para>

  <sect2 id="storage.bp.disk.add">
   <title>Adding Disks</title>
   <important>
    <itemizedlist>
     <listitem>
      <para>
       This can be done on a live cluster without downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       This will cause increased replication traffic between servers.
      </para>
     </listitem>
     <listitem>
      <para>
       Doing this operation repeatedly before the last operation has
       completed replication can save the cluster overall rebuild time.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <para>
    To add a disk (<filename>/dev/sdd</filename> in our example) to a &ceph;
    cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Create a partition <literal>sdd1</literal> on the disk:
     </para>
<screen>sudo parted /dev/sdd1 mkpart primary 0.0 -1s</screen>
    </step>
    <step>
     <para>
      Format the partition with either the XFS (strongly recommended) or
      Btrfs filesystem:
     </para>
<screen>sudo mkfs.xfs -f /dev/sdd1</screen>
    </step>
    <step>
     <para>
      Find out the UUID (Universally Unique Identifier) of the disk:
     </para>
<screen>ls -l /dev/disk/by-uuid | grep sdd1
[...] 04bb24f1-d631-47ff-a2ee-22d94ad4f80c -> ../../sdd1</screen>
    </step>
    <step>
     <para>
      Add the corresponding line to <filename>/etc/fstab</filename> for the
      example disk <literal>osd.12</literal>:
     </para>
<screen>[...]
UUID=04bb24f1-d631-47ff-a2ee-22d94ad4f80c /mnt/osd.12 xfs \
defaults,errors=remount-ro 0 1
[...]</screen>
    </step>
    <step>
     <para>
      Mount the disk:
     </para>
<screen>sudo mount /mnt/osd.12</screen>
    </step>
    <step>
     <para>
      Add the new disk to <filename>/etc/ceph/ceph.conf</filename> and copy
      the updated configuration file to all other nodes in the cluster.
     </para>
    </step>
    <step>
     <para>
      Create the OSD.
     </para>
<screen>ceph osd create 04bb24f1-d631-47ff-a2ee-22d94ad4f80c</screen>
    </step>
    <step>
     <para>
      Make sure that the new OSD is accepted into the cluster:
     </para>
<screen>sudo mkdir /srv/ceph/04bb24f1-d631-47ff-a2ee-22d94ad4f80c
ceph-osd -i 12 --mkfs --mkkey
ceph auth add osd.12 osd 'allow *' mon 'allow rwx' -i /etc/ceph/keyring.osd.12</screen>
    </step>
    <step>
     <para>
      Start the newly added OSD:
     </para>
<screen>sudo systemctl start ceph-osd@12.service</screen>
    </step>
    <step>
     <para>
      Add it to the cluster and allow replication based on &crushmap;:
     </para>
<screen>ceph osd crush set 12 osd.12 1.0 \
pool=<replaceable>pool_name</replaceable> rack=<replaceable>rack_name</replaceable> host=<replaceable>host_name</replaceable>-osd</screen>
    </step>
    <step>
     <para>
      Check that the new OSD is in the right place within the cluster:
     </para>
<screen>ceph osd tree</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="storage.bp.disk.del">
   <title>Deleting disks</title>
   <important>
    <itemizedlist>
     <listitem>
      <para>
       This can be done on a live cluster without downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       This will cause increased replication traffic between servers.
      </para>
     </listitem>
     <listitem>
      <para>
       Be sure not to remove too many disks from your cluster to be able to
       keep the replication rules. See <xref linkend="datamgm.rules"/> for
       more information.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <para>
    To delete a disk (for example <literal>osd.12</literal>) from a &ceph;
    cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Make sure you have the right disk:
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      If the disk is a member of a pool and/or active:
     </para>
     <substeps>
      <step>
       <para>
        Mark the disk out:
       </para>
<screen>ceph osd out 12</screen>
      </step>
      <step>
       <para>
        Wait for data migration to complete with <command>ceph -w</command>,
        then stop it:
       </para>
<screen>sudo systemctl stop ceph-osd@12.service</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Remove the disk from &crushmap;:
     </para>
<screen>ceph osd crush remove osd.12</screen>
    </step>
    <step>
     <para>
      Remove authentication information for the disk:
     </para>
<screen>ceph auth del osd.12</screen>
    </step>
    <step>
     <para>
      Remove the disk from the cluster:
     </para>
<screen>ceph osd rm 12</screen>
    </step>
   </procedure>
   <tip>
    <para>
     The process of preparing/adding a disk can be simplified with the
     <command>ceph-disk</command> command. See
     <ulink url="http://ceph.com/docs/master/man/8/ceph-disk/"/> for more
     information on <command>ceph-disk</command>.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.recover">
  <title>Recovery</title>

  <para></para>

  <sect2 id="storage.bp.recover.stalecalamari">
   <title>Calamari Has a Stale Cluster</title>
   <para>
    The Calamari backend supports operating multiple clusters, while its
    frontend does not yet. This means that if you point Calamari at one
    cluster, then destroy that cluster and create a new one, and then point
    the same Calamari instance at the new cluster, it will still remember
    the old cluster and possibly/probably try to display the old cluster
    state by default.
   </para>
   <para>
    To make Calamari 'forget' the old cluster, run:
   </para>
<screen>sudo calamari-ctl clear --yes-i-am-sure
sudo calamari-ctl initialize</screen>
   <para>
    This will make Calamari forget all the old clusters it knows about. It
    will, however, not clear out the salt minion keys from the master. This
    is fine if you're reusing the same nodes for the new cluster.
   </para>
  </sect2>

  <sect2 id="storage.bp.recover.stuckinactive">
   <title>'<emphasis>nn</emphasis> pg stuck inactive' Status Message</title>
   <para>
    If you receive <literal>stuck inactive</literal> status message after
    running <command>ceph status</command>, it means that &ceph; does not
    know where to replicate the stored data to fulfill the replication
    rules. It can happen shortly after the initial &ceph; setup and fix
    itself automatically. In other cases, this may require a manual
    interaction, such as bringing up a broken OSD, or adding a new OSD to
    the cluster. In very rare cases, reducing the replication level may
    help.
   </para>
  </sect2>

  <sect2 id="storage.bp.recover.osdweight">
   <title>OSD Weight is 0</title>
   <para>
    When OSD starts, it is assigned a weight. The higher the weight, the
    bigger the chance that the cluster writes data to the OSD. The weight is
    either specified in a cluster &crushmap;, or calculated by the OSD's
    start-up script.
   </para>
   <para>
    In some cases, the calculated value for OSD's weight may be rounded down
    to zero. It means that the OSD is not scheduled to store data, and no
    data are written to it. The reason is usually that the disk is too small
    (smaller than 15GB) and should be replaced with a bigger one.
   </para>
  </sect2>

  <sect2 id="storage.bp.recover.osddown">
   <title>OSD is Down</title>
   <para>
    OSD daemon is either running, or stopped/down. There are 3 general
    reasons why an OSD is down:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Hard disk failure.
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD crashed.
     </para>
    </listitem>
    <listitem>
     <para>
      The server crashed.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You can see the detailed status of OSDs by running
   </para>
<screen>ceph osd tree
# id  weight  type name up/down reweight
-1    0.02998  root default
-2    0.009995 host doc-ceph1
0     0.009995      osd.0 up  1 
-3    0.009995 host doc-ceph2
1     0.009995      osd.1 up  1 
-4    0.009995 host doc-ceph3
2     0.009995      osd.2 down  1</screen>
   <para>
    The example listing shows that the <literal>osd.2</literal> is down.
    Then you may check if the disk where the OSD is located is mounted:
   </para>
<screen>lsblk -f
[...]
vdb                      
├─vdb1               /var/lib/ceph/osd/ceph-2
└─vdb2</screen>
   <para>
    You can track the reason why the OSD is down by inspecting its log file
    <filename>/var/log/ceph/ceph-osd.2.log</filename>. After you find and
    fix the why the OSD i snot running, start it with
   </para>
<screen>sudo systemctl start ceph-osd@2.service</screen>
   <para>
    Do not forget to replace <literal>2</literal> with the actual number of
    your stopped OSD.
   </para>
  </sect2>

  <sect2 id="storage.bp.recover.clockskew">
   <title>Fixing Clock Skew Errors</title>
   <para>
    The time information in all cluster nodes must be synchronized. If a
    node's time is not fully synchronized, you may get clock skew errors
    when checking the state of the cluster.
   </para>
   <para>
    Time synchronization is managed with NTP (see
    <ulink
   url="http://en.wikipedia.org/wiki/Network_Time_Protocol"/>).
    Set each node to synchronize their time with one or more NTP servers,
    preferably to the same group of NTP servers. If the time skew still
    occurs on a node, follow these steps to fix it:
   </para>
<screen>sudo rcntpd stop
sudo rcceph stop
sudo rcntpd start
sudo rcceph start</screen>
   <para>
    You can then query the NTP peers and check the time offset with
    <command>sudo ntpq -p</command>.
   </para>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.account">
  <title>Accountancy</title>

  <para></para>

  <sect2 id="storage.bp.account.swiftadd">
   <title>Adding &swift; Users</title>
   <para>
    &swift; is a standard for stored data access compatible with &ostack;.
    It is used to interact with the &ceph; Object Gateway. You need to
    create a &swift; user, access key and secret to enable end users to
    interact with the gateway. There are two types of users: a
    <emphasis>user</emphasis> and <emphasis>subuser</emphasis>. While
    <emphasis>users</emphasis> are used when interacting with the S3
    interface, <emphasis>subusers</emphasis> are users of the &swift;
    interface. Each subuser is associated to a user.
   </para>
   <procedure>
    <step>
     <para>
      To create a &swift; user&mdash;which is a <emphasis>subuser</emphasis>
      in our terminology&mdash;you need to create the associated
      <emphasis>user</emphasis> first.
     </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
--display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
     <para>
      For example:
     </para>
<screen>sudo radosgw-admin user create \
  --uid=example_user \
  --display-name="Example User" \
  --email=penguin@example.com</screen>
    </step>
    <step>
     <para>
      To create a subuser (&swift; interface) for the user, you must specify
      the user ID (--uid=<replaceable>username</replaceable>), a subuser ID,
      and the access level for the subuser.
     </para>
<screen>radosgw-admin subuser create --uid=<replaceable>uid</replaceable> \
--subuser=<replaceable>uid</replaceable> \
--access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
     <para>
      For example:
     </para>
<screen>radosgw-admin subuser create --uid=example_user \
--subuser=example_user:swift --access=full</screen>
    </step>
    <step>
     <para>
      Generate a secret key for the user.
     </para>
<screen>sudo radosgw-admin key create \
  --gen-secret \
  --subuser=example_user:swift \
  --key-type=swift</screen>
    </step>
    <step>
     <para>
      Both commands will output JSON-formatted data showing the user state.
      Notice the following lines, and remember the
      <literal>secret_key</literal> value:
     </para>
<screen>"swift_keys": [
  { "user": "example_user:swift",
    "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
    </step>
   </procedure>
   <para>
    For more information on using &swift; client, see
    <xref
   linkend="swift_user.create"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.account.swiftrm">
   <title>Removing &swift; Users</title>
   <para>
    When you remove a user, the user and subuser are removed from the
    system. However, you may remove just the subuser if you wish. To remove
    a user (and subuser), specify <option>user rm</option> and the user ID.
   </para>
<screen>radosgw-admin user rm --uid=example_user</screen>
   <para>
    To remove the subuser only, specify <command>subuser rm</command> and
    the subuser ID.
   </para>
<screen>radosgw-admin subuser rm --uid=example_user:swift</screen>
   <para>
    You can make use of the following options:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--purge-data</option>
     </term>
     <listitem>
      <para>
       Purges all data associated to the user ID.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--purge-keys</term>
     <listitem>
      <para>
       Purges all keys associated to the user ID.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Removing a Subuser</title>
    <para>
     When you remove a sub user, you are removing access to the Swift
     interface. The user will remain in the system. To remove the subuser,
     specify <option>subuser rm</option> and the subuser ID.
    </para>
<screen>radosgw-admin subuser rm --uid=example_user:swift</screen>
    <para>
     You can make use of the following option:
    </para>
    <variablelist>
     <varlistentry>
      <term>--purge-keys</term>
      <listitem>
       <para>
        Purges all keys associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </tip>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.integration">
  <title>Integration</title>

  <para></para>

  <sect2 id="storage.bp.integration.kvm">
   <title>Storing &kvm; Disks in &ceph; Cluster</title>
   <para>
    You can create a disk image for &kvm;-driven virtual machine, store it
    in a &ceph; pool, optionally convert the content of an existing image to
    it, and then run the virtual machine with <command>qemu-kvm</command>
    making use of the disk image stored in the cluster. For more detailed
    information, see <xref linkend="cha.ceph.kvm"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.integration.libvirt">
   <title>Storing &libvirt; Disks in &ceph; Cluster</title>
   <para>
    Similar to &kvm; (see <xref linkend="storage.bp.integration.kvm"/>), you
    can use &ceph; to store virtual machines driven by &libvirt;. The
    advantage is that you can run any &libvirt;-supported virtualization
    solution, such as &kvm;, &xen;, or LXC. For more information, see
    <xref
   linkend="cha.ceph.libvirt"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.integration.xen">
   <title>Storing &xen; Disks in &ceph; Cluster</title>
   <para>
    One way to use &ceph; for storing &xen; disks is to make use of
    &libvirt; as described in <xref linkend="cha.ceph.libvirt"/>.
   </para>
   <para>
    Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
    block device driver directly:
   </para>
   <procedure>
    <step>
     <para>
      If you have no disk image prepared for &xen;, create a new one:
     </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      List images in the pool <literal>mypool</literal> and check if your
      new image is there:
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Create a new block device by mapping the <literal>myimage</literal>
      image to the <systemitem>rbd</systemitem> kernel module:
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify
       a secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
/path/to/keyring</screen>
      <para>
       or
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>rbd showmapped 
id pool   image   snap device    
0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Now you can configure &xen; to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <command>xl</command>-style domain configuration file:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="storage.bp.integration.mount_rbd">
   <title>Mounting and Unmounting an RBD Image</title>
   <para>
    Images stored inside a &ceph; cluster pool can be mapped to a block
    device. You can then format such device, mount it to be able to exchange
    files, and unmount it when done.
   </para>
   <procedure>
    <step>
     <para>
      Make sure your &ceph; cluster includes a pool with the disk image you
      want to mount. Assume the pool is called <literal>mypool</literal> and
      the image is <literal>myimage</literal>.
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Map the image to a new block device.
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify
       a secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
/path/to/keyring</screen>
      <para>
       or
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>rbd showmapped 
id pool   image   snap device    
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      The device we want to work on is <filename>/dev/rbd0</filename>.
     </para>
    </step>
    <step>
     <para>
      Make a filesystem on the <filename>/dev/rbd0</filename> device. We
      recommend using XFS, although Btrfs filesystem is supported as well.
     </para>
<screen>sudo mkfs.xfs /dev/rbd0
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=2097152, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Mount the device and check it is correctly mounted. Replace
      <filename>/mnt</filename> with your mount point.
     </para>
<screen>sudo mount /dev/rbd0 /mnt
mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Now you can move data from/to the device as if it was a local
      directory.
     </para>
     <tip>
      <title>Increasing the Size of RBD Device</title>
      <para>
       If you find that the size of the RBD device is no longer enough, you
       can easily increase it.
      </para>
      <orderedlist>
       <listitem>
        <para>
         Increase the size of the RBD image, for example up to 10GB.
        </para>
<screen>rbd resize --size 10000  mypool/myimage
Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Grow the filesystem to fill up the new size of the device.
        </para>
<screen>sudo xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      After you finish accessing the device, you can unmount it.
     </para>
<screen>sudo unmount /mnt</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.cluster_mntc">
  <title>Cluster Maintenance</title>

  <para></para>

  <sect2 id="storage.bp.cluster_mntc.add_pool">
   <title>Adding a Pool</title>
   <para>
    After you first deploy a cluster, &ceph; uses the default pools to store
    data. You can later create a new pool with
   </para>
<screen>ceph osd pool create</screen>
   <para>
    For more information on cluster pool creation, see
    <xref linkend="ceph.pools.operate.add_pool"/>
   </para>
  </sect2>

  <sect2 id="storage.bp.cluster_mntc.del_pool">
   <title>Deleting a Pool</title>
   <para>
    By deleting a pool, you permanently destroy all data stored in that
    pool. You can delete a previously created pool with
   </para>
<screen>ceph osd pool delete</screen>
   <para>
    For more information on cluster pool deletion, see
    <xref linkend="ceph.pools.operate.del_pool"/>
   </para>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.performance">
  <title>Performance Diagnosis</title>

  <para></para>

  <sect2 id="storage.bp.performance.slowosd">
   <title>Finding Slow OSD</title>
   <para>
    When tuning the cluster performance, it is very important to identify
    slow storage/OSD within the cluster. The reason is that if the data is
    written to the slow(est) disk, the complete write operation slows down
    as it always waits until it is finished on all the related disks.
   </para>
   <para>
    It is not trivial to find out the storage bottleneck. You need to
    examine each and every OSD to find out the ones slowing down the write
    process. To do a benchmark on a single OSD, run:
   </para>
<screen role="ceph_tell_osd_bench"><command>ceph tell <replaceable>osd_id</replaceable> bench</command></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph tell osd.0 bench
{ "bytes_written": 1073741824,
  "blocksize": 4194304,
  "bytes_per_sec": "19377779.000000"}</screen>
   <para>
    Then you have to run this command on each OSD and compare the
    <literal>bytes_per_sec</literal> value to get the slow(est) OSDs.
   </para>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.srv_maint">
  <title>Server Maintenance</title>

  <para></para>

  <sect2 id="storage.bp.srv_maint.add_server">
   <title>Adding a Server to a Cluster</title>
   <tip>
    <para>
     When adding an OSD to an existing cluster, be aware that the cluster
     will be rebalancing for some time afterwards. To minimize the
     rebalancing periods, it is best to add all the OSDs you intend to add
     at the same time.
    </para>
   </tip>
   <para>
    If you are adding an OSD to a cluster, follow
    <xref
linkend="storage.bp.inst.add_osd_cephdeploy"/>.
   </para>
   <para>
    If you are adding a monitor to a cluster, follow
    <xref
linkend="storage.bp.inst.add_rm_monitor"/>.
   </para>
   <important>
    <para>
     After adding a monitor, make sure that
     <filename>/etc/ceph/ceph.conf</filename> files on each server point to
     the new monitor as well so that it works after next reboot.
    </para>
   </important>
   <tip>
    <para>
     Adding an OSD and monitor on the same server is recommended only for
     small size clusters. Although the monitor can share disk with the
     operating system (preferably an SSD disk for performance reasons), it
     should <emphasis>never</emphasis> share disk with an OSD.
    </para>
   </tip>
  </sect2>

  <sect2 id="storage.bp.srv_maint.rm_server">
   <title>Removing a Server from a Cluster</title>
   <para>
    When removing an OSD from an existing cluster, make sure there are
    enough OSDs left in the cluster so that the replication rules can be
    followed. Also be aware that the cluster will be rebalancing for some
    time after removing the OSD.
   </para>
   <para>
    If you are removing an OSD from a cluster, follow
    <xref
linkend="storage.bp.disk.del"/>.
   </para>
   <para>
    If you are removing a monitor from a cluster, follow
    <xref
linkend="storage.bp.inst.add_rm_monitor.rmmon"/>.
   </para>
  </sect2>

  <sect2 id="storage.bp.srv_maint.fds_inc">
   <title>Increasing File Descriptors</title>
   <para>
    For OSD daemons, the read/write operations are critical to keep the
    &ceph; cluster balanced. They often need to have many files open for
    reading and writing at the same time. On the OS level, the maximum
    number of simultaneously open files is called 'maximum number of file
    descriptors'.
   </para>
   <para>
    To prevent OSDs from running out of file descriptors, you can override
    the OS default value and specify the number in
    <filename>/etc/ceph/ceph.conf</filename>, for example:
   </para>
<screen>max_open_files = 131072</screen>
   <para>
    After you change <option>max_open_files</option>, you need to restart
    the OSD service on the relevant &ceph; node.
   </para>
  </sect2>
 </sect1>
 <sect1 id="storage.bp.net">
  <title>Networking</title>

  <para></para>

  <sect2 id="storage.bp.net.firewall">
   <title>Firewall Settings for &ceph;</title>
   <para>
    We recommend protecting the network cluster communication with SUSE
    Firewall. You can edit its configuration by selecting
    <menuchoice><guimenu>&yast;</guimenu><guimenu>Security and
    Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
    Services</guimenu></menuchoice>.
   </para>
   <para>
    For Calamari, enable the "HTTP Server", "Carbon" and "SaltStack"
    services (ports 80, 2003, 2004, 4505 and 4506).
   </para>
   <para>
    For &ceph; monitor nodes, enable the "Ceph MON" service (port 6789).
   </para>
   <para>
    For &ceph; OSD (or MDS) nodes , enable the "Ceph OSD/MDS" service (ports
    6800-7300).
   </para>
  </sect2>

  <sect2 id="storage.bp.net.private">
   <title>Adding a Private Network to a Running Cluster</title>
   <para>
    If you do not specify a cluster network during &ceph; deployment, it
    assumes a single public network environment. While &ceph; operates fine
    with a public network, its performance and security improves when you
    set a second private cluster network.
   </para>
   <para>
    General recommendation for &ceph; cluster is to have two networks: a
    public (front-side) and cluster (back-side) one. To support two
    networks, each &ceph; node needs to have at least two network cards.
   </para>
   <para>
    You need to apply the following changes to each &ceph; node. It is
    comfortable for a small cluster, but can be very time demanding if you
    have a cluster consisting of hundreds or thousands of nodes.
   </para>
   <procedure>
    <step>
     <para>
      Stop &ceph; related services on each cluster node.
     </para>
<screen>sudo rcceph stop</screen>
    </step>
    <step>
     <para>
      On each node, edit <filename>/etc/ceph/ceph.conf</filename>, and add
      the following options in the <literal>[global]</literal> section:
     </para>
<screen>
[global]
...
cluster network = 10.0.0.0/24
cluster addr = <replaceable>node_ip_address</replaceable> </screen>
     <para>
      Replace <literal>10.0.0.0/24</literal> with the IP address and netmask
      of the cluster network. You can specify more comma-delimited subnets.
      If you need to specifically assign static IP addresses or override
      <option>cluster network</option> settings, you can do so with the
      optional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Check that the private cluster network works as expected on the OS
      level.
     </para>
    </step>
    <step>
     <para>
      Start &ceph; related services on each cluster node.
     </para>
<screen>sudo rcceph start</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
