<?xml version="1.0"?>
<!DOCTYPE glossary PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<glossary id="gloss.storage.glossary">
 <title>Glossary</title>
 <glossdiv id="gloss.storage.general">
  <title>General</title>
  <glossentry id="gloss.storage.adminode"><glossterm>Admin node</glossterm>
   <glossdef>
    <para>
     The node from which you run the <command>ceph-deploy</command> utility
     to deploy &ceph; on OSD nodes.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.bucket"><glossterm>Bucket</glossterm>
   <glossdef>
    <para>
     A point which aggregates other nodes into a hierarchy of physical
     locations.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.crush"><glossterm>CRUSH, &crushmap;</glossterm>
   <glossdef>
    <para>
     An algorithm that determines how to store and retrieve data by
     computing data storage locations. CRUSH requires a map of the cluster
     to pseudo-randomly store and retrieve data in OSDs with a uniform
     distribution of data across the cluster.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.mon"><glossterm>Monitor node, MON</glossterm>
   <glossdef>
    <para>
     A cluster node that maintains maps of cluster state, including the
     monitor map, or the OSD map.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.osd"><glossterm>OSD node</glossterm>
   <glossdef>
    <para>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     &ceph; monitors by checking other &ceph; OSD daemons.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.node"><glossterm>Node</glossterm>
   <glossdef>
    <para>
     Any single machine or server in a Ceph System.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Pool</glossterm>
   <glossdef>
    <para>
     Logical partitions for storing objects such as disk images.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Rule Set</glossterm>
   <glossdef>
    <para>
     Rules to determine data placement for a pool.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>
<!-- === CEPH ====================================================== -->
 <glossdiv id="gloss.storage.ceph">
  <title>&ceph; Specific Terms</title>
  <glossentry><glossterm>Calamari</glossterm>
   <glossdef>
    <para>
     A management and monitoring system for &ceph; storage cluster. It
     provides a Web user interface that makes &ceph; cluster monitoring
     simple.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.ceph_storage_cluster"><glossterm>&ceph; Storage Cluster</glossterm>
   <glossdef>
    <para>
     The core set of storage software which stores the userâ€™s data. Such a
     set consists of &ceph; monitors and OSDs.
    </para>
    <para>
     AKA <quote>&ceph; Object Store</quote>.
    </para>
   </glossdef>
  </glossentry>
  <glossentry id="gloss.storage.rgw"><glossterm>&rgw;</glossterm>
   <glossdef>
    <para>
     The S3/Swift gateway component for &ceph; Object Store.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>
</glossary>
