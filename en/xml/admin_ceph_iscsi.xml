<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.iscsi">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>&ceph; iSCSI Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
<!-- 2015-12-03 tbazant, post-processing:
 * Ceph -> &ceph;
 * iscsid -> <systemitem>iscsid</systemitem>
 * dm-multipath -> <systemitem>dm-multipath</systemitem>
 * lrbd -> <systemitem>lrbd</systemitem>
 -->
 <para>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <emphasis>initiators</emphasis>) to send SCSI commands to SCSI storage
  devices (<emphasis>targets</emphasis>) on remote servers. &storage;
  includes a facility that opens Ceph storage management to heterogeneous
  clients, such as Microsoft Windows and VMware vSphere, through the iSCSI
  protocol. Multipath iSCSI access enables availability and scalability for
  these clients, and the standardized iSCSI protocol also provides an
  additional layer of security isolation between clients and the SUSE
  Enterprise Storage 2 cluster. The configuration facility is named lrbd.
  Using lrbd, Ceph storage administrators can define thin-provisioned,
  replicated, highly-available volumes supporting read-only snapshots,
  read-write clones, and automatic resizing with Ceph RADOS Block Device
  (RBD). Administrators can then export volumes either via a single lrbd
  gateway host, or via multiple gateway hosts supporting multipath failover.
  Linux, Windows, and VMware hosts can connect to volumes using the iSCSI
  protocol, which makes them available like any other SCSI block device.
  This means &storage; 2 customers can effectively run a complete
  block-storage infrastructure subsystem on Ceph that provides all features
  and benefits of a conventional SAN enabling future growth.
 </para>
 <para>
  This chapter introduces detailed information how to set up a &ceph;
  cluster infrastructure together with an iSCSI gateway so that the client
  hosts can utilize remotely stored data as local storage devices using the
  iSCSI protocol.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>iSCSI Block Storage</title>

  <para>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720.
   iSCSI is implemented as a service where a client (the initiator) talks to
   a server (the target) via a session on TCP port 3260. An iSCSI target's
   IP address and port are called an iSCSI portal, where a target can be
   exposed through one or more portals. The combination of a target and one
   or more portals is called the target portal group (TPG).
  </para>

  <para>
   The underlying data link layer protocol for iSCSI is commonly Ethernet.
   More specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet
   or faster networks for optimal throughput. 10 Gigabit Ethernet
   connectivity between the iSCSI gateway and the backend Ceph cluster is
   strongly recommended.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>The Linux Kernel iSCSI Target</title>
   <para>
    The Linux kernel iSCSI target was originally named LIO for
    linux-iscsi.org, the project's original domain and website. For some
    time, no fewer than 4 competing iSCSI target implementations were
    available for the Linux platform, but LIO ultimately prevailed as the
    single iSCSI reference target. The mainline kernel code for LIO uses the
    simple, but somewhat ambiguous name "target", distinguishing between
    "target core" and a variety of front-end and back-end target modules.
   </para>
   <para>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol
    is supported by &storage;.
   </para>
   <para>
    The most frequently used target back-end module is one that is capable
    of simply re-exporting any available block device on the target host.
    This module is named iblock. However, LIO also has an RBD-specific
    backend module supporting parallelized multipath I/O access to RBD
    images.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>iSCSI Initiators</title>
   <para>
    This section introduces a brief information on iSCSI initiators used on
    Linux, Microsoft Windows, and VMware platforms.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     The standard initiator for the Linux platform is open-iscsi. open-iscsi
     launches a daemon, iscsid, which the user can then use to discover
     iSCSI targets on any given portal, log in to targets, and map iSCSI
     volumes. iscsid communicates with the SCSI mid layer to create
     in-kernel block devices that the kernel can then treat like any other
     SCSI block device on the system. The open-iscsi initiator can be
     deploying in conjunction with the Device Mapper Multipath
     (dm-multipath) facility to provide a highly available iSCSI block
     device.
    </para>
   </sect3>
   <sect3>
    <title>Microsoft Windows and Hyper-V</title>
    <para>
     The default iSCSI initiator for the Microsoft Windows operating system
     is the Microsoft iSCSI initiator. The iSCSI service can be configured
     via a Graphical User Interface (GUI), and supports multipath I/O for
     high availability.
    </para>
   </sect3>
   <sect3>
    <title>VMware</title>
    <para>
     The default iSCSI initiator for VMware vSphere and ESX is the VMware
     ESX software iSCSI initiator, <systemitem>vmkiscsi</systemitem>. Once
     enabled, it can be configured either from the vSphere client, or using
     the <command>vmkiscsi-tool</command> command. You can then format
     storage volumes connected through the vSphere iSCSI storage adapter
     with VMFS, and use them like any other VM storage device. The VMware
     initiator also supports multipath I/O for high availability.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrdb">
  <title>Gerneral Information about lrdb</title>

  <para>
   lrbd combines the benefits of RADOS Block Devices with the ubiquitous
   versatility of iSCSI. By employing lrbd on an iSCSI target host (known as
   the lrbd gateway), any application that needs to make use of block
   storage can benefit from Ceph, even if it does not speak any Ceph client
   protocol. Instead, users can use iSCSI or any other target frontend
   protocol to connect to an LIO target, which translates all target I/O to
   RBD storage operations.
  </para>

  <figure>
   <title>&ceph; Cluster with a Single iSCSI Gateway</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   lrbd is inherently highly-available and supports multipath operations.
   Thus, downstream initiator hosts can use multiple iSCSI gateways for both
   high availability and scalability. When communicating with an iSCSI
   configuration with more than one gateway, initiators may load-balance
   iSCSI requests across multiple gateways. In the event of a gateway
   failing, being temporarily unreachable, or being disabled for
   maintenance, I/O will transparently continue via another gateway.
  </para>

  <figure>
   <title>&ceph; Cluster with Multiple iSCSI Gateways</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Deployment Considerations</title>

  <para>
   A minimum configuration of &storage; with lrbd consists of the following
   components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A Ceph storage cluster. The Ceph cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI target server running the LIO iSCSI target, configured via
     lrbd.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI initiator host, running open-iscsi (Linux), the Microsoft
     iSCSI Initiator (Microsoft Windows), or any other compatible iSCSI
     initiator implementation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A recommended production configuration of &storage; with lrbd consists
   of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A Ceph storage cluster. A production Ceph cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running
     10-12 object storage daemons (OSDs), with no fewer than three dedicated
     MON hosts.
    </para>
   </listitem>
   <listitem>
    <para>
     Several iSCSI target servers running the LIO iSCSI target, configured
     via lrbd. For iSCSI fail-over and load-balancing, these servers must
     run a kernel supporting the <systemitem>target_core_rbd</systemitem>
     module. Updates packages are available from the &sls; maintenance
     channel.
    </para>
   </listitem>
   <listitem>
    <para>
     Any number of iSCSI initiator hosts, running open-iscsi (Linux), the
     Microsoft iSCSI Initiator (Microsoft Windows), or any other compatible
     iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation and Configuration</title>

  <para>
   This section describes steps to install and configure iSCSI gateway on
   top of &storage;.
  </para>

  <sect2>
   <title>Install &storage; and Deploy a &ceph; Cluster</title>
   <para>
    Before you start installing and configuring an iSCSI gateway, you need
    to install &storage; and deploy a &ceph; cluster as described in
    <xref linkend="cha.ceph.install"/>.
   </para>
  </sect2>

  <sect2>
   <title>Installing the <systemitem>ceph_iscsi</systemitem> Pattern</title>
   <para>
    On your designated iSCSI target server nodes, install the
    <systemitem>ceph_iscsi</systemitem> pattern. Doing so will automatically
    install <systemitem>lrbd</systemitem>, the necessary Ceph binaries and
    libraries, and the <command>targetcli</command> command-line tool:
   </para>
<screen>sudo zypper in -t pattern ceph_iscsi</screen>
   <para>
    Repeat this step on any node that you want to act as a fail-over or
    load-balancing target server node.
   </para>
  </sect2>

  <sect2>
   <title>Create RBD Images</title>
   <para>
    RBD images are created in the Ceph store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this
    purpose. You can create a volume from any host that is able to connect
    to your storage cluster using the Ceph <command>rbd</command>
    command-line utility. This requires the client to have at least a
    minimal ceph.conf configuration file, and appropriate CephX
    authentication credentials.
   </para>
   <para>
    To create a new volume for subsequent export via iSCSI, use the
    <command>rbd create</command> command, specifying the volume size in
    megabytes. For example, in order to create a 100GB volume named
    <literal>testvol</literal> in the pool named <literal>iscsi</literal>,
    run:
   </para>
<screen>rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    The above command creates an RBD "format 1" volume. In order to enable
    layered snapshots and cloning, you may want to create a "format 2"
    volume instead:
   </para>
<screen>rbd --pool iscsi create --image-format 2 --size=102400 testvol</screen>
   <para>
    To always enable format 2 on all newly-created images, add the following
    configuration options to <filename>/etc/ceph/ceph.conf</filename>:
   </para>
<screen>[client]
   rbd default image format = 2</screen>
   <para>
    Once this option is set, every invocation of <command>rbd
    create</command> becomes functionally equivalent to <command>rbd create
    --image-format 2</command>, and always creates a volume in the new image
    format. Note that this is a client-side option that must be set on any
    host you use to create a new volume. It is not sufficient to set this on
    your Ceph servers.
   </para>
  </sect2>

  <sect2>
   <title>Export RBD Images via iSCSI</title>
   <para>
    To export RBD images via iSCSI, use the lrbd utility. lrbd allows you to
    create, review, and modify the iSCSI target configuration, which uses a
    JSON format.
   </para>
   <para>
    In order to edit the configuration, use <command>lrbd -e</command> or
    <command>lrbd --edit</command>. This command will invoke the default
    editor, as defined by the <literal>EDITOR</literal> environment
    variable. You may override this behavior by setting the
    <option>-E</option> option in addition to <option>-e</option>.
   </para>
   <para>
    Below is an example configuration for
   </para>
   <itemizedlist>
    <listitem>
     <para>
      two iSCSI gateway hosts named <literal>iscsi1.example.com</literal>
      and <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      with a single iSCSI Logical Unit (LU),
     </para>
    </listitem>
    <listitem>
     <para>
      backed by an RBD image named <literal>testvol</literal> in the RADOS
      pool <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      and exporting the target via two portals named "east" and "west":
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "authentication": "none"
        }
    ], 
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "hosts": [
                {
                    "host": "iscsi1.example.com", 
                    "portal": "east"
                }, 
                {
                    "host": "iscsi2.example.com", 
                    "portal": "west"
                }
            ]
        }
    ], 
    "portals": [
        {
            "name": "east", 
            "addresses": [
                "192.168.124.104"
            ]
        }, 
        {
            "name": "west", 
            "addresses": [
                "192.168.124.105"
            ]
        }
    ], 
    "pools": [
        {
            "pool": "rbd", 
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
}</screen>
  </sect2>
 </sect1>
</chapter>
