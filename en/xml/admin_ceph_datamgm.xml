<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--
(13:16:08) oms101: bucket types is sucha  coder perspective
(13:16:18) oms101: do you understand what its trying to do ?
(13:19:26) tbazant_lap: buckets are the hierarchy nodes and define the path to
individual OSDs?
(13:19:59) oms101: most importantly -> they define how data is distibuted
(13:20:04) oms101: and allow you to experess this
(13:20:42) oms101: so they allow the admin to define how they want to allow
things to break 
(13:20:59) tbazant_lap: distributed = where they physically are stored?
(13:21:02) oms101: tehy also define where the data is placed that can speed up
reads if one location is local
(13:21:33) oms101: and unfortuantly will slow data werites if distributed across
to large a number of switch interconnects
(13:21:42) oms101:  distributed = where they physically are stored? -> yes
(13:23:00) oms101: so on large clusters you must compromise in the layout
between maximum fault tollerance , and closeness of data for writes
(13:23:11) oms101: can get a lot of conpromises here
(13:23:30) oms101: for reads its much nicer as you want a copy of the data close
(13:23:50) oms101: but for writes you want all data close for a write -. but far
apart for availabilty 
(13:24:05) oms101: but close for speed
(13:24:32) tbazant_lap: you're too fast form me: wait, i need to get the basics,
so...:
(13:24:46) tbazant_lap: * buckets are difined in crush maps, right
(13:24:47) tbazant_lap: ?
(13:25:18) oms101: crushmaps are a product of buckets and rules
(13:25:28) tbazant_lap: ok
(13:25:35) oms101: both are needed
(13:25:40) oms101: both are distinct
(13:26:34) oms101: rules depend on buckets
(13:26:40) tbazant_lap: so i a crush map, i can define a rack bucket, and
several host buckets containing osd buckets
(13:26:40) oms101: so not distinct
(13:26:47) tbazant_lap: a tree structure, right?
(13:26:56) oms101: if your cluster is big enough yes
(13:27:37) tbazant_lap: ok, and if i need the cluster to give me a specific
file, how does this bucket schema help?
(13:28:17) tbazant_lap: is it like a directory structure?
(13:28:22) oms101: its more -> how do I want to cope with a power supply / rack
/ host / disk breaking 
(13:28:51) oms101: so if yoiu want ceph to survive one data center power supply
dieing 
(13:29:24) oms101: you woudl define your buckets to split data across the unit
of power supply
(13:29:35) tbazant_lap: crush -> crash
(13:29:44) tbazant_lap: :-)
(13:30:07) oms101: if you want to cope with a rack dieing (maybe its a DC based
rack and the power supply or cooling fails)
(13:30:14) oms101: then you define things in racks
(13:30:36) oms101: if you want to make replication on host level
(13:30:48) oms101: then you need hosts in your tree\
(13:30:49) tbazant_lap: the between OSDs
(13:30:57) tbazant_lap: then
(13:31:18) tbazant_lap: so.....you define how data will be replicat for the case
of failure?
(13:31:20) oms101: yes you coudl say i AM WILLING TO LOOSE ACCESS TO DATA AND
JUST REPLICATE BETWEEN DISKS ON A SINGLE HOST
(13:31:26) oms101: \ops sorry for caps
(13:31:38) oms101: I think we just stick with that for now
(13:31:45) oms101: just talk about failure
(13:32:06) oms101: (but thiers a performance trade off too -> which ceph coudl
do better at)
(13:32:24) oms101: (but lets not talk about this yet)
(13:32:41) oms101: (maybe we talk about it when upstream has a fix for this
trade off)
(13:34:17) tbazant_lap: sooo......the more host buckets i define in a crush map,
the less i'm endangered of a host failure?
(13:35:13) oms101: depends 
(13:35:28) oms101: your buckets d3efine yoru failure structures
(13:35:56) oms101: your rule sets pick a point on the bucket -> then spread the
data by selecting leaves
(13:37:18) oms101: We CANT set thsi up automatically 
(13:37:28) oms101: its very data center specific
(13:37:28) tbazant_lap: buckets are static, and rules sets make it sort of
dynamic
(13:37:35) oms101: yes
(13:37:58) oms101: buckets are soem thign you specify to model your data center
and ceph in it
(13:38:11) oms101: rule sets define how you use the model
(13:38:34) tbazant_lap: the upstream doc is not good at this, i did not get the
right picture out of it
(13:39:12) oms101: and worse -> this is a MUST tune to my site as an admin
(13:39:34) tbazant_lap: where did you learn? i need some source not to overwhelm
you
(13:39:59) oms101: I worked it out by fiddling with the crushmap and thinking
(13:40:36) oms101: and the ceph training course we had -> but that just got me
started
(13:40:57) oms101: and also I used to work on dcache -> and we had similar stuff
in dcache
(13:41:45) oms101: though the rules where a lot less simple and more
"proceduraal" and a french admin warped my head with what we called "data
hopping"
(13:42:00) tbazant_lap: do you think that crush maps are the right thing now for
me to focus on? i wont have time to cover everything until i'm assigned to
another project (end of March)
(13:42:20) oms101: I think we need a little more on it 
(13:42:29) oms101: and explanation of failure modes
(13:42:33) oms101: and similar
(13:42:54) oms101: but honestly I think some "tasks" are nearly as important
(13:43:03) oms101: some even more important
(13:43:38) oms101: such as ceph aparently fails rearrly badly when the crushmap
hash goes wrong and one OSD fills up cvompletely -> so rebalnancing has to be
done
-->
<chapter id="cha.storage.datamgm">
 <title>Stored Data Management</title>
 <para>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers &ceph; clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, &ceph;
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </para>
 <para>
  CRUSH requires a map of your cluster, and uses the CRUSH map to
  pseudo-randomly store and retrieve data in OSDs with a uniform
  distribution of data across the cluster.
 </para>
 <para>
  CRUSH maps contain a list of OSDs, a list of ‘buckets’ for aggregating
  the devices into physical locations, and a list of rules that tell CRUSH
  how it should replicate data in a &ceph; cluster’s pools. By reﬂecting
  the underlying physical organization of the installation, CRUSH can
  model—and thereby address—potential sources of correlated device
  failures. Typical sources include physical proximity, a shared power
  source, and a shared network. By encoding this information into the
  cluster map, CRUSH placement policies can separate object replicas across
  different failure domains while still maintaining the desired
  distribution. For example, to address the possibility of concurrent
  failures, it may be desirable to ensure that data replicas are on devices
  using different shelves, racks, power supplies, controllers, and/or
  physical locations.
 </para>
 <para>
  When you create a configuration file and deploy &ceph; with
  <command>ceph-deploy</command>, &ceph; generates a default CRUSH map for
  your configuration. The default CRUSH map is fine for your &ceph; sandbox
  environment. However, when you deploy a large-scale data cluster, you
  should give significant consideration to developing a custom CRUSH map,
  because it will help you manage your &ceph; cluster, improve performance
  and ensure data safety.
 </para>
 <para>
  For example, if an OSD goes down, a CRUSH map can help you can locate the
  physical data center, room, row and rack of the host with the failed OSD
  in the event you need to use on-site support or replace hardware.
 </para>
 <para>
  Similarly, CRUSH may help you identify faults more quickly. For example,
  if all OSDs in a particular rack go down simultaneously, the fault may lie
  with a network switch or power to the rack or the network switch rather
  than the OSDs themselves.
 </para>
 <para>
  A custom CRUSH map can also help you identify the physical locations where
  &ceph; stores redundant copies of data when the placement group(s)
  associated with a failed host are in a degraded state.
 </para>
 <para>
  There are three main sections to a &crushmap;.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="datamgm.devices" xrefstyle="select: title"/> consist of any object storage
    device–i.e., the hard disk corresponding to a
    <systemitem>ceph-osd</systemitem> daemon.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.buckets" xrefstyle="select: title"/> consist of a hierarchical aggregation
    of storage locations (for example rows, racks, hosts, etc.) and their assigned
    weights.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.rules" xrefstyle="select: title"/> consist of the manner of selecting
    buckets.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 id="datamgm.devices">
  <title>Devices</title>

  <para>
   To map placement groups to OSDs, a &crushmap; requires a list of OSD
   devices (the name of the OSD daemon). The list of devices appears
   first in the &crushmap;.
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   For example:
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   As a general rule, an OSD daemon maps to a single disk or to a RAID.
  </para>
 </sect1>
 <sect1 id="datamgm.buckets">
  <title>Buckets</title>

  <para>
   CRUSH maps support the notion of ‘buckets’, which may be thought of
   as nodes that aggregate other buckets into a hierarchy of physical
   locations, where OSD devices are the leaves of the hierarchy. The
   following table lists the default types.
  </para>

  <informaltable frame='none'>
   <tgroup cols='3'>
    <colspec colwidth='0.5in'/>
    <colspec colwidth='1in'/>
    <colspec colwidth='5in'/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        An OSD daemon (osd.1, osd.2, etc).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        Host
       </para>
      </entry>
      <entry>
       <para>
        A host name containing one or more OSDs.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        Rack
       </para>
      </entry>
      <entry>
       <para>
        A computer rack. The default is <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        Row
       </para>
      </entry>
      <entry>
       <para>
        A row in a series of racks.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        Room
       </para>
      </entry>
      <entry>
       <para>
        A room containing racks and rows of hosts.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Data Center
       </para>
      </entry>
      <entry>
       <para>
        A physical data center containing rooms.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pool
       </para>
      </entry>
      <entry>
       <para>
        A data storage pool for storing objects.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    You can remove these types and create your own bucket types.
   </para>
  </tip>

  <para>
   &ceph;’s deployment tools generate a CRUSH map that contains a bucket
   for each host, and a pool named 'default', which is useful for the
   default <literal>data</literal>, <literal>metadata</literal> and
   <literal>rbd</literal> pools. The remaining bucket types provide a means
   for storing information about the physical location of nodes/buckets,
   which makes cluster administration much easier when OSDs, hosts, or
   network hardware malfunction and the administrator needs access to
   physical hardware.
  </para>

  <para>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of
   its item(s), the bucket algorithm ( <literal>straw</literal> by default),
   and the hash (<literal>0</literal> by default, reflecting CRUSH Hash
   <literal>rjenkins1</literal>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a datacenter, a room, a rack and a row.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 id="datamgm.rules">
  <title>Rules Sets</title>

  <para>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and
   rules. The default CRUSH map has a rule for each pool, and one ruleset
   assigned to each of the default pools, which include:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     data
    </para>
   </listitem>
   <listitem>
    <para>
     metadata
    </para>
   </listitem>
   <listitem>
    <para>
     rbd
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </para>
  </note>

  <para>
   A rule takes the following form:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      An integer. Classifies a rule as belonging to a set of rules.
      Activated by setting the ruleset in a pool. This option is required.
      Default is <literal>0</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      A string. Describes a rule for either a hard disk (replicated) or a
      RAID. This option is required. Default is
      <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      An integer. If a placement group makes fewer replicas than this
      number, CRUSH will NOT select this rule. This option is required.
      Default is <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      An integer. If a placement group makes more replicas than this number,
      CRUSH will NOT select this rule. This option is required. Default is
      <literal>10</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      Takes a bucket name, and begins iterating down the tree. This option
      is required.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step choose firstn <replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      Selects the number of buckets of the given type. Where N is the number
      of options available, if <replaceable>num</replaceable> &gt; 0
      &amp;&amp; &lt; N, choose that many buckets; if
      <replaceable>num</replaceable> &lt; 0, it means N -
      <replaceable>num</replaceable>; and, if <replaceable>num</replaceable>
      == 0, choose N buckets (all available). Follows <literal>step
      take</literal> or <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to from different trees in the
      same rule. Follows <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    To activate one or more rules with a common ruleset number to a pool,
    set the ruleset number to the pool.
   </para>
  </important>
 </sect1>
 <sect1 id="op.crush">
  <title>&crushmap; Manipulation</title>

  <para>
   This section introduces ways to basic &crushmap; manipulation, such as
   editing a &crushmap;, changing &crushmap; parameters, and
   adding/moving/removing an OSD.
  </para>

  <sect2>
   <title>Editing a &crushmap;</title>
   <para>
    To edit an existing CRUSH map, do the following:
   </para>
   <procedure>
    <step>
     <para>
      <emphasis>Get a &crushmap;.</emphasis> To get the &crushmap; for your
      cluster, execute the following:
     </para>
<screen>ceph osd getcrushmap -o
<replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will output (<option>-o</option>) a compiled &crushmap; to the
      file name you specified. Since the &crushmap; is in a compiled form,
      you must decompile it first before you can edit it.
     </para>
    </step>
    <step>
     <para>
      <emphasis>Decompile a &crushmap;.</emphasis> To decompile a
      &crushmap;, execute the following:
     </para>
<screen>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will decompile (<option>-d</option>) the compiled CRUSH map and
      output (<option>-o</option>) it to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      Edit at least one of Devices, Buckets and Rules parameters.
     </para>
    </step>
    <step>
     <para>
      <emphasis>Compile a &crushmap;.</emphasis> To compile a &crushmap;,
      execute the following:
     </para>
<screen>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      &ceph; will store a compiled CRUSH map to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      <emphasis>Set a &crushmap;.</emphasis> To set the &crushmap; for your
      cluster, execute the following:
     </para>
<screen>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will input the compiled &crushmap; of the file name you
      specified as the &crushmap; for the cluster.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="op.crush.addosd">
   <title>Add/Move an OSD</title>
   <para>
    To add or move an OSD in the CRUSH map of a running cluster, execute the
    following:
   </para>
<screen>ceph osd crush set <replaceable>id</replaceable> <replaceable>name</replaceable> <replaceable>weight</replaceable> pool=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       An integer. The numeric ID of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       A double. The CRUSH weight for the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Key/value pairs. You may specify the OSD’s location in the CRUSH
       hierarchy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following example adds <literal>osd.0</literal> to the hierarchy, or
    moves the OSD from a previous location.
   </para>
<screen>ceph osd crush set 0 osd.0 1.0 pool=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 id="op.crush.osdweight">
   <title>Adjust an OSD’s CRUSH Weight</title>
   <para>
    To adjust an OSD’s crush weight in the CRUSH map of a running cluster,
    execute the following:
   </para>
<screen>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       A double. The CRUSH weight for the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="op.crush.osdremove">
   <title>Remove an OSD</title>
   <para>
    To remove an OSD from the CRUSH map of a running cluster, execute the
    following:
   </para>
<screen>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="op.crush.movebucket">
   <title>Move a Bucket</title>
   <para>
    To move a bucket to a different location or position in the CRUSH map
    hierarchy, execute the following:
   </para>
<screen>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       A string. The name of the bucket to move/reposition. This option is
       required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Key/value pairs. You may specify the bucket’s location in the CRUSH
       hierarchy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
