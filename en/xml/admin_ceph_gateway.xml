<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ceph.gw">
 <title>&ceph; Object Gateway</title>
<!-- 2014-12-12 tbazant: reusing http://ceph.com/docs/master/radosgw/ -->
<!-- 2015-01-22 ke     : now it is mostly based on
     https://wiki.innerweb.novell.com/index.php/SUSE/Storage/rados-gw
-->
 <para>
  &ceph; Object Gateway is an object storage interface built on top of
  <systemitem>librgw</systemitem> to provide applications with a RESTful
  gateway to &ceph; Storage Clusters. &ceph; Object Storage supports two
  interfaces:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the Amazon S3 RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  &ceph; Object Storage uses the &ceph; Object Gateway daemon
  (<systemitem>radosgw</systemitem>), which is a FastCGI module for
  interacting with a &ceph; Storage Cluster. Since it provides interfaces
  compatible with OpenStack Swift and Amazon S3, the &ceph; Object Gateway
  has its own user management. &ceph; Object Gateway can store data in the
  same &ceph; Storage Cluster used to store data from &ceph; File System
  clients or &ceph; Block Device clients. The S3 and Swift APIs share a
  common name space, so you may write data with one API and retrieve it with
  the other.
 </para>
 <sect1 id="rgw.civet">
  <title>Manual Installation and Service Activation</title>

  <para>
   The &ceph; Object Gateway daemon runs on an embedded Web server Civetweb.
   An installed and configured &ceph; cluster is a prerequisite.
  </para>

  <procedure>
   <step>
    <para>
     Install &rgw;. The following command installs all required components:
    </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
   </step>
   <step>
    <para>
     Disable automatic start of the Apache server:
    </para>
<screen>sudo systemctl disable apache2.service</screen>
   </step>
   <step>
    <para>
     If the Apache server from the previous &rgw; instance is running, stop
     it:
    </para>
<screen>sudo systemctl stop apache2.service</screen>
   </step>
   <step>
    <para>
     Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
     lines:
    </para>
<screen>[client.radosgw.gateway]
rgw frontends = "civetweb port=80"</screen>
   </step>
   <step>
    <para>
     Restart &rgw;:
    </para>
    <screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway_name</replaceable>.service</screen>
   </step>
  </procedure>
 </sect1>
<!-- ============================================================== -->
 <sect1 id="ses.rgw.config">
  <title>Configuring the &rgw;</title>

<!-- Simple Configuration -->

  <para>
   Several steps are required to configure a &rgw;.
  </para>

  <sect2>
   <title>Basic Configuration</title>
   <para>
    Configuring a &ceph; Object Gateway requires a running &ceph; Storage
    Cluster. The &ceph; Object Gateway is a client of the &ceph; Storage
    Cluster. As a &ceph; Storage Cluster client, it requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A host name for the gateway instance, for example
      <systemitem>gateway</systemitem>.
     </para>
    </listitem>
    <listitem>
     <para>
      A storage cluster user name with appropriate permissions in a keyring.
     </para>
    </listitem>
    <listitem>
     <para>
      Pools to store its data.
     </para>
    </listitem>
    <listitem>
     <para>
      A data directory for the gateway instance.
     </para>
    </listitem>
    <listitem>
     <para>
      An instance entry in the &ceph; Configuration file.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Each instance must have a user name and key to communicate with a &ceph;
    Storage Cluster. In the following steps, we use an admin node to create
    a keyring. Then, we create a client user name and key. Next, we add the
    key to the &ceph; Storage Cluster. Finally, we distribute the keyring to
    the node containing the gateway instance.
   </para>
   <procedure>
    <step>
     <para>
      Create a keyring for the gateway:
     </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</screen>
    </step>
    <step>
     <para>
      Generate a &ceph; Object Gateway user name and key for each instance.
      As an example, we will use the name <systemitem>gateway</systemitem>
      after <systemitem>client.radosgw</systemitem>:
     </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring \
  -n client.radosgw.gateway --gen-key</screen>
    </step>
    <step>
     <para>
      Add capabilities to the key:
     </para>
<screen>sudo ceph-authtool -n client.radosgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring</screen>
    </step>
    <step>
     <para>
      Once you have created a keyring and key to enable the &ceph; Object
      Gateway with access to the &ceph; Storage Cluster, add the key to your
      &ceph; Storage Cluster. For example:
     </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.gateway \
  -i /etc/ceph/ceph.client.radosgw.keyring</screen>
    </step>
    <step>
     <para>
      Distribute the keyring to the node with the gateway instance:
     </para>
<screen>sudo scp /etc/ceph/ceph.client.radosgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring</screen>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Create Pools</title>
   <para>
    &ceph; Object Gateways require &ceph; Storage Cluster pools to store
    specific gateway data. If the user you created has proper permissions,
    the gateway will create the pools automatically. However, ensure that
    you have set an appropriate default number of placement groups per pool
    in the &ceph; configuration file.
   </para>
   <para>
    When configuring a gateway with the default region and zone, the naming
    convention for pools typically omits region and zone naming, but you can
    use any naming convention you prefer. For example:
   </para>
<screen>.rgw
.rgw.root
.rgw.control
.rgw.gc
.rgw.buckets
.rgw.buckets.index
.log
.intent-log
.usage
.users
.users.email
.users.swift
.users.uid</screen>
  </sect2>

  <sect2>
   <title>Adding Gateway Configuration to &ceph;</title>
   <para>
    Add the &ceph; Object Gateway configuration to the &ceph; Configuration
    file. The &ceph; Object Gateway configuration requires you to identify
    the &ceph; Object Gateway instance. Then, specify the host name where
    you installed the &ceph; Object Gateway daemon, a keyring (for use with
    cephx), the socket path for FastCGI and a log file. For example:
   </para>
<screen>[client.radosgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.<replaceable>instance-name</replaceable>.fastcgi.sock
log file = /var/log/radosgw/client.radosgw.<replaceable>instance-name</replaceable>.log</screen>
   <para>
    The <literal>[client.radosgw.*]</literal> portion of the gateway
    instance identifies this portion of the &ceph; configuration file as
    configuring a &ceph; Storage Cluster client where the client type is a
    &ceph; Object Gateway (radosgw). The instance name follows. For example:
   </para>
<screen>[client.radosgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.gateway.fastcgi.sock
log file = /var/log/radosgw/client.radosgw.gateway.log</screen>
   <note>
    <para>
     The host must be your machine host name, not the FQDN. The name you use
     for the FastCGI socket is not the same as the one used for the object
     gateway, which is
     <literal>ceph-client.radosgw.<replaceable>instance-name</replaceable>.asok</literal>
     by default. Use the same name in your S3 FastCGI file, too.
    </para>
   </note>
   <para>
    Then turn off <literal>print continue</literal>. If you have it set to
    true, you may encounter problems with PUT operations:
   </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
   <para>
    To use a &ceph; Object Gateway with subdomain S3 calls (for example
    <literal>http://bucketname.hostname</literal>), you must add the &ceph;
    Object Gateway DNS name under the
    <literal>[client.radosgw.gateway]</literal> section of the &ceph;
    configuration file:
   </para>
<screen>[client.radosgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
   <para>
    You should also consider installing a DNS server such as Dnsmasq on your
    client machine(s) when using the
    <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
    syntax. The <filename>dnsmasq.conf</filename> file should include the
    following settings:
   </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
   <para>
    Then, add the <replaceable>client-loopback-ip</replaceable> IP address
    as the first DNS server on the client machine(s).
   </para>
  </sect2>

  <sect2>
   <title>Redeploy &ceph; Configuration</title>
   <para>
    Use <command>ceph-deploy</command> to push a new copy of the
    configuration to the hosts in your cluster:
   </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
  </sect2>

  <sect2>
   <title>Add a &ceph; Object Gateway Script</title>
   <para>
    Add a <filename>s3gw.fcgi</filename> file (use the same name referenced
    in the first line of <filename>rgw.conf</filename>). Save the file to
    the <filename>/srv/www/fastcgi</filename> directory. Assuming a cluster
    named <systemitem>ceph</systemitem> (default), and the user created in
    previous steps, the contents of the file should include:
   </para>
<screen>#!/bin/sh
exec /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway</screen>
   <para>
    Apply execute permissions to <filename>s3gw.fcgi</filename>:
   </para>
<screen>sudo chmod +x s3gw.fcgi</screen>
  </sect2>

  <sect2>
   <title>Create Data Directory</title>
   <para>
    Deployment scripts may not create the default &ceph; Object Gateway data
    directory. Create data directories for each instance of a radosgw daemon
    if not already done. The <literal>host</literal> variables in the &ceph;
    configuration file determine which host runs each instance of a radosgw
    daemon. The typical form specifies the radosgw daemon, the cluster name
    and the daemon ID.
   </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
   <para>
    Using the exemplary ceph.conf settings above, you would execute the
    following:
   </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
  </sect2>

  <sect2>
   <title>Restart Services and Start the Gateway</title>
   <para>
    To ensure that all components have reloaded their configurations, we
    recommend restarting your &ceph; Storage Cluster service. Then, start up
    the <systemitem>radosgw</systemitem> service.
   </para>
   <para>
    For the &ceph; Storage Cluster, see
    <xref linkend="cha.ceph.operating"/>.
   </para>
   <screen>sudo systemctl restart ceph-radosgw@<replaceable>gateway</replaceable>.service</screen>
   <para>
    After the service is up and running, you can make an anonymous GET
    request to see if the gateway returns a response. A simple HTTP request
    to the domain name should return the following:
   </para>
<screen><![CDATA[<ListAllMyBucketsResult>
      <Owner>
              <ID>anonymous</ID>
              <DisplayName/>
      </Owner>
      <Buckets/>
</ListAllMyBucketsResult>]]></screen>
  </sect2>
 </sect1>
 <sect1>
  <title>Managing &rgw; with <command>ceph-deploy</command></title>

  <para>
   The <command>ceph-deploy</command> script includes the
   <literal>rwg</literal> component that helps you manage the &rgw;
   activation and operation.
  </para>

  <para>
   Before running <command>ceph-deploy rgw</command>, you need to have the
   &ceph; cluster installed (see <xref linkend="cha.ceph.install"/> for more
   information).
  </para>

  <procedure>
   <step>
    <para>
     Prepare node(s). You can specify several pairs of
     <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
     to install &rgw; on a required number of nodes.
    </para>
<screen>ceph-deploy --overwrite-conf rgw prepare \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
    <para>
     For example:
    </para>
<screen>ceph-deploy --overwrite-conf rgw prepare ceph-node1:gateway1
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw prepare ceph-node1:gateway1
[ceph-node1][INFO  ] Running command: sudo ceph -f json auth list
[ceph-node1][INFO  ] Running command: sudo ceph -f json osd lspools
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.email 3
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .intent-log 7
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .usage 8
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.uid 9
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw.control 10
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users 11
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .log 12
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw.gc 13
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.swift 14
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw 15
[ceph-node1][INFO  ] Running command: sudo zypper --non-interactive \
  --gpg-auto-import-keys refresh
[ceph-node1][INFO  ] Running command: sudo zypper --non-interactive --quiet \
  install ceph-radosgw
[...]
[ceph_deploy.rgw][INFO  ] Writing:/srv/www/radosgw/s3gw_80.fcgi</screen>
   </step>
   <step>
    <para>
     Activate node(s). Specify the same pairs of
     <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
     you did when preparing the nodes.
    </para>
<screen>ceph-deploy --overwrite-conf rgw activate \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
    <para>
     For example:
    </para>
<screen>ceph-deploy --overwrite-conf rgw activate ceph-node1:gateway1
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw activate ceph-node1:gateway1
[ceph-node1][INFO  ] Running command: sudo systemctl start ceph-radosgw@gateway1
[ceph-node1][INFO  ] Running command: sudo systemctl status ceph-radosgw@gateway1 \
  --output json
[ceph-node1][INFO  ] Running command: sudo systemctl enable ceph-radosgw@gateway1</screen>
    <tip>
     <para>
      To combine the <emphasis>prepare</emphasis> and
      <emphasis>activate</emphasis> steps into one, you can use the
      following command:
     </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
    </tip>
   </step>
  </procedure>

  <para>
   You now have a working &rgw; on the specified nodes. Now you need to give
   access to a client.
  </para>

  <procedure id="swift_user.create">
   <title>Adding a New &swift; User</title>
   <step>
    <para>
     Create a new user.
    </para>
<screen>sudo radosgw-admin user create \
  --uid=example_user \
  --display-name="Example User" \
  --email=penguin@example.com</screen>
   </step>
   <step>
    <para>
     Generate a secret key for the user.
    </para>
<screen>sudo radosgw-admin key create \
  --gen-secret \
  --subuser=example_user:swift \
  --key-type=swift</screen>
   </step>
   <step>
    <para>
     Both commands will output JSON-formatted data showing the user state.
     Notice the following lines, and remember the
     <literal>secret_key</literal> value:
    </para>
<screen>"swift_keys": [
  { "user": "example_user:swift",
    "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
   </step>
   <step>
    <para>
     On a client node (any host, typically outside the &ceph; cluster),
     install the swift client that understands the &rgw; API.
    </para>
<screen>sudo zypper install python-swiftclient</screen>
   </step>
   <step>
    <para>
     After the swift client is installed, you can run
    </para>
<screen>swift -v -A http://<replaceable>radosgw_host</replaceable>/auth/v1.0 \
  -U <replaceable>username</replaceable>:swift \
  --key='<replaceable>secret_key</replaceable>' \
  stat</screen>
    <para>
     For example:
    </para>
<screen>swift -v -A http://ceph_node1.example.com/auth/v1.0 \
  -U example_user:swift \
  --key='r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' \
  stat</screen>
   </step>
  </procedure>

  <sect2>
   <title>Removing &rgw; from a Node</title>
   <para>
    To remove a &rgw; installation from the node where it was previously
    installed, run:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete  \
  <replaceable>short_hostname</replaceable>:<replaceable>gatewayname</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete ceph-node1:gateway1
[ceph_deploy.cli][INFO] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw delete ceph-node1:gateway1
[...]
[ceph-node1][INFO] Running command: sudo systemctl stop ceph-radosgw@gateway1
[ceph-node1][INFO] Running command: sudo systemctl disable ceph-radosgw@gateway1
[ceph-node1][INFO] Running command: sudo ceph -f json auth list
[ceph-node1][INFO] Running command: sudo ceph auth del client.radosgw.gateway1</screen>
   <tip>
    <para>
     You need a copy of the local <command>ceph.conf</command> file, in your
     current working directory. If you do not have a copy of it, copy it
     from your cluster.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Listing &rgw; Installations</title>
   <para>
    To list &rgw; installations with the &ceph; cluster, run:
   </para>
<screen>ceph-deploy rgw list</screen>
  </sect2>
 </sect1>
</chapter>
