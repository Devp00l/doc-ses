<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.gw">
<!-- ============================================================== -->
 <title>&ceph; Object Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; Object Gateway is an object storage interface built on top of
  <systemitem>librgw</systemitem> to provide applications with a RESTful
  gateway to &ceph; Storage Clusters. &ceph; Object Storage supports two
  interfaces:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the Amazon S3 RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  &ceph; Object Storage uses the &ceph; Object Gateway daemon
  (<systemitem>radosgw</systemitem>), which uses an embedded HTTP server
  (CivetWeb) for interacting with a &ceph; Storage Cluster. Since it
  provides interfaces compatible with OpenStack Swift and Amazon S3, the
  &ceph; Object Gateway has its own user management. &ceph; Object Gateway
  can store data in the same &ceph; Storage Cluster used to store data from
  &ceph; File System clients or &ceph; Block Device clients. The S3 and
  Swift APIs share a common name space, so you may write data with one API
  and retrieve it with the other.
 </para>
 <para>
  This section helps you install and manage the &ceph; Object Gateway
  (&rgw;). You can either choose to utilize the
  <command>ceph-deploy</command> tool, or do the installation and management
  manually.
 </para>
 <important>
  <para>
   Before installing &rgw;, you need to have the &ceph; cluster installed
   first (see <xref linkend="cha.ceph.install"/> for more information).
  </para>
 </important>
 <sect1 xml:id="ceph.rgw.cephdeploy">
  <title>Managing &rgw; with <command>ceph-deploy</command></title>

  <para>
   This section describes how to install and configure &rgw; with
   <command>ceph-deploy</command>.
  </para>

  <sect2 xml:id="ceph.rgw.cephdeploy.install">
   <title>Installation</title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <literal>rwg</literal> component that helps you manage the &rgw;
    activation and operation.
   </para>
   <procedure>
    <step>
     <para>
      Prepare the node(s). You can specify several pairs of
      <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
      to install &rgw; on a required number of nodes.
     </para>
<screen>ceph-deploy --overwrite-conf rgw prepare \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
     <para>
      For example:
     </para>
<screen>ceph-deploy --overwrite-conf rgw prepare ceph-node1:gateway1
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw prepare ceph-node1:gateway1
[ceph-node1][INFO  ] Running command: sudo ceph -f json auth list
[ceph-node1][INFO  ] Running command: sudo ceph -f json osd lspools
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.email 3
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .intent-log 7
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .usage 8
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.uid 9
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw.control 10
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users 11
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .log 12
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw.gc 13
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .users.swift 14
[ceph-node1][INFO  ] Running command: sudo ceph osd pool create .rgw 15
[ceph-node1][INFO  ] Running command: sudo zypper --non-interactive \
  --gpg-auto-import-keys refresh
[ceph-node1][INFO  ] Running command: sudo zypper --non-interactive --quiet \
  install ceph-radosgw
[...]</screen>
    </step>
    <step>
     <para>
      Activate node(s). Specify the same pairs of
      <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
      you did when preparing the nodes.
     </para>
<screen>ceph-deploy --overwrite-conf rgw activate \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
     <para>
      For example:
     </para>
<screen>ceph-deploy --overwrite-conf rgw activate ceph-node1:gateway1
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw activate ceph-node1:gateway1
[ceph-node1][INFO  ] Running command: sudo systemctl start ceph-radosgw@gateway1
[ceph-node1][INFO  ] Running command: sudo systemctl status ceph-radosgw@gateway1 \
  --output json
[ceph-node1][INFO  ] Running command: sudo systemctl enable ceph-radosgw@gateway1</screen>
     <tip>
      <para>
       To combine the <emphasis>prepare</emphasis> and
       <emphasis>activate</emphasis> steps into one, you can use the
       following command:
      </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
     </tip>
    </step>
   </procedure>
   <para>
    You now have a working &rgw; on the specified nodes, and you need to
    give access to a client. For more information, see
    <xref linkend="ceph.rgw.access"/>.
   </para>
  </sect2>

  <sect2>
   <title>Listing &rgw; Installations</title>
   <para>
    To list all &rgw; instances within the &ceph; cluster, run:
   </para>
<screen>ceph-deploy rgw list</screen>
  </sect2>

  <sect2>
   <title>Removing &rgw; from a Node</title>
   <para>
    To remove a &rgw; installation from the node where it was previously
    installed, run:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete  \
  <replaceable>short_hostname</replaceable>:<replaceable>gatewayname</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete ceph-node1:gateway1
[ceph_deploy.cli][INFO] Invoked (1.5.19): /usr/bin/ceph-deploy \
  --overwrite-conf rgw delete ceph-node1:gateway1
[...]
[ceph-node1][INFO] Running command: sudo systemctl stop ceph-radosgw@gateway1
[ceph-node1][INFO] Running command: sudo systemctl disable ceph-radosgw@gateway1
[ceph-node1][INFO] Running command: sudo ceph -f json auth list
[ceph-node1][INFO] Running command: sudo ceph auth del client.radosgw.gateway1</screen>
   <tip>
    <para>
     You need a copy of the local <command>ceph.conf</command> file, in your
     current working directory. If you do not have a copy of it, copy it
     from your cluster.
    </para>
   </tip>
  </sect2>

<!-- ============================================================== -->
 </sect1>
 <sect1 xml:id="ceph.rgw.manual">
  <title>Managing &rgw; Manually</title>

  <para>
   This section describes how to install and configure &rgw; with manually.
  </para>

  <sect2>
   <title>Installation</title>
   <procedure>
    <step>
     <para>
      Install &rgw;. The following command installs all required components:
     </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
    </step>
    <step>
     <para>
      Disable automatic start of the Apache server:
     </para>
<screen>sudo systemctl disable apache2.service</screen>
    </step>
    <step>
     <para>
      If the Apache server from the previous &rgw; instance is running, stop
      it and disable the relevant service:
     </para>
<screen>sudo systemctl stop disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.radosgw.gateway]
 rgw frontends = "civetweb port=80"</screen>
     <tip>
      <para>
       If you want to configure &rgw;/Civetweb for use with SSL encryption,
       modify the line accordingly:
      </para>
<screen>rgw frontends = civetweb port=7480 ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Restart the &rgw; service. See <xref linkend="ceph.rgw.operating"/>
      for more information.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ses.rgw.config">
   <title>Configuring &rgw;</title>
   <para>
    Several steps are required to configure a &rgw;.
   </para>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a &ceph; Object Gateway requires a running &ceph; Storage
     Cluster. The &ceph; Object Gateway is a client of the &ceph; Storage
     Cluster. As a &ceph; Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions in a
       keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the &ceph; Configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a
     &ceph; Storage Cluster. In the following steps, we use an admin node to
     create a keyring. Then, we create a client user name and key. Next, we
     add the key to the &ceph; Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a &ceph; Object Gateway user name and key for each instance.
       As an example, we will use the name <systemitem>gateway</systemitem>
       after <systemitem>client.radosgw</systemitem>:
      </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring \
  -n client.radosgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>sudo ceph-authtool -n client.radosgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the &ceph; Object
       Gateway with access to the &ceph; Storage Cluster, add the key to
       your &ceph; Storage Cluster. For example:
      </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.gateway \
  -i /etc/ceph/ceph.client.radosgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>sudo scp /etc/ceph/ceph.client.radosgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Create Pools</title>
    <para>
     &ceph; Object Gateways require &ceph; Storage Cluster pools to store
     specific gateway data. If the user you created has proper permissions,
     the gateway will create the pools automatically. However, ensure that
     you have set an appropriate default number of placement groups per pool
     in the &ceph; configuration file.
    </para>
    <para>
     When configuring a gateway with the default region and zone, the naming
     convention for pools typically omits region and zone naming, but you
     can use any naming convention you prefer. For example:
    </para>
<screen>.rgw
.rgw.root
.rgw.control
.rgw.gc
.rgw.buckets
.rgw.buckets.index
.log
.intent-log
.usage
.users
.users.email
.users.swift
.users.uid</screen>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to &ceph;</title>
    <para>
     Add the &ceph; Object Gateway configuration to the &ceph; Configuration
     file. The &ceph; Object Gateway configuration requires you to identify
     the &ceph; Object Gateway instance. Then, specify the host name where
     you installed the &ceph; Object Gateway daemon, a keyring (for use with
     cephx), and a log file. For example:
    </para>
<screen>[client.radosgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.radosgw.keyring
log file = /var/log/radosgw/client.radosgw.<replaceable>instance-name</replaceable>.log</screen>
    <para>
     The <literal>[client.radosgw.*]</literal> portion of the gateway
     instance identifies this portion of the &ceph; configuration file as
     configuring a &ceph; Storage Cluster client where the client type is a
     &ceph; Object Gateway (radosgw). The instance name follows. For
     example:
    </para>
<screen>[client.radosgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.radosgw.keyring
log file = /var/log/radosgw/client.radosgw.gateway.log</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
    <para>
     To use a &ceph; Object Gateway with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the &ceph;
     Object Gateway DNS name under the
     <literal>[client.radosgw.gateway]</literal> section of the &ceph;
     configuration file:
    </para>
<screen>[client.radosgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on
     your client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address
     as the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Redeploy &ceph; Configuration</title>
    <para>
     Use <command>ceph-deploy</command> to push a new copy of the
     configuration to the hosts in your cluster:
    </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default &ceph; Object Gateway
     data directory. Create data directories for each instance of a radosgw
     daemon if not already done. The <literal>host</literal> variables in
     the &ceph; configuration file determine which host runs each instance
     of a radosgw daemon. The typical form specifies the radosgw daemon, the
     cluster name and the daemon ID.
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the exemplary ceph.conf settings above, you would execute the
     following:
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your &ceph; Storage Cluster service. Then, start
     up the <systemitem>radosgw</systemitem> service. For more information,
     see <xref linkend="cha.ceph.operating"/> and
     <xref linkend="ceph.rgw.operating"/>
    </para>
    <para>
     After the service is up and running, you can make an anonymous GET
     request to see if the gateway returns a response. A simple HTTP request
     to the domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Operating the &rgw; Service</title>

  <para>
   &rgw; service is operated with the <command>systemctl</command> command.
   You need to have &rootuser; privileges to operate the &rgw; service. Note
   that <replaceable>gateway_host</replaceable> is the host name of the
   server which &rgw; instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the &rgw; service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Managing &rgw; Access</title>

  <para>
   You can communicate with &rgw; using either S3- or &swift;-compatible
   interface. Both interfaces require you to create a specific user, and
   install the relevant client software to communicate with the gateway
   using the user's secret key.
  </para>

  <para>
   For an introduction and a few practical examples on &rgw; access, see
   <xref linkend="storage.bp.inst.rgw_client"/>.
  </para>

  <sect2>
   <title>Managing S3 Access</title>
   <para>
    S3 interface is compatible with a large subset of the Amazon
    S3 RESTful API.
   </para>
   <tip>
    <para>
     S3cmd is a command line S3 client. You can find it in the
     <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both &sle;
     and &opensuse; based distributions.
    </para>
   </tip>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3add"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3rm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing User Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Setting Quotas</title>
    <para>
     See <xref linkend="storage.bp.account.s3quota"/>.
    </para>
   </sect3>
  </sect2>

  <sect2>
   <title>Managing &swift; Access</title>
   <para>
    &swift; interface is compatible with a large subset of the
    &ostack; &swift; API. 
   </para>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftadd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftrm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
