<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.storage.about">
 <title>About &productname;</title>
 <sect1 id="storage.intro">
  <title>Introduction</title>
  <para>
   &ceph; is a distributed storage designed for scalability, reliability
   and performance. As opposed to conventional systems which have
   allocation tables to store and fetch data, ceph uses a pseudo-random
   data distribution function to store data, which reduces the number
   of look-ups required in storage. Data is stored on intelligent
   object storage devices (OSDs), which automates data management tasks
   such as data distribution, data replication, failure detection
   and recovery. &ceph; is both self-healing and self-managing which
   results in reduction of administrative and budget overhead.
  </para>

    <para>
     The ceph system has the following features:
    </para>
    <variablelist>
     <varlistentry>
      <term>Controlled, Scalable, Decentralized Placement of replicated Data using CRUSH</term>
      <listitem>
       <para>
	The ceph system uses a unique map called CRUSH (Controlled
	Replication Under Scalable Hashing) to assign data to
	OSDs in an efficient manner. Data assignment offsets are generated
	as opposed to looked in tables. This does away with disk look-up
	which come with conventional allocation table based systems,
	reducing the communication between the storage and the client.
	The client armed with the CRUSH map and the meta data such as
	object name and byte offset knows where it can find the
	data or which OSD it needs to place the data.
       </para>
       <para>
	CRUSH maintains a hierarchy of devices and the
	replica placement policy. As new devices are added, data from
	existing nodes is moved to the new device to improve distribution
	with regard to workload and resilience. As a part of the replica
	placement policy, it can add weights to the devices so, some
	devices are more favored as opposed to others. This could be used
	to give more weights to Solid State Devices (SSDs) and lower weights
	to conventional rotational hard disks to get overall better
	performance.
       </para>
       <para>
        CRUSH is designed to optimally distribute data to make use of available
	devices efficiently. CRUSH supports different ways of data
	distribution such as the following:
       </para>
	<itemizedlist>
	 <listitem>
	  <para>
	   n-way replication (mirroring)
	  </para>
	 </listitem>
	 <listitem>
	  <para>
	   RAID parity schemes
	  </para>
	 </listitem>
	 <listitem>
	  <para>
	   Erasure Coding
	  </para>
	 </listitem>
	 <listitem>
	  <para>
	   Hybrid approaches such as RAID-10
	  </para>
	 </listitem>
	</itemizedlist>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Reliable Autonomic Distributed Object Storage (&rados;)</term>
      <listitem>
       <para>
	The intelligence in the OSDs allows tasks such as data
	replication and migration for self-management and self-healing
	automatically. By default, data written to ceph storage is
	replicated within the OSDs. The level and type of replication is
	configurable. In case of failures, the CRUSH map is updated and
	data is written to new (replicated) OSDs.
       </para>
       <para>
	The intelligence of OSDs enables to handle data replication,
	data migration, failure detection and recovery. These tasks are
	automatically and autonomously managed. This also allows the
	creation of various pools for different sort of I/O.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Replicated Monitor Servers</term>
      <listitem>
       <para>
	The monitor servers keep track of all the devices in the
	system. They manage the CRUSH map which is used to determine
	where the data needs to be placed. In case of failures of any
	of the OSDs, the CRUSH map is re-generated and re-distributed to
	the rest of the system. At a given time, it is recommended that
	a system contains multiple monitor servers to add redundancy
	and improve resilience.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>

 </sect1>
</chapter>
