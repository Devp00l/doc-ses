<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
"novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ceph.operating">
 <title>Operating a &ceph; Cluster</title>
 <para>
  This section introduces information on basic cluster services operating.
  You will learn how to start or stop &ceph; services, how to monitor a
  cluster's state, how to use and modify CRUSH maps, and how to manage
  storage pools.
 </para>
 <sect1 id="ceph.operating.services">
  <title>Operating &ceph; Services</title>

  <para>
   &ceph; related services are operated with the
   <command>systemctl</command> command. The operation takes place on the
   node you are currently logged in. You need to have &rootuser; privileges
   to be able to operate on &ceph; services.
  </para>

  <para>
   The following subcommands are supported for all &ceph; services:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service(s) specified by
      <replaceable>target</replaceable>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service(s) specified by <replaceable>target</replaceable>
      if they are not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service(s) specified by
      <replaceable>target</replaceable>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service(s) specified by <replaceable>target</replaceable>
      so that they are automatically started on system start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service(s) specified by <replaceable>target</replaceable>
      so that they are not automatically started on system start-up. You
      have to start them manually with <command>systemctl start</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following subcommand is supported for OSD related targets:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl mask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Prevents the service from being started manually or automatically.
      <command>systemctl</command> is creating a symlink from the specific
      target file located in <filename>/etc/systemd/system/</filename> to
      <filename>/dev/null</filename>. Targets in
      <filename>/etc/systemd</filename> override those provided by packages
      in <filename>/usr/lib/systemd</filename>. &systemd; recognizes the
      symlink and will not start the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl unmask <replaceable>target</replaceable>
    </term>
    <listitem>
     <para>
      Opposite to <command>systemctl mask</command>. Removes the symlink
      from the specific target file located in
      <filename>/etc/systemd/system/</filename> to
      <filename>/dev/null</filename>, so that targets located in
      <filename>/usr/lib/systemd</filename> are valid again.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   You can either operate on all &ceph; services at once, or all &ceph;
   services of a certain type (such as OSD or monitor), or a specific
   instance of service identified by its instance name.
  </para>

  <sect2>
   <title>Operating on All &ceph; Services</title>
   <para>
    To operate on all &ceph; services at once, run:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph*</screen>
   <para>
    For example:
   </para>
<screen>systemctl status ceph*
ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:09:12 EST; 4min 52s ago
  Main PID: 2788 (ceph-osd)
    CGroup:
/system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─2788 /usr/bin/ceph-osd -f \-\-cluster ceph \-\-id 0
Jan 23 09:09:12 ceph-node2 ceph-osd-prestart.sh[2737]:
create-or-move updated item name 'osd.0' weight 0.01 at location
{host=ceph-node2,root=default} to crush map
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]: starting osd.0 at
:/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
 Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device

ceph-radosgw@ceph4.service - Ceph rados gateway
    Loaded: loaded
(/usr/lib/systemd/system/ceph-radosgw@.service; enabled)
    Active: active (running) since Fri 2015-01-23 09:11:29
EST; 2min 35s ago
  Main PID: 3741 (radosgw)
    CGroup:
/system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@ceph4.service
            └─3741 /usr/bin/radosgw -f \-\-conf
/etc/ceph/ceph.conf \-\-name client.radosgw.ceph4
 Jan 23 09:11:31 ceph-node2 radosgw[3741]: 2015-01-23
09:11:31.206129 7f60e1df9700 -1 failed to list objects pool_iterate returned r=-2</screen>
   <tip>
    <para>
     <command>systemctl</command> &ceph; commands cannot operate on services
     not enabled, not masked, and those which have never been started. To
     include all present services on a node, use the
     <command>rcceph</command> wrapper. For example:
    </para>
<screen>rcceph status</screen>
   </tip>
  </sect2>

  <sect2>
   <title>Operating on All &ceph; Services of a Specific Type</title>
   <para>
    It is possible to address services of a specific type only &mdash; such
    as OSDs, monitors, or &rgw;:
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>*</screen>
   <para>
    A few practical examples follow:
   </para>
   <para>
    Print the status of all monitor services on a node:
   </para>
<screen>systemctl status ceph-mon*</screen>
   <para>
    Start all OSD services on a node:
   </para>
<screen>systemctl start ceph-osd*</screen>
   <para>
    Stop all &rgw; services on a node:
   </para>
<screen>systemctl stop ceph-radosgw*</screen>
  </sect2>

  <sect2>
   <title>Operating on a Specific &ceph; Service</title>
   <para>
    If you want to operate on a single service on a node, you need to know
    its instance name. It is available in the output of <command>systemctl
    status ceph*</command>.
   </para>
<screen>systemctl <replaceable>subcommand</replaceable> ceph-<replaceable>service_type</replaceable>@<replaceable>instance_name</replaceable>.service</screen>
   <para>
    Print the status information of the OSD service with instance name
    <literal>0</literal>:
   </para>
<screen>systemctl status ceph-osd@0.service</screen>
  </sect2>

<!--
* possible subcommands: status, start, stop, enable, disable, mask (for OSD * only)
* possible services: ceph-osd, ceph-mon, ceph-radpsgw
* systemctl status ceph* - print the status of all &ceph; related services on a
* systemctl status ceph-osd@* - print the status of all OSDs services on a
* systemctl status ceph-mon@* - print the status of all monitor services on a
* systemctl status ceph-radosgw@* - print the status of all &rgw; services on a node
* systemctl status ceph-osd@<instance_name>.service - print information about * the specific OSD service.
* systemctl status ceph-mon@<instance_name>.service - print information about * the specific monitor service.
* systemctl status ceph-radosgw@<instance_name>.service - print information about the specific &rgw; service.
-->

<!--
  http://docs.ceph.com/docs/master/rados/operations/operating/


(15:03:41) oms101: exact commadns to follow in a shrot time
(15:03:50) oms101: but you should see the answer with
(15:04:02) oms101: systemctrl status ceph*
(15:04:06) oms101: if they are running
(15:04:12) oms101: I think its 
(15:04:34) oms101: systemctrl status ceph-radosgw@(instancename).service
(15:04:50) oms101: systemctrl start ceph-radosgw@(instancename).service
(15:04:58) oms101: systemctrl stop ceph-radosgw@(instancename).service
(15:05:07) oms101: systemctrl enable ceph-radosgw@(instancename).service
(15:05:15) oms101: systemctrl disable ceph-radosgw@(instancename).service
(15:05:44) oms101: does that answe rmost questions?
(15:12:51) oms101: systemctl
(15:13:01) oms101: reemove the r
(15:13:46) tbazant_lap: and these command are to be run on each node/monitor?
(15:14:08) oms101: on each node with manual install of radosrg
(15:14:14) oms101: radosgw
(15:14:38) tbazant_lap: and apart from rgw, to operate osds and monitors we have
the same commands?
(15:14:40) oms101: # systemctl status  ceph*
(15:14:40) oms101: ceph-osd@0.service - Ceph object storage daemon
(15:14:40) oms101:    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service;
enabled)
(15:14:40) oms101:    Active: active (running) since Fri 2015-01-23 09:09:12
EST; 4min 52s ago
(15:14:40) oms101:  Main PID: 2788 (ceph-osd)
(15:14:40) oms101:    CGroup:
/system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
(15:14:42) oms101:            └─2788 /usr/bin/ceph-osd -f \-\-cluster ceph \-\-id 0
(15:14:44) oms101: Jan 23 09:09:12 ceph-node2 ceph-osd-prestart.sh[2737]:
create-or-move updated item name 'osd.0' weight 0.01 at location
{host=ceph-node2,root=default} to crush map
(15:14:47) oms101: Jan 23 09:09:12 ceph-node2 ceph-osd[2788]: starting osd.0 at
:/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
(15:14:50) oms101: Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
(15:14:52) oms101: Jan 23 09:09:12 ceph-node2 ceph-osd[2788]:
HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
(15:14:54) oms101: ceph-radosgw@ceph4.service - Ceph rados gateway
(15:14:56) oms101:    Loaded: loaded
(/usr/lib/systemd/system/ceph-radosgw@.service; enabled)
(15:15:00) oms101:    Active: active (running) since Fri 2015-01-23 09:11:29
EST; 2min 35s ago
(15:15:02) oms101:  Main PID: 3741 (radosgw)
(15:15:04) oms101:    CGroup:
/system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@ceph4.service
(15:15:06) oms101:            └─3741 /usr/bin/radosgw -f \-\-conf
/etc/ceph/ceph.conf \-\-name client.radosgw.ceph4
(15:15:08) oms101: Jan 23 09:11:31 ceph-node2 radosgw[3741]: 2015-01-23
09:11:31.206129 7f60e1df9700 -1 failed to list objects pool_iterate returned
r=-2
(15:15:11) oms101: this nmode has rados on it and a osd
(15:15:13) oms101: yes
(15:15:19) oms101: instancename is different for each radosgw and each mon and
each osd
(15:16:14) tbazant_lap: what does the star mean in ceph* ?
(15:16:43) oms101: it measn match all things which start with ceph
(15:16:58) oms101: (but wont match services that are not started / enabled
(15:17:09) oms101: * is dos style wildcard
(15:17:21) tbazant_lap: ok
(15:17:42) tbazant_lap: so for instance how do i query if the osd i'm logged in
is running?
(15:17:47) tbazant_lap: a single ome
(15:17:50) tbazant_lap: one
(15:18:25) oms101: two ways
(15:18:29) oms101: either 
(15:18:40) oms101: systemctl status ceph*
(15:19:22) oms101: rcceph status
(15:19:28) oms101: systemctl status ceph-osd@*
(15:19:53) oms101: rcceph status will show even not started not enabled osds
(15:20:29) oms101: while unstarted . neverstarted / not enabled osds wont be
seen with "systemctl status"
(15:21:46) tbazant_lap: so we have ceph-radosgw@, ceph-osd@...and something
else?
  (15:22:54) oms101:  ceph-mon@ ceph-radosgw@
  (15:23:18) oms101: rcceph will be expanded to include radosgw later
  (15:24:58) tbazant_lap: thanks,that's enough for now
  (15:25:23) oms101: greatV
(15:42:32) tbazant_lap: one more question: do we have systemctl start ceph-all
(15:43:17) oms101: systemctl start ceph
(15:43:30) oms101: shodul start things that have been enabled / started before
(15:43:46) oms101: but if not enabled or started before it wont start them
(15:43:50) oms101: rcceph start 
(15:43:52) oms101: will
(15:44:25) tbazant_lap: so what is the reason to use systemctl start ceph ?
(15:45:09) oms101: soem peopel want to use systemctl rather than a suseism
script I made up :)
(15:45:48) oms101: rc* scripts are a suseism
(15:45:54) tbazant_lap: and how do they start what hasnt been started before?
(15:46:13) oms101: this is why I made rcceph
(15:46:23) oms101: as I think systemd is not user friendly
(15:46:33) tbazant_lap: it is not at all :-)
(15:46:34) oms101: tim thinks Im mad -> and rcceph shoudl be deleted
(15:46:49) tbazant_lap: systemd should be deleted
(15:47:04) oms101: systemctrl is systemd and does in my opinion a poor job on
"not started or enabled paramterised services"
(15:47:41) tbazant_lap: one more: systemctl start ceph-osd
(15:47:45) tbazant_lap: do we have rhis?
(15:47:48) tbazant_lap: this
(15:48:15) oms101: nope
(15:48:21) oms101: systemctl start ceph-osd*
(15:48:34) oms101: will work if the service has been started before or enabled 
(15:48:59) tbazant_lap: andit will start only osd type of daemon?
(15:49:01) oms101: but not if the service has never been started before or
enabled
(15:49:07) oms101: yes
(15:49:24) oms101: it will start only osd type of daemon? -> yes
(16:00:17) tbazant_lap: and systemctl start ceph-mon*
(16:00:21) tbazant_lap: exists?
(16:00:45) oms101: yes
(16:01:05) tbazant_lap: and systemctl ceph-mds*
(16:01:07) tbazant_lap: ?
(16:01:14) oms101: not doen
(16:01:25) oms101: we dont support this (and have not released it yet)
(16:02:59) tbazant_lap: ok
(16:05:07) tbazant_lap: and how about /etc/init.d.ceph - it's not supported
infavour of systemctl
(16:05:24) oms101: you go it 
(16:05:31) oms101: dont you just love systemd
(16:05:51) oms101: tim -> was cross with me for putting rcceph in thier -> so it
coudl provide backwards compatability
  -->
 </sect1>
 <sect1 id="ceph.monitor">
  <title>Monitoring a Cluster</title>

<!-- 2014-01-13 tbazant: reusing http://docs.ceph.com/docs/master/rados/operations/monitoring/ -->

  <para>
   Once you have a running cluster, you may use the <command>ceph</command>
   tool to monitor your cluster. Monitoring a cluster typically involves
   checking OSD status, monitor status, placement group status and metadata
   server status.
  </para>

  <tip>
   <title>Interactive Mode</title>
   <para>
    To run the <command>ceph</command> tool in an interactive mode, type
    <command>ceph</command> at the command line with no arguments. The
    interactive mode is more convenient if you are going to enter more
    <command>ceph</command> commands in a row. For example:
   </para>
<screen>ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status</screen>
  </tip>

  <sect2 id="monitor.health">
   <title>Checking Cluster Health</title>
   <para>
    After you start your cluster, and before you start reading and/or
    writing data, check your cluster’s health first. You can check on the
    health of your &ceph; cluster with the following:
   </para>
<screen>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>
   <para>
    If you specified non-default locations for your configuration or
    keyring, you may specify their locations:
   </para>
<screen>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
   <para>
    Upon starting the &ceph; cluster, you will likely encounter a health
    warning such as <literal>HEALTH_WARN XXX num placement groups
    stale</literal>. Wait a few moments and check it again. When your
    cluster is ready, <command>ceph health</command> should return a message
    such as <literal>HEALTH_OK</literal>. At that point, it is okay to begin
    using the cluster.
   </para>
  </sect2>

  <sect2 id="monitor.watch">
   <title>Watching a Cluster</title>
   <para>
    To watch the cluster’s ongoing events, open a new terminal and enter:
   </para>
<screen>ceph -w</screen>
   <para>
    &ceph; will print each event. For example, a tiny &ceph; cluster
    consisting of one monitor, and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41338: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
             952 active+clean

2014-06-02 15:45:21.655871 osd.0 [INF] 17.71 deep-scrub ok
2014-06-02 15:45:47.880608 osd.1 [INF] 1.0 scrub ok
2014-06-02 15:45:48.865375 osd.1 [INF] 1.3 scrub ok
2014-06-02 15:45:50.866479 osd.1 [INF] 1.4 scrub ok
[...]
2014-06-02 15:45:55.720929 mon.0 [INF] pgmap v41343: 952 pgs: \
 1 active+clean+scrubbing+deep, 951 active+clean; 17130 MB data, 115 GB used, \
 167 GB / 297 GB avail</screen>
   <para>
    The output provides the following information:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Cluster ID
     </para>
    </listitem>
    <listitem>
     <para>
      Cluster health status
     </para>
    </listitem>
    <listitem>
     <para>
      The monitor map epoch and the status of the monitor quorum
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD map epoch and the status of OSDs
     </para>
    </listitem>
    <listitem>
     <para>
      The placement group map version
     </para>
    </listitem>
    <listitem>
     <para>
      The number of placement groups and pools
     </para>
    </listitem>
    <listitem>
     <para>
      The <emphasis>notional</emphasis> amount of data stored and the number
      of objects stored; and,
     </para>
    </listitem>
    <listitem>
     <para>
      The total amount of data stored.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>How &ceph; Calculates Data Usage</title>
    <para>
     The <literal>used</literal> value reflects the actual amount of raw
     storage used. The <literal>xxx GB / xxx GB</literal> value means the
     amount available (the lesser number) of the overall storage capacity of
     the cluster. The notional number reflects the size of the stored data
     before it is replicated, cloned or snapshotted. Therefore, the amount
     of data actually stored typically exceeds the notional amount stored,
     because &ceph; creates replicas of the data and may also use storage
     capacity for cloning and snapshotting.
    </para>
   </tip>
  </sect2>

  <sect2 id="monitor.stats">
   <title>Checking a Cluster’s Usage Stats</title>
   <para>
    To check a cluster’s data usage and data distribution among pools, you
    can use the <command>df</command> option. It is similar to Linux
    <command>df</command>. Execute the following:
   </para>
<screen>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED 
    27570M     27304M         266M          0.97 
POOLS:
    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS 
    data             0       120         0         5064M           4 
    metadata         1         0         0         5064M           0 
    rbd              2         0         0         5064M           0 
    hot-storage      4       134         0         4033M           2 
    cold-storage     5      227k         0         5064M           1 
    pool1            6         0         0         5064M           0</screen>
   <para>
    The <literal>GLOBAL</literal> section of the output provides an overview
    of the amount of storage your cluster uses for your data.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>SIZE</literal>: The overall storage capacity of the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>AVAIL</literal>: The amount of free space available in the
      cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>RAW USED</literal>: The amount of raw storage used.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>% RAW USED</literal>: The percentage of raw storage used. Use
      this number in conjunction with the <literal>full ratio</literal> and
      <literal>near full ratio</literal> to ensure that you are not reaching
      your cluster’s capacity.
      <remark>tbazant: add xref to
     http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacity</remark>
      See Storage Capacity for additional details.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <literal>POOLS</literal> section of the output provides a list of
    pools and the notional usage of each pool. The output from this section
    <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
    example, if you store an object with 1MB of data, the notional usage
    will be 1MB, but the actual usage may be 2MB or more depending on the
    number of replicas, clones and snapshots.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>NAME</literal>: The name of the pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ID</literal>: The pool ID.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>USED</literal>: The notional amount of data stored in
      kilobytes, unless the number appends M for megabytes or G for
      gigabytes.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>%USED</literal>: The notional percentage of storage used per
      pool.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>OBJECTS</literal>: The notional number of objects stored per
      pool.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     The numbers in the POOLS section are notional. They are not inclusive
     of the number of replicas, snapshots or clones. As a result, the sum of
     the USED and %USED amounts will not add up to the RAW USED and %RAW
     USED amounts in the %GLOBAL section of the output.
    </para>
   </note>
  </sect2>

  <sect2 id="monitor.status">
   <title>Checking a Cluster’s Status</title>
   <para>
    To check a cluster’s status, execute the following:
   </para>
<screen>ceph status</screen>
   <para>
    or
   </para>
<screen>ceph -s</screen>
   <para>
    In interactive mode, type <command>status</command> and press
    <keycap
function="enter"/>.
   </para>
<screen>ceph> status</screen>
   <para>
    &ceph; will print the cluster status. For example, a tiny &ceph; cluster
    consisting of one monitor, and two OSDs may print the following:
   </para>
<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
  </sect2>

  <sect2 id="monitor.osdstatus">
   <title>Checking OSD Status</title>
   <para>
    You can check OSDs to ensure they are up and in by executing:
   </para>
<screen>ceph osd stat</screen>
   <para>
    or
   </para>
<screen>ceph osd dump</screen>
   <para>
    You can also check view OSDs according to their position in the CRUSH
    map.
   </para>
<screen>ceph osd tree</screen>
   <para>
    &ceph; will print out a CRUSH tree with a host, its OSDs, whether they
    are up and their weight.
   </para>
<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
  </sect2>

  <sect2 id="monitor.monstatus" >
   <title>Checking Monitor Status</title>
   <para>
    If your cluster has multiple monitors (likely), you should check the
    monitor quorum status after you start the cluster before reading and/or
    writing data. A quorum must be present when multiple monitors are
    running. You should also check monitor status periodically to ensure
    that they are running.
   </para>
   <para>
    To display the monitor map, execute the following:
   </para>
<screen>ceph mon stat</screen>
   <para>
    or
   </para>
<screen>ceph mon dump</screen>
   <para>
    To check the quorum status for the monitor cluster, execute the
    following:
   </para>
<screen>ceph quorum_status</screen>
   <para>
    &ceph; will return the quorum status. For example, a &ceph; cluster
    consisting of three monitors may return the following:
   </para>
<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
  </sect2>

  <sect2 id="monitor.mdsstatus">
   <title>Checking MDS Status</title>
   <para>
    Metadata servers provide metadata services for &ceph; FS. Metadata
    servers have two sets of states: <literal>up | down</literal> and
    <literal>active | inactive</literal>. To ensure your metadata servers
    are <literal>up</literal> and <literal>active</literal>, execute the
    following:
   </para>
<screen>ceph mds stat</screen>
   <para>
    To display details of the metadata cluster, execute the following:
   </para>
<screen>ceph mds dump</screen>
  </sect2>

  <sect2 id="monitor.pgroupstatus">
   <title>Checking Placement Group States</title>
   <para>
    Placement groups map objects to OSDs. When you monitor your placement
    groups, you will want them to be <literal>active</literal> and
    <literal>clean</literal>. For a detailed discussion, refer to
    <remark>add xref
  to
  http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg</remark>
    Monitoring OSDs and Placement Groups.
   </para>
  </sect2>

  <sect2 id="monitor.adminsocket">
   <title>Using the Admin Socket</title>
   <para>
    The &ceph; admin socket allows you to query a daemon via a socket
    interface. By default, &ceph; sockets reside under
    <filename>/var/run/ceph</filename>. To access a daemon via the admin
    socket, login to the host running the daemon and use the following
    command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>
   <para>
    To view the available admin socket commands, execute the following
    command:
   </para>
<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>
   <para>
    The admin socket command enables you to show and set your configuration
    at runtime. See
    <remark>add xref to
  http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config</remark>
    Viewing a Configuration at Runtime for details.
   </para>
   <para>
    Additionally, you can set configuration values at runtime directly
    (i.e., the admin socket bypasses the monitor, unlike <command>ceph tell
    <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
    injectargs</command>, which relies on the monitor but does not require
    you to login directly to the host in question ).
   </para>
  </sect2>
 </sect1>
 <sect1 id="ceph.pools">
  <title>Managing Storage Pools</title>

<!-- 2014-11-21 tbazant: reusing
 http://ceph.com/docs/master/rados/operations/pools/ -->

  <para>
   When you first deploy a cluster without creating a pool, &ceph; uses the
   default pools for storing data. A pool provides you with:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Resilience</emphasis>: You can set how many OSDs are allowed
     to fail without losing data. For replicated pools, it is the desired
     number of copies/replicas of an object. A typical configuration stores
     an object and one additional copy (i.e., <emphasis>size =
     2</emphasis>), but you can determine the number of copies/replicas. For
     erasure coded pools, it is the number of coding chunks (i.e.
     <emphasis>m=2</emphasis> in the erasure code profile).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Placement Groups</emphasis>: You can set the number of
     placement groups for the pool. A typical configuration uses
     approximately 100 placement groups per OSD to provide optimal balancing
     without using up too many computing resources. When setting up multiple
     pools, be careful to ensure you set a reasonable number of placement
     groups for both the pool and the cluster as a whole.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, a
     CRUSH ruleset mapped to the pool enables CRUSH to identify a rule for
     the placement of the object and its replicas (or chunks for erasure
     coded pools) in your cluster. You can create a custom CRUSH rule for
     your pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Snapshots</emphasis>: When you create snapshots with
     <command>ceph osd pool mksnap</command>, you effectively take a
     snapshot of a particular pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Set Ownership</emphasis>: You can set a user ID as the owner
     of a pool.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To organize data into pools, you can list, create, and remove pools. You
   can also view the utilization statistics for each pool.
  </para>

  <sect2 id="ceph.pools.operate">
   <title>Operating Pools</title>
   <para>
    This section introduces practical information to perform basic tasks
    with pools. You can find out how to list, create, delete pools, as well
    as show pool statistics or manage snapshots of a pool.
   </para>
   <sect3>
    <title>List Pools</title>
    <para>
     To list your cluster’s pools, execute:
    </para>
<screen>ceph osd lspools
0 data,1 metadata,2 rbd,</screen>
    <para>
     The command outputs information about data, metadata, and rados block
     devices (rbd).
    </para>
   </sect3>
   <sect3>
    <title>Create a Pool</title>
    <para>
     To create a replicated pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool-name</replaceable> <replaceable>pg-num</replaceable> <replaceable>pgp-num</replaceable> <replaceable>pool-type</replaceable> <replaceable>crush-ruleset-name</replaceable></screen>
    <para>
     To create an erasure pool, execute:
    </para>
<screen>ceph osd pool create <replaceable>pool-name</replaceable> <replaceable>pg-num</replaceable> <replaceable>pgp-num</replaceable> <replaceable>pool-type</replaceable> <replaceable>erasure-code-profile</replaceable> \
<replaceable>crush-ruleset-name</replaceable></screen>
    <variablelist>
     <varlistentry>
      <term>pool-name</term>
      <listitem>
       <para>
        The name of the pool. It must be unique. This option is required.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The total number of placement groups for the pool. This option is
        required. Default value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp-num</term>
      <listitem>
       <para>
        The total number of placement groups for placement purposes. This
        should be equal to the total number of placement groups, except for
        placement group splitting scenarios. This option is required.
        Default value is 8.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp-type</term>
      <listitem>
       <para>
        The pool type which may either be <emphasis>replicated</emphasis> to
        recover from lost OSDs by keeping multiple copies of the objects or
        <emphasis>erasure</emphasis> to get a kind of generalized RAID5
        capability. The replicated pools require more raw storage but
        implement all &ceph; operations. The erasure pools require less raw
        storage but only implement a subset of the available operations.
        Default is 'replicated'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush-ruleset-name</term>
      <listitem>
       <para>
        The name of the crush ruleset for this pool. If the specified
        ruleset does not exist, the creation of replicated pool will fail
        with -ENOENT. But replicated pool will create a new erasure ruleset
        with specified name. The default value is 'erasure-code' for erasure
        pool. Picks up &ceph; configuration variable
        <option>osd_pool_default_crush_replicated_ruleset</option> for
        replicated pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>erasure-code-profile=profile</term>
      <listitem>
       <para>
        For erasure pools only. Use the erasure code profile. It must be an
        existing profile as defined by <command>osd erasure-code-profile
        set</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     When you create a pool, set the number of placement groups to a
     reasonable value (e.g., 100). Consider the total number of placement
     groups per OSD too. Placement groups are computationally expensive, so
     performance will degrade when you have many pools with many placement
     groups (e.g., 50 pools with 100 placement groups each). The point of
     diminishing returns depends upon the power of the OSD host.
    </para>
    <para>
     See
     <ulink
    url="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement
     Groups</ulink> for details on calculating an appropriate number of
     placement groups for your pool.
    </para>
   </sect3>
   <sect3>
    <title>Set Pool Quotas</title>
    <para>
     You can set pool quotas for the maximum number of bytes and/or the
     maximum number of objects per pool.
    </para>
<screen>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph osd pool set-quota data max_objects 10000</screen>
    <para>
     To remove a quota, set its value to 0.
    </para>
   </sect3>
   <sect3>
    <title>Delete a Pool</title>
    <para>
     To delete a pool, execute:
    </para>
<screen>ceph osd pool delete <replaceable>pool-name</replaceable> <replaceable>pool-name</replaceable> --yes-i-really-really-mean-it</screen>
    <para>
     If you created your own rulesets and rules for a pool you created, you
     should consider removing them when you no longer need your pool. If you
     created users with permissions strictly for a pool that no longer
     exists, you should consider deleting those users too.
    </para>
   </sect3>
   <sect3>
    <title>Rename a Pool</title>
    <para>
     To rename a pool, execute:
    </para>
<screen>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
    <para>
     If you rename a pool and you have per-pool capabilities for an
     authenticated user, you must update the user’s capabilities (i.e.,
     caps) with the new pool name.
    </para>
   </sect3>
   <sect3>
    <title>Show Pool Statistics</title>
    <para>
     To show a pool’s utilization statistics, execute:
    </para>
<screen>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
   </sect3>
   <sect3>
    <title>Make a Snapshot of a Pool</title>
    <para>
     To make a snapshot of a pool, execute:
    </para>
<screen>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
   </sect3>
   <sect3>
    <title>Remove a Snapshot of a Pool</title>
    <para>
     To remove a snapshot of a pool, execute:
    </para>
<screen>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   </sect3>
   <sect3 id="ceph.pools.values">
    <title>Set Pool Values</title>
    <para>
     To set a value to a pool, execute:
    </para>
<screen>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
    <para>
     You may set values for the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>size</term>
      <listitem>
       <para>
        Sets the number of replicas for objects in the pool. See
        <xref
       linkend="ceph.pools.options.num_of_replicas"/> for
        further details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>min_size</term>
      <listitem>
       <para>
        Sets the minimum number of replicas required for I/O. See
        <xref
       linkend="ceph.pools.options.num_of_replicas"/> for
        further details. Replicated pools only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crash_replay_interval</term>
      <listitem>
       <para>
        The number of seconds to allow clients to replay acknowledged, but
        uncommitted requests.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating
        data placement.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>crush_ruleset</term>
      <listitem>
       <para>
        The ruleset to use for mapping object placement in the cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hashpspool</term>
      <listitem>
       <para>
        Set/Unset HASHPSPOOL flag on a given pool. 1 sets flag, 0 unsets
        flag.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_type</term>
      <listitem>
       <para>
        Enables hit set tracking for cache pools. See
        <ulink url="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</ulink> for additional information. This option can have the
        following values: <literal>bloom</literal>,
        <literal>explicit_hash</literal>,
        <literal>explicit_object</literal>. Default is
        <literal>bloom</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_count</term>
      <listitem>
       <para>
        The number of hit sets to store for cache pools. The higher the
        number, the more RAM consumed by the
        <systemitem>ceph-osd</systemitem> daemon. Default is
        <literal>1</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_period</term>
      <listitem>
       <para>
        The duration of a hit set period in seconds for cache pools. The
        higher the number, the more RAM consumed by the
        <systemitem>ceph-osd</systemitem> daemon.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>hit_set_fpp</term>
      <listitem>
       <para>
        The false positive probability for the bloom hit set type. See
        <ulink url="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
        Filter</ulink> for additional information. Valid range is 0.0 - 1.0
        Default is <literal>0.05</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_dirty_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing modified (dirty) objects
        before the cache tiering agent will flush them to the backing
        storage pool. Default is <literal>.4</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_target_full_ratio</term>
      <listitem>
       <para>
        The percentage of the cache pool containing unmodified (clean)
        objects before the cache tiering agent will evict them from the
        cache pool. Default is <literal>.8</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_bytes</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_bytes</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_max_objects</term>
      <listitem>
       <para>
        &ceph; will begin flushing or evicting objects when the
        <option>max_objects</option> threshold is triggered.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_flush_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will flush an
        object from the cache pool to the storage pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cache_min_evict_age</term>
      <listitem>
       <para>
        The time (in seconds) before the cache tiering agent will evict an
        object from the cache pool.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Get Pool Values</title>
    <para>
     To get a value from a pool, execute:
    </para>
<screen>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
    <para>
     You can get values for keys listed in
     <xref linkend="ceph.pools.values"/> plus the following keys:
    </para>
    <variablelist>
     <varlistentry>
      <term>pg_num</term>
      <listitem>
       <para>
        The number of placement groups for the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pgp_num</term>
      <listitem>
       <para>
        The effective number of placement groups to use when calculating
        data placement. Valid range is equal to or less than
        <literal>pg_num</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 id="ceph.pools.options.num_of_replicas">
    <title>Set the Number of Object Replicas</title>
    <para>
     To set the number of object replicas on a replicated pool, execute the
     following:
    </para>
<screen>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
    <important>
     <para>
      The <replaceable>num-replicas</replaceable> includes the object
      itself. If you for example want the object and two copies of the
      object for a total of three instances of the object, specify 3.
     </para>
    </important>
    <para>
     For example:
    </para>
<screen>ceph osd pool set data size 3</screen>
    <para>
     You may execute this command for each pool.
    </para>
    <note>
     <para>
      An object might accept I/Os in degraded mode with fewer than
      <literal>pool size</literal> replicas. To set a minimum number of
      required replicas for I/O, you should use the
      <literal>min_size</literal> setting. For example:
     </para>
<screen>ceph osd pool set data min_size 2</screen>
     <para>
      This ensures that no object in the data pool will receive I/O with
      fewer than <literal>min_size</literal> replicas.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Get the Number of Object Replicas</title>
    <para>
     To get the number of object replicas, execute the following:
    </para>
<screen>ceph osd dump | grep 'replicated size'</screen>
    <para>
     &ceph; will list the pools, with the <literal>replicated size</literal>
     attribute highlighted. By default, &ceph; creates two replicas of an
     object (a total of three copies, or a size of 3).
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
