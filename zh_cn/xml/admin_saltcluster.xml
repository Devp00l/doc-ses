<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage.salt.cluster">
 <title>Salt 群集管理</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>是</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  部署 Ceph 群集后，有些时候可能还需要对它执行数项修改。这些修改包括添加或去除新的节点、磁盘或服务。本章介绍该如何完成这些管理任务。
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>添加新的群集节点</title>

  <para>
   为群集添加新节点的过程与<xref linkend="ceph.install.saltstack"/>中所述的初始群集节点部署过程几乎完全相同。
  </para>

  <procedure>
   <step>
    <para>
     在新节点上安装 SUSE Linux Enterprise Server 12 SP3、配置其网络设置使它能够正确解析 Salt 主控端主机名，并安装 <systemitem>salt-minion</systemitem> 包：
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     如果 Salt 主控端的主机名不是 <literal>salt</literal>，请编辑 <filename>/etc/salt/minion</filename> 并添加下面一行：
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     如果您对上面提到的配置文件进行了任何更改，请重启动 <systemitem>salt.minion</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     在 Salt 主控端上接受所有 Salt 密钥：
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     校验 <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> 是否也以新的 Salt 受控端为目标。有关更多细节，请参见<xref linkend="ds.depl.stages"/>的<xref linkend="ds.depl.s1"/>。
    </para>
   </step>
   <step>
    <para>
     运行准备阶段。该阶段会同步模块和粒度，以便新的受控端可以提供 DeepSea 期望的所有信息：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     运行发现阶段。该阶段将在 <filename>/srv/pillar/ceph/proposals</filename> 目录中写入新的文件项，您可在其中编辑相关的 .yml 文件：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     （可选）如果新添加的主机与现有命名模式不匹配，请更改 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。
    </para>
   </step>
   <step>
    <para>
     运行配置阶段。该阶段会读取 <filename>/srv/pillar/ceph</filename> 下的所有内容并相应地更新 Pillar。Pillar 用于储存可以使用 <command>salt '*' pillar.items</command> 访问的数据。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     配置和部署阶段包含新添加的节点：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>为节点添加新的角色</title>

  <para>
   您可通过 DeepSea 部署所有受支持的角色类型。有关受支持角色类型的更多信息以及匹配示例，请参见<xref linkend="policy.role.assignment"/>。
  </para>

  <tip>
   <title>必需的与可选的角色和阶段</title>
   <para>
    一般而言，在将新角色添加到群集节点时，建议您运行全部部署阶段 0 到 5。为了节省一些时间，可根据要部署的角色类型，跳过阶段 3 或 4。OSD 和 MON 角色包含核心服务，是 Ceph 的必需项，而其他角色（例如 RADOS 网关）则是可选项。DeepSea 部署阶段是分层的：阶段 3 部署核心服务，而阶段 4 则部署可选服务。
   </para>
   <para>
    因此，部署核心角色（例如在现有 OSD 节点上部署 MON）时需要运行阶段 3，可以跳过阶段 4。
   </para>
   <para>
    同样，部署可选服务（例如 RADOS 网关）时可以跳过阶段 3，但需要运行阶段 4。
   </para>
  </tip>

  <para>
   要将新服务添加到现有节点，请执行下列步骤：
  </para>

  <procedure>
   <step>
    <para>
     使用 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 来匹配现有主机与新角色。例如，如果您需要在 MON 节点上运行 RADOS 网关，命令行如下所示：
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     运行阶段 2 以更新 Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     运行阶段 3 以部署核心服务，或者运行阶段 4 以部署可选服务。同时运行这两个阶段也没有问题。
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    将 OSD 添加到现有群集时请注意，群集将在此后的一段时间内进行重新平衡。为了尽可能缩短重新平衡的时间，建议您同时添加所有要添加的 OSD。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>去除和重新安装群集节点</title>

  <para>
   要从群集中去除角色，请编辑 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 并去除相应的行。
  </para>

  <para>
   如果您只想去除角色，请运行阶段 2 和 5，如<xref linkend="ceph.install.stack"/>中所述。
  </para>

  <para>
   如果您要在去除的<emphasis>同时</emphasis>添加角色，请运行阶段 2 到 5，如<xref linkend="ceph.install.stack"/>中所述。
  </para>

  <note>
   <title>从群集中去除 OSD</title>
   <para>
    如果您需要从群集中去除特定 OSD 节点，请确保群集的可用磁盘空间多于要去除的磁盘空间。切记，去除 OSD 会导致整个群集进行重新平衡。
   </para>
  </note>

  <para>
   从受控端中去除角色时，其目的是撤销与该角色相关的所有更改。对于大部分角色，要实现该任务都很简单，但可能会存在包依赖性问题。如果卸装包，其依赖性就不存在。
  </para>

  <para>
   去除的 OSD 会显示为空白驱动器。相关任务会重写文件系统的开头，并会去除备份分区以及擦除分区表。
  </para>

  <note>
   <title>保留由其他方法创建的分区</title>
   <para>
    先前由其他方法（例如 <command>ceph-deploy</command>）配置的磁盘驱动器可能仍然包含分区。DeepSea 不会自动销毁这些分区。管理员必须重新声明这些驱动器。
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>从群集中去除 Salt 受控端</title>
   <para>
    举例来说，如果您的储存受控端名为“data1.ceph”、“data2.ceph”...“data6.ceph”，则 <filename>policy.cfg</filename> 中的相关行如下所示：
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    要去除 Salt 受控端“data2.ceph”，请将这些行更改为：
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    然后运行阶段 2 和 5：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>迁移节点</title>
   <para>
    假设出现下面的情况：在全新群集安装期间，您（管理员）在等待网关硬件就绪之时，将其中一个储存节点分配为独立的对象网关。现在，当网关的永久硬件就绪时，您就可以将所需角色最终指派给备份储存节点并去除网关角色。
   </para>
   <para>
    在针对新硬件运行阶段 0 和 1（请参见<xref linkend="ds.depl.stages"/>）之后，您将新网关命名为 <literal>rgw1</literal>。如果节点 <literal>data8</literal> 需要去除对象网关角色并添加储存角色，且 <filename>policy.cfg</filename> 如下所示：
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    则将它更改为：
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    运行阶段 2 到 5。阶段 3 会将 <literal>data8</literal> 添加为储存节点。稍候片刻，<literal>data8</literal> 将同时具有两个角色。阶段 4 会将对象网关角色添加到 <literal>rgw1</literal>，而阶段 5 会从 <literal>data8</literal> 中去除对象网关角色。
   </para>
  </example>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>为节点添加 OSD</title>
  <para>
   要将磁盘添加到现有 OSD 节点，请将磁盘添加到节点的 YAML 文件。该文件的路径是 <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/NODE_NAME.yml</filename>。保存之后，运行 DeepSea 阶段 2 和 3：
  </para>
  <screen><prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
  <para>
   无需要手动编辑 YAML 文件，DeepSea 可创建新的配置文件。要让 DeepSea 创建新的配置文件，必须移走现有的配置文件：
  </para>
  <screen><prompt>root@master # </prompt><command>old</command> /srv/pillar/ceph/proposals/profile-default/
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.1
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>去除 OSD</title>

  <para>
   可通过运行以下命令从群集中去除 Ceph OSD：
  </para>

<screen><prompt>root@master # </prompt><command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>通过 Salt 进行自动安装</title>

  <para>
   通过使用 Salt 反应器可让安装自动进行。对于虚拟环境或一致的硬件环境，此配置将允许创建具有指定行为的 Ceph 群集。
  </para>

  <warning>
   <para>
    Salt 无法根据反应器事件执行依赖性检查。存在让 Salt 主控端进入死循环的风险。
   </para>
  </warning>

  <para>
   自动安装需要：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     正确创建的 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。
    </para>
   </listitem>
   <listitem>
    <para>
     准备好放入 <filename>/srv/pillar/ceph/stack</filename> 目录中的自定义配置。
    </para>
   </listitem>
   <listitem>
    <para>
     必须将示例反应器文件 <filename>/usr/share/doc/packages/deepsea/reactor.conf</filename> 复制到 <filename>/etc/salt/master.d/reactor.conf</filename>。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   默认反应器配置只会运行阶段 0 和 1。如此可不必等待后续阶段完成即可测试反应器。
  </para>

  <para>
   第一个 salt-minion 启动时，即会开始阶段 0。锁定可阻止多个实例。所有受控端都完成阶段 0 后，将开始阶段 1。
  </para>

  <para>
   如果正确执行该操作，则会将 <filename>/etc/salt/master.d/reactor.conf</filename> 中的最后一行：
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   更改为
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="Deepsea.restart">
  <title>使用 DeepSea 重启动 Ceph 服务</title>

  <para>
   将更新应用到群集节点之后，需要重启动已指派的服务，以使用最近安装的版本。
  </para>

  <note>
   <title>检查重启动</title>
   <para>
    重启动群集的过程可能需要一些时间。可通过运行以下命令来使用 Salt 事件总线检查事件：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.event pretty=True</screen>
  </note>

  <sect2 xml:id="deepsea.restart.all">
   <title>重启动所有服务</title>
   <para>
    要重启动群集上的<emphasis>所有</emphasis>服务，请运行以下命令：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart</screen>
   <para>
    该脚本会按以下顺序迭代所有已配置的角色：MON、MGR、OSD、MDS、RADOS 网关、iSCSI 网关、NFS Ganesha。为了保持较短的停机时间并尽早发现潜在问题，请按顺序重启动各节点。例如，一次只重启动一个监视节点。如果群集处于降级、不良的状态，该命令还会等待群集恢复。
   </para>
  </sect2>

  <sect2 xml:id="deepsea.restart.specific">
   <title>重启动特定服务</title>
   <para>
    要重启动群集上的特定服务，请运行以下命令：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.<replaceable>service_name</replaceable></screen>
   <para>
    例如，要重启动所有 RADOS 网关，请运行以下命令：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.rgw</screen>
   <para>
    您可以使用以下目标：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.mon</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.mgr</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.osd</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.mds</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.rgw</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.igw</screen>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.ganesha</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>更新群集节点</title>

  <para>
   最好定期对您的群集节点应用滚动更新。要应用更新，请运行阶段 0：
  </para>

<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>

  <para>
   如果 DeepSea 检测到正在运行的 Ceph 群集，它会按顺序应用更新并重启动节点。DeepSea 遵照 Ceph 的官方建议，会先更新监视器，然后更新 OSD，最后更新附加的服务，例如 MDS、RADOS 网关、iSCSI 网关或 NFS Ganesha。DeepSea 如果检测到群集中存在问题，会停止更新过程。问题的触发因素可能是：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Ceph 报告“HEALTH_ERR”的持续时长超过 300 秒。
    </para>
   </listitem>
   <listitem>
    <para>
     查询 Salt 受控端以了解所指派的服务在更新之后是否仍然启动且正在运行。如果服务处于关闭状态超过 900 秒，则更新失败。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   做出这些安排可确保即使对于损坏或失败的更新，Ceph 群集仍可以运作。
  </para>

  <para>
   DeepSea 阶段 0 通过 <command>zypper update</command> 更新系统，并重引导系统（如果更新了内核）。若想免于强制重引导所有潜在节点，请确保在开始 DeepSea 阶段 0 之前，最新的内核已安装且正在运行。
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    如果您更想使用 <command>zypper patch</command> 命令来更新系统，请编辑 <filename>/srv/pillar/ceph/stack/global.yml</filename> 并添加下面一行：
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   您可以通过将下面几行添加到 <filename>/srv/pillar/ceph/stack/global.yml</filename>，更改 DeepSea 阶段 0 的默认重引导行为：
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal> 用于设置 Salt 主控端的阶段 0 行为，<literal>stage_prep_minion</literal> 用于设置所有受控端的行为。所有可用的参数如下：
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      安装更新并在更新之后重引导。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      安装更新而不重引导。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      重引导而不安装更新。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      不安装更新，也不重引导。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>停止或重引导群集</title>

  <para>
   在某些情况下，可能需要停止或重引导整个群集。建议您仔细检查运行中服务的依赖性。下列步骤概要说明如何停止和启动群集：
  </para>

  <procedure>
   <step>
    <para>
     告知 Ceph 群集不要将 OSD 标记为 out：
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     以下面的顺序停止守护程序和节点：
    </para>
    <orderedlist>
     <listitem>
      <para>
       储存客户端
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或 RADOS 网关
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 管理器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 监视器
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     如果需要，执行维护任务。
    </para>
   </step>
   <step>
    <para>
     以与关闭过程相反的顺序启动节点和服务器：
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph 监视器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 管理器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或 RADOS 网关
      </para>
     </listitem>
     <listitem>
      <para>
       储存客户端
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     去除 noout 标志：
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>自定义 <filename>ceph.conf</filename> 文件</title>

  <para>
   如果您需要将自定义设置放入 <filename>ceph.conf</filename> 文件中，可通过修改 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename> 目录中的配置文件来实现：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
    </listitem>
    <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>运行阶段 3</title>
   <para>
    对上述配置文件进行自定义更改之后，运行阶段 3 以将这些更改应用到群集节点：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   这些文件来自 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 模板文件，与 Ceph 配置文件接受的不同部分相对应。将配置片段放入正确的文件以便 DeepSea 将该文件放入正确的部分中。不需要添加任何部分标题。
  </para>

  <tip>
   <para>
    要将任何配置选项仅应用于守护程序的特定实例，请添加标题，例如 <literal>[osd.1]</literal>。以下配置选项将只应用于 ID 为 1 的 OSD 守护程序。
   </para>
  </tip>

  <sect2>
   <title>覆盖默认值</title>
   <para>
    位于部分后面的语句会重写前面的语句。因此，可以按 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 模板的指定覆盖默认配置。例如，要关闭 cephx 身份验证，可将下面三行添加到 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 文件：
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>包括配置文件</title>
   <para>
    如果您需要应用许多自定义配置，请在自定义配置文件中使用以下包括语句来让文件管理更轻松。下面是 <filename>osd.conf</filename> 文件的示例：
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    在前面的示例中，<filename>osd1.conf</filename>、<filename>osd2.conf</filename>、<filename>osd3.conf</filename> 和 <filename>osd4.conf</filename> 文件包含特定于相关 OSD 的配置选项。
   </para>
   <tip>
    <title>运行时配置</title>
    <para>
     对 Ceph 配置文件所做的更改将在相关 Ceph 守护程序重启动之后生效。有关更改 Ceph 运行时配置的更多信息，请参见<xref linkend="ceph.config.runtime"/>。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.config.runtime">
  <title>运行时 Ceph 配置</title>

  <para>
   <xref linkend="ds.custom.cephconf"/>介绍如何对 Ceph 配置文件 <filename>ceph.conf</filename> 进行更改。但是，实际群集行为并非由 <filename>ceph.conf</filename> 文件的当前状态决定，而是由正在运行的 Ceph 守护程序的配置（储存在内存中）而定。
  </para>

  <para>
   如果您要查询个别 Ceph 守护程序以了解特定的配置设置，可以在运行守护程序的节点上使用 <emphasis>admin socket</emphasis>。例如，以下命令可获取名为 <literal>osd.0</literal> 的守护程序中 <option>osd_max_write_size</option> 配置参数的值：
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
 config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   当 Ceph 守护程序启动时，它会分析 <filename>ceph.conf</filename> 文件以获取该文件的初始配置。确保 <filename>ceph.conf</filename> 更改生效的一个方法是重引导整个群集（所有群集节点）。这可确保重启动所有 Ceph 守护程序。
  </para>

  <para>
   在生产群集中，并不希望重引导所有节点甚或重启动所有守护程序以读取配置更改。在此情况下，可以让守护程序更改它们的内存内设置。例如，以下命令可针对群集中所有 OSD 将 <option>osd_max_write_size</option> 参数更改为“50”：
  </para>

<screen><prompt>root # </prompt>ceph tell osd.* config set osd_max_write_size 50</screen>

  <para>
   通过这种方法，您就可以更改正在运行的守护程序的配置，而不必重引导任何节点或重启动任何守护程序。
  </para>
 </sect1>
</chapter>
