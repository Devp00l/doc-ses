<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha.ceph.upgrade">
 <title>从旧版本升级</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>编辑</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  本章介绍将 SUSE Enterprise Storage 从旧版本升级到最新版本的步骤。
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>阅读发行说明</title>

  <para>
   在发行说明中，可以找到有关自 SUSE Enterprise Storage 的上一个版本发行后所进行的更改的其他信息。检查发行说明以了解：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     您的硬件是否有特殊的注意事项。
    </para>
   </listitem>
   <listitem>
    <para>
     所用的任何软件包是否已发生重大更改。
    </para>
   </listitem>
   <listitem>
    <para>
     是否需要对您的安装实施特殊预防措施。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   发行说明还提供未能及时编入手册中的信息。它们还包含有关已知问题的说明。
  </para>

  <para>
   安装包 <package>release-notes-ses</package> 之后，可在本地的 <filename>/usr/share/doc/release-notes</filename> 目录中或 <link xlink:href="https://www.suse.com/releasenotes/"/> 网页上找到发行说明。
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>一般升级过程</title>

  <para>
   在升级 Ceph 群集之前，需要针对 SCC 或 SMT 正确注册底层 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage。当群集已联机并正在运行时，可以升级群集中的守护程序。某些类型的守护程序依赖于其他守护程序。例如，Ceph RADOS 网关依赖于 Ceph 监视器和 Ceph OSD 守护程序。建议按以下顺序升级：
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     Ceph 监视器
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph 管理器
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph OSD
    </para>
   </listitem>
   <listitem>
    <para>
     元数据服务器
    </para>
   </listitem>
   <listitem>
    <para>
     RADOS 网关
    </para>
   </listitem>
   <listitem>
    <para>
     iSCSI 网关
    </para>
   </listitem>
   <listitem>
    <para>
     NFS Ganesha
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    建议逐个升级特定类型的所有守护程序（例如，所有监视器守护程序或所有 OSD 守护程序），以确保它们全部采用相同的版本。另外，建议在尝试体验某个版本的新功能之前，升级群集中的所有守护程序。
   </para>
   <para>
    升级特定类型的所有守护程序之后，请检查守护程序的状态。
   </para>
   <para>
    确保在升级所有监视器之后，每个监视器已重新加入仲裁：
   </para>
<screen><prompt>root # </prompt>ceph mon stat</screen>
   <para>
    确保在升级所有 OSD 之后，每个 Ceph OSD 守护程序已重新加入群集：
   </para>
<screen><prompt>root # </prompt>ceph osd stat</screen>
  </tip>

  <tip xml:id="ceph.upgrade.luminous_flag">
   <title>设置 require-osd-release luminous 标志</title>
   <para>
    将最后一个 OSD 升级到 SUSE Enterprise Storage 5 之后，监视器节点会检测所有 OSD 是否正在运行“luminous”版本的 Ceph，并可能会指出未设置 <option>require-osd-release luminous</option> osdmap 标志。在这种情况下，您需要手动设置此标志，以确认由于群集已升级到“luminous”，无法将它降级回 Ceph“jewel”。运行以下命令来设置该标志：
   </para>
<screen><prompt>root@minion &gt; </prompt>sudo ceph osd require-osd-release luminous</screen>
   <para>
    该命令完成后，警告将会消失。
   </para>
   <para>
    在全新安装的 SUSE Enterprise Storage 5 上，当 Ceph 监视器创建初始 osdmap 时，会自动设置此标志，因此最终用户无需执行任何操作。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>从 SUSE Enterprise Storage 4（DeepSea 部署）升级到版本 5</title>

  <important xml:id="u4to5.softreq">
   <title>软件要求</title>
   <para>
    您需要在要升级的所有 Ceph 节点上安装以下软件并将其更新到最新的包版本，然后才能开始升级过程：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    此外，在开始升级之前，需要通过运行 <command>zypper migration</command>（或偏好的升级方式）将 Salt 主控端节点升级到 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5。
   </para>
  </important>

  <warning>
   <title>升级前需考虑的要点</title>
   <itemizedlist>
    <listitem>
     <para>
      检查 AppArmor 服务是否正在运行，并在每个群集节点上禁用该服务。启动 YaST AppArmor 模块，选择<guimenu>设置</guimenu>，然后取消选择<guimenu>启用 Apparmor</guimenu> 复选框。单击<guimenu>完成</guimenu>进行确认。
     </para>
     <para>
      请注意，在启用 AppArmor 的情况下，SUSE Enterprise Storage 将<emphasis>无法</emphasis>正常工作。
     </para>
    </listitem>
    <listitem>
     <para>
      尽管群集在升级期间完全可正常运行，但 DeepSea 会设置“noout”标志，用于阻止 Ceph 在停机期间重新平衡数据，从而避免不必要的数据传输。
     </para>
    </listitem>
    <listitem>
     <para>
      为了优化升级过程，DeepSea 会根据节点的指派角色并遵循 Ceph 上游的建议，按以下顺序升级节点：MON、MGR、OSD、MDS、RGW、IGW 和 NFS Ganesha。
     </para>
     <para>
      请注意，如果节点运行多个服务，DeepSea 将无法防止违反上述顺序。
     </para>
    </listitem>
    <listitem>
     <para>
      尽管 Ceph 群集在升级期间可正常运行，但节点可能需要重引导才能应用新内核版本等设置。为了减少等待中的 I/O 操作，建议在升级过程中拒绝传入请求。
     </para>
    </listitem>
    <listitem>
     <para>
      群集升级可能要花费很长时间 — 所需时间大约为升级一台计算机的时间乘以群集节点数。
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   要将 SUSE Enterprise Storage 4 群集升级到版本 5，请执行以下步骤：
  </para>

  <procedure>
   <step>
    <para>
     使用 <command>RPM -q deepsea</command> 校验 Salt 主控端节点上的 DeepSea 包版本是否至少以 <literal>0.7</literal> 开头。例如：
    </para>
<screen><prompt>root@master # </prompt>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     如果 DeepSea 包版本号以 0.6 开头，请再次确认是否已成功将 Salt 主控端节点迁移到 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5（请参见本节开头的<xref linkend="u4to5.softreq"/>）。在开始升级过程之前，必须满足此先决条件。
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       如果已将系统注册到 SUSEConnect 并使用 SCC/SMT，则不需要执行进一步的操作。继续 <xref linkend="step.updatepillar"/>。
      </para>
     </step>
     <step>
      <para>
       如果使用的<emphasis role="bold">不是</emphasis> SCC/SMT，而是 Media-ISO 或其他包源，请更改 Pillar 数据以使用不同的策略。编辑
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       并添加以下几行：
      </para>
<screen>upgrade_init: zypper-dup</screen>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     要更新 Pillar，请执行以下命令：
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     校验是否已成功写入 Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt '*' pillar.get upgrade_init</screen>
    <para>
     该命令的输出应会镜像您添加的项。
    </para>
   </step>
   <step>
    <para>
     升级 Salt 受控端：
    </para>
<screen><prompt>root@master # </prompt>salt '*' state.apply ceph.updates</screen>
   </step>
   <step>
    <para>
     校验是否已升级所有 Salt 受控端：
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.version</screen>
   </step>
   <step>
    <para>
     包含群集的 Salt 受控端。有关更多细节，请参见<xref linkend="ds.depl.stages"/>的<xref linkend="ds.depl.s1"/>。
    </para>
   </step>
   <step>
    <para>
     开始升级 SUSE Linux Enterprise Server 和 Ceph：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>重引导后重新运行</title>
     <para>
      如果升级过程导致 Salt 主控端重引导，请重新运行该命令，以再次启动 Salt 受控端的升级过程。
     </para>
    </tip>
   </step>
   <step>
    <para>
     升级后，检查所有节点上的 AppArmor 是否已禁用并已停止：
    </para>
<screen><prompt>root # </prompt>systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     升级后，Ceph 管理器暂时还未安装。要让群集保持正常状态，请执行以下操作：
    </para>
    <substeps>
     <step>
      <para>
       运行阶段 0 以启用 Salt REST API：
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       运行阶段 1 以创建 <filename>role-mgr/</filename> 子目录：
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       根据<xref linkend="policy.configuration"/>中所述编辑 <guimenu>policy.cfg</guimenu>，并将一个 Ceph 管理器角色添加到部署了 Ceph 监视器的节点。另外，将 openATTIC 角色添加到管理节点。
      </para>
     </step>
     <step>
      <para>
       运行阶段 2 以更新 Pillar：
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       运行阶段 3 以部署 Ceph 管理器：
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       运行阶段 4 以正确配置 openATTIC：
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>Ceph 密钥功能不匹配</title>
     <para>
      如果 <literal>ceph.stage.3</literal> 失败并出现“错误 EINVAL：实体 client.bootstrap-osd 存在，但功能不匹配”，则表示现有群集的 <literal>client.bootstrap.osd</literal> 密钥的密钥功能 (caps) 与 DeepSea 尝试设置的功能不匹配。在红色错误讯息的上方，可以看到失败命令 <command>ceph auth</command> 的转储。请查看此命令，检查所用的密钥 ID 和文件。对于 <literal>client.bootstrap-osd</literal>，该命令是
     </para>
<screen><prompt>root # </prompt>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      要解决密钥功能不匹配问题，请检查 DeepSea 正在尝试部署的密钥环文件的内容，例如：
     </para>
<screen><prompt>cephadm &gt; </prompt>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      将此内容与 <command>ceph auth get client.bootstrap-osd</command> 的输出进行比较：
     </para>
<screen><prompt>root # </prompt>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      可以看到，后一个密钥缺少 <literal>caps mgr = "allow r"</literal>。要解决此问题，请运行：
     </para>
<screen><prompt>root # </prompt>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      现在，运行 <literal>ceph.stage.3</literal> 应该会成功。
     </para>
     <para>
      运行 <literal>ceph.stage.4</literal> 时，元数据服务器和对象网关密钥环可能会出现相同的问题。可以运用上述相同的过程：检查失败的命令、要部署的密钥环文件，以及现有密钥的功能。然后运行 <command>ceph auth caps</command> 更新现有的密钥功能，使之与 DeepSea 正在部署的功能匹配。
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>升级失败</title>
   <para>
    如果群集处于“HEALTH_ERR”状态的持续时间超过 300 秒，或者每个指派角色的服务之一处于关闭状态的持续时间超过 900 秒，则表示升级失败。在这种情况下，请尝试找出并解决问题，然后重新运行升级过程。请注意，在虚拟化环境中，超时会更短。
   </para>
  </important>

  <important>
   <title>重引导 OSD</title>
   <para>
    升级到 SUSE Enterprise Storage 5 之后，FileStore OSD 启动所需的时间大约会多五分钟，因为 OSD 将对其磁盘中的文件执行一次性转换。
   </para>
  </important>

  <tip>
   <title>检查群集组件/节点的版本</title>
   <para>
    如果您需要确定单个群集组件和节点的版本（例如，确定所有节点在升级后是否真正处于相同的增补程序级别），可以运行
   </para>
<screen><prompt>root@master # </prompt>salt-run status.report</screen>
   <para>
    该命令将会遍历连接的 Salt 受控端，扫描 Ceph、Salt 和 SUSE Linux Enterprise Server 的版本号，并提供一份报告，其中会列出大多数节点的版本，并显示其版本与大多数节点不相同的节点。
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>将 OSD 迁移到 BlueStore</title>
   <para>
    OSD BlueStore 是 OSD 守护程序的新后端。从 SUSE Enterprise Storage 5 开始，它是默认的选项。与以文件形式将对象储存在 XFS 文件系统中的 FileStore 相比，BlueStore 可提供更高的性能，因为它直接将对象储存在底层块设备中。BlueStore 还能实现 FileStore 所不能提供的其他功能，例如内置压缩和 EC 重写。
   </para>
   <para>
    具体而言，在 BlueStore 中，OSD 包含一个“wal”（预写式日志）设备和一个“db”（RocksDB 数据库）设备。RocksDB 数据库会保存 BlueStore OSD 的元数据。默认情况下，这两个设备驻留在 OSD 所在的同一台设备上，但也可以将它们放置在更快/不同的媒体上。
   </para>
   <para>
    在 SES 5 中，FileStore 和 BlueStore 均受支持，FileStore 和 BlueStore OSD 可在单个群集中共存。在 SUSE Enterprise Storage 升级过程中，FileStore OSD 不会自动转换到 BlueStore。请注意，在尚未迁移到 BlueStore 的 OSD 上，将无法使用特定于 BlueStore 的功能。
   </para>
   <para>
    在转换到 BlueStore 之前，OSD 需要运行 SUSE Enterprise Storage 5。转换是个缓慢的过程，因为所有数据需要重新写入两次。尽管迁移过程可能需要很长时间才能完成，但在此期间，群集的工作不会中断，所有客户端都可继续访问群集。但是，迁移期间的性能势必会下降，原因是需要重新平衡和回填群集数据。
   </para>
   <para>
    请执行以下过程将 FileStore OSD 迁移到 BlueStore：
   </para>
   <procedure>
    <step>
     <para>
      迁移硬件配置文件：
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.policy</screen>
     <para>
      此运行程序会迁移 <filename>policy.cfg</filename> 文件当前正在使用的所有硬件配置文件。它会处理 <filename>policy.cfg</filename>，查找使用原始数据结构的任何硬件配置文件，并将其转换为新的数据结构。这样会产生名为“migrated-<replaceable>原始名称</replaceable>”的新硬件配置文件。<filename>policy.cfg</filename> 也会更新。
     </para>
     <para>
      如果原始配置包含单独的日记，BlueStore 配置将对该 OSD 的“wal”和“db”使用相同的设备。
     </para>
    </step>
    <step>
     <para>
      DeepSea 通过将 OSD 的权重设置为 0（“抽取”数据，直到 OSD 已空）来迁移 OSD。您可以逐个迁移 OSD，也可以一次性迁移所有 OSD。在任一情况下，当 OSD 已空时，编制进程会去除该 OSD，然后使用新配置重新创建 OSD。
     </para>
     <tip>
      <title>建议的方法</title>
      <para>
       如果您有大量的物理储存节点，或几乎没有任何数据，请使用 <command>ceph.migrate.nodes</command>。如果一个节点占用的容量小于总容量的 10%，则使用 <command>ceph.migrate.nodes</command> 同时移动这些 OSD 中的所有数据的速度可能略微快一点。
      </para>
      <para>
       如果您不确定要使用哪种方法，或者站点包含的储存节点较少（例如，每个节点包含的数据大于群集数据的 10%），请选择 <command>ceph.migrate.osds</command>。
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        要逐个迁移 OSD，请运行：
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        要同时迁移每个节点上的所有 OSD，请运行：
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       编制进程不提供有关迁移进度的反馈，您可以使用
      </para>
<screen><prompt>root # </prompt>ceph osd tree</screen>
      <para>
       定期查看哪些 OSD 的权重为 0。
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    迁移到 BlueStore 之后，对象计数将保持不变，磁盘使用率也几乎相同。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>从 SUSE Enterprise Storage 4（<command>ceph-deploy</command> 部署）升级到版本 5</title>

  <important>
   <title>软件要求</title>
   <para>
    您需要在要升级的所有 Ceph 节点上安装以下软件并将其更新到最新的包版本，然后才能开始升级过程：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   要将使用 <command>ceph-deploy</command> 部署的 SUSE Enterprise Storage 4 群集升级到版本 5，请执行以下步骤：
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>要对所有群集节点应用的步骤</title>
   <step>
    <para>
     如果您使用的是已注册到 SCC 的系统，则无需执行任何操作。否则，请准备软件储存库。禁用除 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5 储存库以外的所有储存库。如果缺少这些储存库，请加以添加。
    </para>
   </step>
   <step>
    <para>
     安装 <systemitem>salt</systemitem> 包：
    </para>
<screen><prompt>root # </prompt>zypper install salt</screen>
   </step>
   <step>
    <para>
     安装 <systemitem>salt-minion</systemitem> 包，然后启用并启动相关的服务：
    </para>
<screen><prompt>root # </prompt>zypper install salt-minion
<prompt>root # </prompt>systemctl enable salt-minion
<prompt>root # </prompt>systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     确保主机名“salt”可解析成 Salt 主控端/管理节点的 IP 地址。如果无法通过主机名 <literal>salt</literal> 访问 Salt 主控端，请编辑文件 <filename>/etc/salt/minion</filename>，或创建包含以下内容的新文件 <filename>/etc/salt/minion.d/master.conf</filename>：
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     如果对上述配置文件执行了任何更改，请在所有 Salt 受控端上重启动 Salt 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>要对管理节点/Salt 主控端应用的步骤</title>
   <step>
    <para>
     安装 <systemitem>salt-master</systemitem> 包，然后启用并启动相关的服务：
    </para>
<screen><prompt>root@master # </prompt>zypper install salt-master
<prompt>root@master # </prompt>systemctl enable salt-master
<prompt>root@master # </prompt>systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     通过列出所有 Salt 受控端的密钥来校验这些受控端是否存在：
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     将所有 Salt 受控端密钥添加到 Salt 主控端（包括主受控端）：
    </para>
<screen><prompt>root@master # </prompt>salt-key -A -y</screen>
   </step>
   <step>
    <para>
     确保所有 Salt 受控端的密钥已被接受：
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     确保管理节点上的软件是最新的：
    </para>
<screen><prompt>root@master # </prompt>zypper migration</screen>
   </step>
   <step>
    <para>
     安装 <systemitem>deepsea</systemitem> 包：
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     包含群集的 Salt 受控端。有关更多细节，请参见<xref linkend="ds.depl.stages"/>的<xref linkend="ds.depl.s1"/>。
    </para>
   </step>
   <step>
    <para>
     汲取 <command>ceph-deploy</command> 安装的现有群集：
    </para>
<screen><prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster</screen>
    <para>
     该命令将执行以下操作：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       将全部所需的 Salt 和 DeepSea 模块分发到所有 Salt 受控端。
      </para>
     </listitem>
     <listitem>
      <para>
       检查运行中的 Ceph 群集，并在 <filename>/srv/pillar/ceph/proposals</filename> 中填充群集的布局。
      </para>
      <para>
       将使用与所有检测到的运行中 Ceph 服务匹配的角色创建 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。查看此文件可以校验每个现有的 MON、OSD、RGW 和 MDS 节点是否具有相应的角色。OSD 节点将导入到 <filename>profile-import/</filename> 子目录，因此您可以检查 <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename> 和 <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename> 中的文件，以确认是否正确选取了 OSD。
      </para>
      <note>
       <para>
        对于 Salt 主控端节点，生成的 <filename>policy.cfg</filename> 只会应用检测到的 Ceph 服务的角色“role-mon”、“role-mgr”、“role-mds”、“role-rgw”、“role-admin”和“role-master”。如果需要其他角色，则需手动将其添加到该文件（请参见<xref linkend="policy.role.assignment"/>）。
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       现有群集的 <filename>ceph.conf</filename> 将保存到 <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>。
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> 将包含群集的 fsid、群集网络和公共网络，并指定 <option>configuration_init: default-import</option> 选项，如此 DeepSea 将使用前面所述的 <filename>ceph.conf.import</filename> 配置文件，而不是使用 DeepSea 的默认模板 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>。
      </para>
     </listitem>
     <listitem>
      <para>
       群集的各个密钥环将保存到以下目录：
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mgr/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     运行阶段 1 以创建所有可能的角色：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     在 <filename>/srv/pillar/ceph/stack</filename> 下生成所需的子目录：
    </para>
<screen><prompt>root@master # </prompt>salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     从现在起，执行<xref linkend="ceph.upgrade.4to5"/>中所述的过程。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>从 SUSE Enterprise Storage 3 升级到版本 5</title>

  <important>
   <title>软件要求</title>
   <para>
    您需要在要升级的所有 Ceph 节点上安装以下软件并将其更新到最新的包版本，然后才能开始升级过程：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   要将 SUSE Enterprise Storage 3 群集升级到版本 5，请依次执行<xref linkend="upgrade4to5cephdeploy.all"/>和<xref linkend="upgrade4to5cephdeploy.admin"/>中所述的步骤。
  </para>
 </sect1>
</chapter>
