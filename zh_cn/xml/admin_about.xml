<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha.storage.about">
 <title>SUSE Enterprise Storage 和 Ceph</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>编辑</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  SUSE Enterprise Storage 是基于 Ceph 技术的分布式储存系统，旨在提高可伸缩性、可靠性和性能。Ceph 群集可在常见网络（例如以太网）中的市售服务器上运行。该群集能够正常扩展到数千台服务器（后面称为“节点”）以及 PB 量级的处理能力。与使用分配表来储存和提取数据的传统系统相比，Ceph 使用确定性算法来为数据分配储存空间，并且不采用任何集中式信息结构。Ceph 假设在储存群集中，硬件的添加或去除属于惯例，而不是例外情况。Ceph 群集可将数据分布和重新分布、数据复制、故障检测和恢复等管理任务自动化。Ceph 既可自我修复，又可自我管理，因此可以降低管理开销和预算开销。
 </para>
 <para>
  本章提供 SUSE Enterprise Storage 的综合概述，并简要介绍一些最重要的组件。
 </para>
 <tip>
  <para>
   从 SUSE Enterprise Storage 5 开始，唯一的群集部署方法为 DeepSea。有关部署过程的细节，请参见<xref linkend="ceph.install.saltstack"/>。
  </para>
 </tip>
 <sect1 xml:id="storage.intro.features">
  <title>Ceph 的特性</title>

  <para>
   Ceph 环境具有以下特性：
  </para>

  <variablelist>
   <varlistentry>
    <term>可伸缩性</term>
    <listitem>
     <para>
      Ceph 可扩展到数千个节点，并可管理 PB 量级的储存。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>市售硬件</term>
    <listitem>
     <para>
      无需特殊的硬件即可运行 Ceph 群集。有关细节，请参见<xref linkend="storage.bp.hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>自我管理</term>
    <listitem>
     <para>
      Ceph 群集可自我管理。添加、去除节点或节点发生故障时，群集可自动重新分布数据。此外，它还能识别过载的磁盘。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>无单一故障点</term>
    <listitem>
     <para>
      重要的信息不会单独储存在群集中的某个节点上。可以配置冗余数量。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>开放源代码软件</term>
    <listitem>
     <para>
      Ceph 是一套开源软件解决方案，独立于特定的硬件或供应商。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.intro.core">
  <title>核心组件</title>

  <para>
   要充分利用 Ceph 的强大功能，需要了解一些基本的组件和概念。本节介绍经常在其他章节中提到的 Ceph 的某些组成部分。
  </para>

  <sect2 xml:id="storage.intro.core.rados">
   <title>RADOS</title>
   <para>
    Ceph 的基本组件称为 <emphasis>RADOS</emphasis>
    <emphasis>（可靠自主分布式对象储存）</emphasis>。该组件负责管理群集中储存的数据。Ceph 中的数据通常以对象的形式储存。每个对象由标识符和数据组成。
   </para>
   <para>
    RADOS 提供以下方法来访问涉及许多用例的储存对象：
   </para>
   <variablelist>
    <varlistentry>
     <term>RADOS 网关</term>
     <listitem>
      <para>
       RADOS 网关是 RADOS 对象储存区的 HTTP REST 网关。使用该网关可以直接访问 Ceph 群集中储存的对象。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RADOS 块设备</term>
     <listitem>
      <para>
       可以像访问任何其他块设备一样访问 RADOS 块设备 (RBD)。例如，可将这些设备与 <systemitem class="library">libvirt</systemitem> 结合使用，以实现虚拟化。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       Ceph 文件系统是与 POSIX 兼容的文件系统。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem>
     </term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> 是一个库，可与许多编程语言结合使用，以创建能够直接与储存群集交互的应用程序。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> 由 RADOS 网关和 RBD 使用，而 CephFS 直接与 RADOS 连接。请参见<xref linkend="storage.intro.core.rados.figure"/>。
   </para>
   <figure xml:id="storage.intro.core.rados.figure">
    <title>与 Ceph 对象储存区连接</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage.intro.core.crush">
   <title>CRUSH</title>
   <para>
    Ceph 群集的核心是 <emphasis>CRUSH</emphasis> 算法。CRUSH 是 <emphasis>Controlled Replication Under Scalable Hashing</emphasis>（基于可缩放哈希的受控复制）的缩写。CRUSH 是处理储存分配的函数，所需的参数相对较少。这意味着，只需提供少量的信息就能计算对象的储存位置。参数是群集的当前拓扑图，包括运行状况、管理员定义的某些放置规则，以及需要储存或检索的对象名称。提供这些信息后，Ceph 群集中的所有节点即可计算对象及其副本的储存位置。这使数据写入或读取变得非常有效。CRUSH 会尝试在群集中的所有节点上均匀分布数据。
   </para>
   <para>
    <emphasis>CRUSH 拓扑图</emphasis>包含所有储存节点，以及管理员定义的有关在群集中储存对象的放置规则。该拓扑图定义了通常对应于群集物理结构的分层结构。例如，包含数据的磁盘位于主机中，主机位于机架中，机架位于设备排中，而设备排位于数据中心中。此结构可用于定义<emphasis>故障域</emphasis>。然后，Ceph 可确保将复制项储存在特定故障域的不同分支中。
   </para>
   <para>
    如果将故障域设置为机架，则在不同的机架中分布对象复制项。这样就可以缓解机架中的交换机发生故障所造成的服务中断。如果某个电源分配单元为一排机架供电，则可将故障域设置为设备排。当电源分配单元发生故障时，其他设备排中仍可提供复制的数据。
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.core.nodes">
   <title>Ceph 节点和守护程序</title>
   <para>
    在 Ceph 中，节点是为群集工作的服务器。它们可以运行多种不同类型的守护程序。建议在每个节点上只运行一种类型的守护程序，但 MGR 守护程序除外，此类守护程序可与 MON 共置。每个群集至少需要有 MON、MGR 和 OSD 守护程序：
   </para>
   <variablelist>
    <varlistentry>
     <term>Ceph 监视器</term>
     <listitem>
      <para>
       <emphasis>Ceph 监视器</emphasis>（往往缩写为 <emphasis>MON</emphasis>）节点维护有关群集运行状况、所有节点的拓扑图和数据分布规则的信息（请参见<xref linkend="storage.intro.core.crush"/>）。
      </para>
      <para>
       如果发生故障或冲突，群集中的 Ceph 监视器节点会根据多数派原则确定哪些信息是正确的。为了构成有效的多数派，建议设置奇数数量的 Ceph 监视器节点，并至少设置三个这样的节点。
      </para>
      <para>
       如果使用多个站点，应在奇数个站点之间分布 Ceph 监视器节点。每个站点的 Ceph 监视器节点数应满足以下要求：当一个站点发生故障时，50% 以上的 Ceph 监视器节点可保持正常运行。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph 管理器</term>
     <listitem>
      <para>
       Ceph 管理器 (MGR) 会从整个群集收集状态信息。Ceph 管理器守护程序与监视器守护程序一同运行。它提供附加的监视功能，并与外部监视和管理系统连接。
      </para>
      <para>
       Ceph 管理器不需要额外的配置，只需确保它在运行即可。您可以使用 DeepSea 将它部署为独立的角色。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       <emphasis>Ceph OSD</emphasis> 是一个守护程序，负责处理属于物理或逻辑储存单元（硬盘或分区）的<emphasis>对象储存设备</emphasis>。对象储存设备可以是物理磁盘/分区，也可以是逻辑卷。此外，该守护程序会处理数据复制，并在添加或去除节点后进行重新平衡。
      </para>
      <para>
       Ceph OSD 守护程序与监视器守护程序通讯，并为其提供其他 OSD 守护程序的状态。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    要使用 CephFS、RADOS 网关、NFS Ganesha 或 iSCSI 网关，还需要其他节点：
   </para>
   <variablelist>
    <varlistentry>
     <term>元数据服务器 (MDS)</term>
     <listitem>
      <para>
       元数据服务器储存 CephFS 的元数据。使用 MDS 可以执行 <command>ls</command> 等基本文件系统命令，而不会让群集过载。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RADOS 网关</term>
     <listitem>
      <para>
       RADOS 网关提供的 Ceph 对象网关是 RADOS 对象储存区的 HTTP REST 网关。它与 OpenStack Swift 和 Amazon S3 兼容，具有自己的用户管理功能。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       使用 NFS Ganesha 可通过 NFS 访问 RADOS 网关或 CephFS。该组件在用户空间而不是内核空间中运行，直接与 RADOS 网关或 CephFS 交互。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI 网关</term>
     <listitem>
      <para>
       iSCSI 是一种储存网络协议，可让客户端将 SCSI 命令发送到远程服务器上的 SCSI 储存设备（目标）。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.intro.structure">
  <title>储存结构</title>

  <sect2 xml:id="storage.intro.structure.pool">
   <title>池</title>
   <para>
    Ceph 群集中储存的对象放置在<emphasis>池</emphasis>中。对外部环境而言，池代表群集的逻辑分区。对于每个池，可以定义一组规则，例如，每个对象必须存在多少个复制项。池的标准配置称为<emphasis>副本池</emphasis>。
   </para>
   <para>
    池通常包含多个对象，但也可以将其配置为充当类似 RAID 5 的作用。在此配置中，对象连同其他编码大块一起储存在大块中。编码大块包含冗余信息。管理员可以定义数据和编码大块的数量。在此配置中，池称为<emphasis>纠删码池</emphasis>。
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.pg">
   <title>归置组</title>
   <para>
    <emphasis>归置组</emphasis> (PG) 用于在池中分布数据。创建池时，会设置特定数目的归置组。归置组在内部用于将对象分组，是影响 Ceph 群集性能的重要因素。对象的 PG 根据该对象的名称确定。
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.example">
   <title>示例</title>
   <para>
    本节提供有关 Ceph 如何管理数据的简化示例（请参见<xref linkend="storage.intro.structure.example.figure"/>）。此示例并不代表 Ceph 群集的建议配置。硬件设置由三个储存节点或 Ceph OSD（<literal>主机 1</literal>、<literal>主机 2</literal> 和<literal>主机 3</literal>）组成。每个节点包含三个用作 OSD（<literal>osd.0</literal> 到 <literal> osd.9</literal>）的硬盘。此示例中忽略了 Ceph 监视器节点。
   </para>
   <note>
    <title>Ceph OSD 与 OSD 之间的差别</title>
    <para>
     尽管 <emphasis>Ceph OSD</emphasis> 或 <emphasis>Ceph OSD 守护程序</emphasis>是指节点上运行的守护程序，但 <emphasis>OSD</emphasis> 一词指的是与守护程序交互的逻辑磁盘。
    </para>
   </note>
   <para>
    群集包含两个池：<literal>池 A</literal> 和<literal>池 B</literal>。尽管池 A 仅复制对象两次，但池 B 的恢复能力更重要，该池中的每个对象都有三个复制项。
   </para>
   <para>
    当应用程序将某个对象放入池中时（例如，通过 REST API），将会根据池和对象名称选择归置组（<literal>PG1</literal> 到 <literal>PG4</literal>）。然后，CRUSH 算法会根据包含对象的归置组，计算要将对象储存到哪些 OSD。
   </para>
   <para>
    在此示例中，故障域设置为主机。这可确保将对象的复制项储存在不同的主机上。根据针对池设置的复制级别，对象将储存在归置组使用的两个或三个 OSD 上。
   </para>
   <para>
    写入对象的应用程序只与一个 Ceph OSD（主要 Ceph OSD）交互。主要 Ceph OSD 处理复制过程，并在所有其他 OSD 储存对象后确认写入过程完成。
   </para>
   <para>
    如果 <literal>osd.5</literal> 发生故障，<literal>osd.1</literal> 上仍会提供 <literal>PG1</literal> 中的所有对象。一旦群集发现某个 OSD 发生故障，另一个 OSD 会立即接管工作。在此示例中，<literal>osd.4</literal> 用于取代 <literal>osd.5</literal>。然后，<literal>osd.1</literal> 上储存的对象会复制到 <literal>osd.4</literal>，以恢复复制级别。
   </para>
   <figure xml:id="storage.intro.structure.example.figure">
    <title>小规模 Ceph 示例</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    如果将包含新 OSD 的新节点添加到群集，群集拓扑图将会更改。然后，CRUSH 函数将返回对象的不同位置。接收新位置的对象将被重新定位。此过程导致所有 OSD 被平衡使用。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.moreinfo">
  <title>其他信息</title>

  <itemizedlist>
   <listitem>
    <para>
     作为一个社区项目，Ceph 自身具有丰富的联机文档。对于本手册中未介绍的主题，请参见 <link xlink:href="http://ceph.com/docs/luminous/"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     由 <emphasis>S.A. Weil、S.A. Brandt、E.L. Miller 和 C. Maltzahn</emphasis> 撰写的原始文献 <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis>（CRUSH：复制数据的受控、可缩放、分布式放置）提供了有关 Ceph 内部工作原理的有用见解。特别是在部署大规模群集时，推荐您阅读此文章。可在 <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/> 上找到该文献。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
